---
title: "NACC data project"
author: Dr. Siddharth Ramanan, MRC Cognition and Brain Sciences Unit, The University
  of Cambridge, UK
date: '2023-5-22'
geometry: margin=2cm
output:
  html_document: default
  word_document: default
  pdf_document: default
---

# Project introduction

**Background:** Using data-driven decomposition to understand longitudinal changes in cognitive-behavioural performance in dementia syndromes while accounting for individual-level differences and group-level heterogeneity.

**Aims:**

1.  Explore patterns of heterogeneity and overlap between different dementia syndromes on neuropsych and behavioural tests
2.  Explore how these patterns change with time on longitudinal data
3.  Investigate associations with brain structure features

**Possible methods to think about using:** Linear mixed models; Latent Growth Curve Models; Event-based modelling; epidemic spreading models; Principal component analysis; Optimal Transport methods; Procrustes analyses for analysing shape changes between two different matrices (i.e., on PCA data from Time 1 vs. Time 2); Time varying graphical lasso; functional data analysis; Structural Equation Modelling, self-organising maps...

**Troubleshooting**: If MASS package is installed, select function won't work unless you specify dplyr::select.

**Description of dataset:** I'm using the National Alzheimer Coordinating Centre - Uniform Data Set 3. This dataset has hundreds of typical and atypical AD, FTD and other neurodegenerative disease group patients recruited across the USA. Full information here: <https://naccdata.org/>

**People involved**: SR, MALR, JBR, KP, DA, SKH, Matt R, KA

# Project data storage details

**Locations of files:** General folder with all project information: /Group/MLR-Lab/Sid/OngoingProjects/NACC

-   Within this folder:

    -   Behavioural data: /Data
    -   Data access and correspondence files: /Access documents-Correspondence
    -   Data dictionary: /Data/Data Dictionary

-   R code for this project (i.e., this Rmd file) is stored locally (on my CBU computer) in C:\\Users\\sr06\\Documents\\NACCProject. Final copy of code in my project folder in MALR drive

-   **NOTE:** The project code is stored on my GitHub /siddharthramanan.

-   Acronyms used in this report:

    -   NACC/UDS = National Alzheimer's Coordinating Centre - Uniform Data Set

    -   DD-UDS = Data dictionary - Uniform Data Set (demographics, clinic and neuropsychological variables)

    -   DD-NP = Data dictionary - neuropathology

    -   DD-I = Data dictionary - imaging

    -   DD-G = Data dictionary - genetics

\newpage

# Package loading and data reading

I will load all the necessary packages.

```{r}

knitr::opts_chunk$set(error = TRUE)

suppressWarnings(suppressPackageStartupMessages({ # suppress package start up messages and any warnings
  library(tidyverse) # data wrangling
  library(dplyr) # data wrangling
  library(corrgram) # correlation heatmap plotting
  library(reshape2) # data wrangling
  library(data.table) # data wrangling - I will need fread() from this package to read this large dataset in
  library(broom) # data wrangling
  library(purrr) # data wrangling
  library(dunn.test) # Sidak tests for post-hoc (might not be required for the sample sizes being used here)
  library(psych) # PCA nad procrustes
  library(GPArotation) #procrustes
  library(ggridges) # ridgeline plots
  library(ggrepel) # plotting
  library(gghighlight) # highlight specific points in ggplot
  library(stringr) # change patterns of variable names
  library(ggplot2) # data plotting
  library(ggalluvial) # alluvial plots
  library(vegan) # data analysis
  library(viridis) # colours for ggplot
  library(readxl) # writing/reading xls files
  library(cowplot) # data plotting
  library(forcats) # for factorising
  library(naniar) # missing value handling
  library(effectsize) # for effect size plotting
  library(ggpubr) # to plot p/r values into the plot
  library(reticulate) # for executing python codes in this Rmd document
  library(RColorBrewer) # for colour palettes
  library(mice) # for multiple imputation
  library(miceadds) # extra helper functions for mice ^
  library(openxlsx) # write df into excel sheet
  library(distances) # euclidean distance calculation
  library(ez) # for ezANOVA
  library(seriation) # for dissimilarity matrices
  library(Rtsne) # for t-SNEs
  library(umap) # for UMAPs
  library(ggdensity) # for plotting density probability maps
  library(gganimate) # for animating UMAP figures
  library(gifski) # for animating UMAP figures
  library(av) # for video saving of animated UMAP figures
  library(MASS) # for KDE
  library(sf) # for spatial analysis and centroid calculation
  library(spatstat) # for average nearest neighbour analysis
  library(maptools) # for spatial data analysis
  library(ks) # for KDE test of distribtuion differences
  library(DHARMa) # for testing autocorrelation effects for spatial regression
  library(mgcv) # for generalised additive models.
  library(geosphere) # for centroid calculation
  library(pROC) # ROC
  library(trajr) # trajectory analysis
  }))
```

I will also load a colour-blind friendly palette for figure plotting. You can also use a palette from `RColourBrewer`. To see these palettes, see this link: <https://www.datanovia.com/en/blog/the-a-z-of-rcolorbrewer-palette/>.

```{r}

# all Dx colours
group.colours <- c("AD" = "#D55E00", "bvFTD" = "#009E73", "FTLD-motor" = "#0072B2", "CBD" = "#0072B2", "FTLD-NOS" = "#56B4E9", "PPA" = "#CC79A7", "lvPPA" = "#CC79A7", "nfvPPA" = "#CC79A7", "svPPA" = "#CC79A7")

# Dx change colours
dx_change.group.colours <- c("FirstDx-AD" = "#D55E00", "FirstDx-bvFTD" = "#009E73", "FirstDx-FTLD-motor" = "#0072B2", "CBD" = "#0072B2", "FirstDx-FTLD-NOS" = "#56B4E9", "FirstDx-PPA" = "#CC79A7", "lvPPA" = "#CC79A7", "nfvPPA" = "#CC79A7", "svPPA" = "#CC79A7")

# colour blind palette
colorBlindBlack_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# pathology palette
path.colours <- c("red", 
                  "coral",
                  "darkred", 
                  "chocolate1", 
                  "cornsilk4", 
                  "cyan1",
                  "darkgoldenrod2",
                  "darkorchid2",
                  "deeppink2",
                  "darkslategray4", 
                  "chartreuse4",
                  "brown1",
                  "blueviolet",
                  "aquamarine",
                  "blue3", 
                  "azure4", 
                  "darkorange3", 
                  "brown3", "burlywood4")

# set colours for each group 
# AD = "#D55E00"; bvFTD = "#009E73"; FTLD-motor = "#0072B2" ; FTLD-NOS = "#56B4E9" ; PPA = "#CC79A7"
```

I will now read in the behavioural dataset. Its worth noting that this dataset is \>400mb so it'll take time to read. I can speed the time up using `data.table::fread()`. As the data are located on the CBU secured server, I have mapped the //cbsu/data/group/mlr-lab link to the Z: drive on my computer. This helps R markdown read the data from the local PC itself (previously, I tried making it read the data from the network but the problem is that the github repository link is stored locally so I also need to store Rmd file in the same location locally).

```{r}

# Read in UDS dataset
nacc <- data.table::fread("Z:/Sid/OngoingProjects/NACC/Data/investigator_nacc56 (1).csv", header = T)
```

The dataset is huge! It has `r nrow(nacc)` rows and `r ncol(nacc)` columns!

In parallel, I will also read in imaging and CSF datasets. I will merge these in later with the final behavioural dataset.

```{r}

# Read in Imaging dataset
nacc_imaging <- data.table::fread("Z:/Sid/OngoingProjects/NACC/Data/investigator_mri_nacc56 (1).csv", header = T)

# Read in CSF dataset
nacc_csf <- data.table::fread("Z:/Sid/OngoingProjects/NACC/Data/investigator_fcsf_nacc56 (1).csv", header = T)
```

# Data cleaning and wrangling

This step is divided into many sub-steps. First, we'll inspect part of this dataframe.

```{r}

# Inspect the dataset
nacc %>%
  select (1:10) %>% # select first ten columns
  slice (1:10) # show first ten rows
```

There are many data cleaning and wrangling steps we need to do. We will do this in the order below:

## **1. Keeping variables of interest**

The first step is to shrink the dataset down to keep only the main columns of interest. Once we have a smaller dataframe to work with, then we can conduct other wrangling steps in a faster manner with more computational efficiency.

For this step, I'll rely on all DDs and create a vector of variables that we will keep. It is faster to select which columns to keep than which to drop (as there are just too many columns to list out to drop). To create this vector, I will simply copy and paste variable names from the DD-UDS/I/NP/G into the code below.

```{r results = "hide"}

# For this chunk of code, I want to hide the output from .rmd file (because it'll produce a page full of column names) so I'm using results="hide"

# read in all column names into a vector
nacc_cols <- dput(colnames(nacc))

# from the nacc_cols vector, keep only the columns of interest (refer to each DD for information on each variable)

keep_cols <- c("NACCID", "NACCADC", "NACCUDSD", "PACKET", "FORMVER", "VISITMO", "VISITDAY", 
"VISITYR", "NACCVNUM", "NACCAVST", "NACCNVST", "NACCDAYS", "NACCFDYS","NACCDIED", "NACCMOD", "NACCYOD", "NACCINT",  "BIRTHYR", "SEX", "EDUC", "RACE", "RACEX", "PRIMLANG", "INDEPEND", "HANDED", "NACCAGEB", "NACCAGE",  "NACCFAM", "NACCMOM", "NACCDAD", "NACCAM", "NACCAMX", "NACCAMS", "NACCAMSX", "NACCFM", "NACCFMX", "NACCFMS", "NACCFMSX", "NACCOM", "NACCOMX", "NACCOMS", "NACCOMSX", "NACCFADM", "NACCFFTD", "CBSTROKE", "STROKMUL", "PD", "TBI", "NCOTHR", "NCOTHRX", "B12DEF", "ALCOHOL", "ABUSOTHR", "ABUSX", "PTSD", "BIPOLAR", "SCHIZ", "DEP2YRS", "DEPOTHR", "ANXIETY", "OCD", "NPSYDEV", "PSYCDIS", "PSYCDISX",  "ABRUPT", "STEPWISE", "SOMATIC", "EMOT", "HXHYPER", "HXSTROKE", "FOCLSYM", "FOCLSIGN", "HACHIN", 
"CVDCOG", "STROKCOG", "CVDIMAG", "CVDIMAG1", "CVDIMAG2", "CVDIMAG3", 
"CVDIMAG4", "CVDIMAGX", "PDNORMAL", "MEMORY", "ORIENT", "JUDGMENT", 
"COMMUN", "HOMEHOBB", "PERSCARE", "CDRSUM", "CDRGLOB", "COMPORT", 
"CDRLANG", "NPIQINF", "NPIQINFX", "DEL", "DELSEV", "HALL", "HALLSEV", 
"AGIT", "AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", 
"ELATSEV", "APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", 
"MOT", "MOTSEV", "NITE", "NITESEV", "APP", "APPSEV", "NOGDS", 
"SATIS", "DROPACT", "EMPTY", "BORED", "SPIRITS", "AFRAID", "HAPPY", 
"HELPLESS", "STAYHOME", "MEMPROB", "WONDRFUL", "WRTHLESS", "ENERGY", 
"HOPELESS", "BETTER", "NACCGDS", "BILLS", "TAXES", "SHOPPING", 
"GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", 
"TRAVEL", "NACCNREX", "NORMEXAM", "FOCLDEF", "GAITDIS", "EYEMOVE", 
"PARKSIGN", "RESTTRL", "RESTTRR", "SLOWINGL", "SLOWINGR", "RIGIDL", 
"RIGIDR", "BRADY", "PARKGAIT", "POSTINST", "CVDSIGNS", "CORTDEF", 
"SIVDFIND", "CVDMOTL", "CVDMOTR", "CORTVISL", "CORTVISR", "SOMATL", 
"SOMATR", "POSTCORT", "PSPCBS", "EYEPSP", "DYSPSP", "AXIALPSP", 
"GAITPSP", "APRAXSP", "APRAXL", "APRAXR", "CORTSENL", "CORTSENR", 
"ATAXL", "ATAXR", "ALIENLML", "ALIENLMR", "DYSTONL", "DYSTONR", 
"MYOCLLT", "MYOCLRT", "ALSFIND", "GAITNPH", "OTHNEUR", "OTHNEURX", 
"B9CHG", "DECSUB", "DECIN", "DECCLIN", "DECCLCOG", "COGMEM", 
"COGORI", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", 
"COGFLAGO", "COGOTHR", "COGOTHRX", "NACCCOGF", "NACCCGFX", "COGMODE", 
"COGMODEX", "DECAGE", "DECCLBE", "BEAPATHY", "BEDEP", "BEVHALL", 
"BEVWELL", "BEVHAGO", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", 
"BEAGIT", "BEPERCH", "BEREM", "BEREMAGO", "BEANX", "BEOTHR", 
"BEOTHRX", "NACCBEHF", "NACCBEFX", "BEMODE", "BEMODEX", "BEAGE", 
"DECCLMOT", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW", "NACCMOTF", 
"MOMODE", "MOMODEX", "MOMOPARK", "PARKAGE", "MOMOALS", "ALSAGE", 
"MOAGE", "COURSE", "FRSTCHG", "LBDEVAL", "FTLDEVAL", "MMSECOMP", 
"MMSELOC", "MMSELAN", "MMSELANX", "MMSEVIS", "MMSEHEAR", "MMSEORDA", 
"MMSEORLO", "PENTAGON", "NACCMMSE", "NPSYCLOC", "NPSYLAN", "NPSYLANX", 
"LOGIMO", "LOGIDAY", "LOGIYR", "LOGIPREV", "LOGIMEM", "MEMUNITS", 
"MEMTIME", "UDSBENTC", "UDSBENTD", "UDSBENRS", "DIGIF", "DIGIFLEN", 
"DIGIB", "DIGIBLEN", "ANIMALS", "VEG", "TRAILA", "TRAILARR", 
"TRAILALI", "TRAILB", "TRAILBRR", "TRAILBLI", "WAIS", "BOSTON", 
"UDSVERFC", "UDSVERFN", "UDSVERNF", "UDSVERLC", "UDSVERLR", "UDSVERLN", 
"UDSVERTN", "UDSVERTE", "UDSVERTI", "COGSTAT", "NACCC1", "MOCACOMP", 
"MOCAREAS", "MOCALOC", "MOCALAN", "MOCALANX", "MOCAVIS", "MOCAHEAR", 
"MOCATOTS", "MOCATRAI", "MOCACUBE", "MOCACLOC", "MOCACLON", "MOCACLOH", 
"MOCANAMI", "MOCAREGI", "MOCADIGI", "MOCALETT", "MOCASER7", "MOCAREPE", 
"MOCAFLUE", "MOCAABST", "MOCARECN", "MOCARECC", "MOCARECR", "MOCAORDT", 
"MOCAORMO", "MOCAORYR", "MOCAORDY", "MOCAORPL", "MOCAORCT", "NACCMOCA", 
"CRAFTVRS", "CRAFTURS", "DIGFORCT", "DIGFORSL", "DIGBACCT", "DIGBACLS", 
"CRAFTDVR", "CRAFTDRE", "CRAFTDTI", "CRAFTCUE", "MINTTOTS", "MINTTOTW", 
"MINTSCNG", "MINTSCNC", "MINTPCNG", "MINTPCNC", "NACCC2", "MODCOMM", 
"MOCBTOTS", "NACCMOCB", "REY1REC", "REY1INT", "REY2REC", "REY2INT", 
"REY3REC", "REY3INT", "REY4REC", "REY4INT", "REY5REC", "REY5INT", 
"REY6REC", "REY6INT", "OTRAILA", "OTRLARR", "OTRLALI", "OTRAILB", 
"OTRLBRR", "OTRLBLI", "REYDREC", "REYDINT", "REYTCOR", "REYFPOS", 
"VNTTOTW", "VNTPCNC", "RESPVAL", "RESPHEAR", "RESPDIST", "RESPINTR", 
"RESPDISN", "RESPFATG", "RESPEMOT", "RESPASST", "RESPOTH", "RESPOTHX",  "NORMCOG", "DEMENTED", "NACCUDSD", "AMNDEM", "PCA", "NACCPPA", "NACCPPAG", "NACCPPME", "NACCBVFT", "NACCLBDS", "NAMNDEM", "NACCTMCI", "NACCMCIL", "NACCMCIA", "NACCMCIE", "NACCMCIV", "NACCMCII", "IMPNOMCI", "AMYLPET", "AMYLCSF", "FDGAD", "HIPPATR", "TAUPETAD", 
"CSFTAU", "FDGFTLD", "TPETFTLD", "MRFTLD", "DATSCAN", "OTHBIOM", 
"OTHBIOMX", "IMAGLINF", "IMAGLAC", "IMAGMACH", "IMAGMICH", "IMAGMWMH", 
"IMAGEWMH", "OTHMUT", "OTHMUTX", "NACCALZD", "NACCALZP", "PROBAD", 
"PROBADIF", "POSSAD", "POSSADIF", "NACCLBDE", "NACCLBDP", "PARK", 
"MSA", "MSAIF", "PSP", "PSPIF", "CORT", "CORTIF", "FTLDMO", "FTLDMOIF", 
"FTLDNOS", "FTLDNOIF", "FTD", "FTDIF", "PPAPH", "PPAPHIF", "FTLDSUBT", 
"FTLDSUBX", "CVD", "CVDIF", "PREVSTK", "STROKDEC", "STKIMAG", 
"INFNETW", "INFWMH", "VASC", "VASCIF", "VASCPS", "VASCPSIF", 
"STROKE", "STROKIF", "ESSTREM", "ESSTREIF", "DOWNS", "DOWNSIF", 
"HUNT", "HUNTIF", "PRION", "PRIONIF", "BRNINJ", "BRNINJIF", "BRNINCTE", 
"HYCEPH", "HYCEPHIF", "EPILEP", "EPILEPIF", "NEOP", "NEOPIF", 
"NEOPSTAT", "HIV", "HIVIF", "OTHCOG", "OTHCOGIF", "OTHCOGX", 
"DEP", "DEPIF", "DEPTREAT", "BIPOLDX", "BIPOLDIF", "SCHIZOP", 
"SCHIZOIF", "ANXIET", "ANXIETIF", "DELIR", "DELIRIF", "PTSDDX", 
"PTSDDXIF", "OTHPSY", "OTHPSYIF", "OTHPSYX", "ALCDEM", "ALCDEMIF", 
"ALCABUSE", "IMPSUB", "IMPSUBIF", "DYSILL", "DYSILLIF", "MEDS", 
"MEDSIF", "DEMUN", "DEMUNIF", "COGOTH", "COGOTHIF", "COGOTHX", 
"COGOTH2", "COGOTH2F", "COGOTH2X", "COGOTH3", "COGOTH3F", "COGOTH3X", "NACCETPR", "NACCADMU", "NACCFTDM", "NACCAPOE", "NACCNE4S", "NPPMIH", "NACCBRNN", "NACCLEWY", "NACCPICK", "NACCCBD", "NACCPROG", "NPPAD", "NPCAD", "NPPLEWY", "NPCLEWY", "NPPVASC", "NPCVASC", "NPPFTLD", "NPCFTLD", "NPPHIPP", "NPCHIPP", "NPPPRION", 
"NPCPRION", "NPPADP", "NPCADP", "NPPOTH1", "NPCOTH1", "NPOTH1X", "NPPOTH2", "NPCOTH2", "NPOTH2X", "NPPOTH3", "NPCOTH3", "NPOTH3X")

# now make a version of the original nacc dataset with only the cols from keep_cols
nacc_1 <- nacc %>% 
  select(any_of(keep_cols))
```

## **2. Keeping only cases with 3 longitudinal assessments**

As I am interested in longitudinal data, I will keep only cases that have 3 observations (min and max). For this step, I will group the dataset by ID and then I will use the recurrence pattern of each person's ID as a proxy of the number of visits they have.

```{r}

# first, I will select those with three longitudinal visits
nacc_2 <- nacc_1 %>%
  group_by(NACCID) %>% # group dataset by ID
  filter(n() == 3) # keep IDs that have 3 occurrences (i.e., 3 visits)

# check whether the minimum number of observations retained are 3
nacc_2 %>%
  group_by(NACCID) %>% # group by ID
  summarise(n = sum(n())) %>% # summarise the frequency of each ID
  arrange(n) # arrange in ascending order

```

After this filtering process, there are `r nacc_2 %>% group_by(NACCID) %>% n_distinct()` unique observations remaining in the dataset.

## **3. Recoding diagnosis labels**

All diagnoses are stored as numeric values in respective columns but assigning a diagnostic label is a little complicated (see Narrowing your dataset based on eligibility criteria in <https://naccdata.org/requesting-data/data-request-process#naccHandbook>). This link states the following:

> ***Restricting based on cognitive status and etiologic diagnosis:***
>
> *On the UDS Clinician Diagnosis Form D1, subjects receive a diagnosis corresponding to cognitive status: normal cognition, impaired-not-MCI (mild cognitive impairment), MCI, or demented). Subjects also receive an etiologic diagnosis --- what the clinician suspected to be the cause (whether primary, contributing, or non-contributing) of any cognitive impairment. Both the variable on cognitive status (NACCUDSD) and one or more variables concerning etiologic diagnosis (for example, NACCALZD) will often be required to focus on a specific diagnostic group of interest.*
>
> *For example, subjects with an etiologic diagnosis of Alzheimer's disease (NACCALZD=1) can have a cognitive status of impaired-not-MCI, MCI, or dementia. The only way to focus on those with AD dementia is to use both the cognitive status variable (NACCUDSD = 4) and the etiologic diagnosis variable (NACCALZD = 1).*

Accordingly, lets write some code to assign a new diagnosis variable to each of these patients. We will have to do a few blocks of code for Time 1, Time 2, and Time 3 as sometimes the diagnosis may change between each visit. It will be interesting to see how the diagnosis changes between each of these visits and how that corresponds to performance changes.

I will create 2 new columns = ClinDiagDementia and ClinDiagMCI indicating the presence of dementia or MCI at visit.

```{r}

# Let us recode diagnoses as per the DD-UDS
# any one who does not fit these criteria, I'm coding them as "other Dx" so I can go back to their data and inspect details.

nacc_3 <- nacc_2 %>%
  group_by(NACCID) %>%
  mutate(
    ClinDiagDementia = case_when(   # For dementia diagnosis at all visits
      NACCUDSD == 4 & NACCVNUM == 1 & NACCALZD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & PROBAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & POSSAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & PCA == 1 ~ "PCA",
      NACCUDSD == 4 & NACCVNUM == 1 & NACCLBDE == 1 ~ "DLBDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & PARK == 1 ~ "PDDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & MSA == 1 ~ "MSADementia",
      NACCUDSD == 4 & NACCVNUM == 1 & CORT == 1 ~ "CBDDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & FTLDMO == 1 ~ "FTLD-MNDDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & FTLDNOS == 1 ~ "FTLD-NOSDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & NACCBVFT == 1 ~ "bvFTDDementia",
      NACCUDSD == 4 & NACCVNUM == 1 & PPAPHIF == 1 ~ "PPADementia",
      NACCUDSD == 4 & NACCVNUM == 1 & CVD == 1 ~ "VBIDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & NACCALZD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & PROBAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & POSSAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & PCA == 1 ~ "PCA",
      NACCUDSD == 4 & NACCVNUM == 2 & NACCLBDE == 1 ~ "DLBDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & PARK == 1 ~ "PDDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & MSA == 1 ~ "MSADementia",
      NACCUDSD == 4 & NACCVNUM == 2 & CORT == 1 ~ "CBDDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & FTLDMO == 1 ~ "FTLD-MNDDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & FTLDNOS == 1 ~ "FTLD-NOSDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & FTD == 1 ~ "bvFTDDementia",
      NACCUDSD == 4 & NACCVNUM == 2 & PPAPH == 1 ~ "PPADementia",
      NACCUDSD == 4 & NACCVNUM == 2 & CVD == 1 ~ "VBIDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & NACCALZD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & PROBAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & POSSAD == 1 ~ "ADDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & PCA == 1 ~ "PCA",
      NACCUDSD == 4 & NACCVNUM == 3 & NACCLBDE == 1 ~ "DLBDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & PARK == 1 ~ "PDDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & MSA == 1 ~ "MSADementia",
      NACCUDSD == 4 & NACCVNUM == 3 & CORT == 1 ~ "CBDDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & FTLDMO == 1 ~ "FTLD-MNDDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & FTLDNOS == 1 ~ "FTLD-NOSDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & FTD == 1 ~ "bvFTDDementia",
      NACCUDSD == 4 & NACCVNUM == 3 & PPAPH == 1 ~ "PPADementia",
      NACCUDSD == 4 & NACCVNUM == 3 & CVD == 1 ~ "VBIDementia",
      TRUE ~ "OtherDx"
    )
  ) %>% 
  mutate(
    ClinDiagMCI = case_when(   # For MCI diagnosis at all visits
      NACCUDSD == 3 & NACCVNUM == 1 & NACCALZD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & PROBAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & POSSAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & NACCLBDE == 1 ~ "DLBMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & PARK == 1 ~ "PDMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & MSA == 1 ~ "MSAMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & CORT == 1 ~ "CBDMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & FTLDMO == 1 ~ "FTLD-MNDMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & FTLDNOS == 1 ~ "FTLD-NOSMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & FTD == 1 ~ "bvFTDMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & PPAPH == 1 ~ "PPAMCI",
      NACCUDSD == 3 & NACCVNUM == 1 & CVD == 1 ~ "VBIMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & NACCALZD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & PROBAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & POSSAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & NACCLBDE == 1 ~ "DLBMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & PARK == 1 ~ "PDMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & MSA == 1 ~ "MSAMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & CORT == 1 ~ "CBDMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & FTLDMO == 1 ~ "FTLD-MNDMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & FTLDNOS == 1 ~ "FTLD-NOSMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & FTD == 1 ~ "bvFTDMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & PPAPH == 1 ~ "PPAMCI",
      NACCUDSD == 3 & NACCVNUM == 2 & CVD == 1 ~ "VBIMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & NACCALZD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & PROBAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & POSSAD == 1 ~ "ADMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & NACCLBDE == 1 ~ "DLBMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & PARK == 1 ~ "PDMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & MSA == 1 ~ "MSAMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & CORT == 1 ~ "CBDMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & FTLDMO == 1 ~ "FTLD-MNDMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & FTLDNOS == 1 ~ "FTLD-NOSMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & FTD == 1 ~ "bvFTDMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & PPAPH == 1 ~ "PPAMCI",
      NACCUDSD == 3 & NACCVNUM == 3 & CVD == 1 ~ "VBIMCI",
      TRUE ~ "OtherDx"
    )
  ) %>%
  mutate( # Recoding PPA subdiagnosis at all visits
    ClinDiagDementia = case_when(
      NACCPPAG == 1 ~ "svPPA",
      NACCPPAG == 2 ~ "lvPPA",
      NACCPPAG == 3 ~ "nfvPPA",
      NACCPPAG == 4 ~ "PPANOS",
      TRUE ~ ClinDiagDementia
    )
  ) %>%
  mutate(  # Recoding MCI subdiagnosis at all visits
    ClinDiagMCISubtype = case_when(
      NACCTMCI == 1 ~ "AmnesticSD",
      NACCTMCI == 2 ~ "AmnesticMD",
      NACCTMCI == 3 ~ "NonAmnSD",
      NACCTMCI == 4 ~ "NonAmnMD",
      TRUE ~ "NotMCI"
    )
  )
  
# Lets view the first 100 rows and see what the output looks like
nacc_3 %>%
  select(NACCID, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype) %>%
  head(100)
```

```{r}

# in a new dataset:
# here, I will extract all individuals who have "Other Dx" in both ClinDiagDementia and ClinDiagMCI columns for all three time points and explore their diagnosis codes in more detail. I had some help from stackoverflow to write this code: https://stackoverflow.com/questions/73400912/filter-and-extract-rows-based-on-multiple-conditions

nacc_diag_investigate <- nacc_3 %>%
  group_by(NACCID) %>%
  filter(all(ClinDiagDementia == "OtherDx" & ClinDiagMCI == "OtherDx")) %>%
  ungroup() %>%
  select(NACCID, NACCVNUM, ClinDiagDementia, ClinDiagMCI, NACCUDSD, 422:456)
```

By the looks of this new dataframe, a number of cases have a NACCUDSD code = 1 (Normal cognition) or = 2 (impaired-not-MCI). will remove all cases where NACCUDSD code = 1 or 2 for all time points. At all stages, I will keep checking if the number of rows remaining after filtering is still divisible by 3 as that will tell me if the code has removed one too many/few rows.

```{r}

nacc_diag_investigate_1 <- nacc_diag_investigate %>%
  group_by(NACCID) %>%
  filter(!all(NACCUDSD == 1 | NACCUDSD == 2)) %>% 
  ungroup()
```

In the next step, I will have to remove all missing values from the diagnosis codes as that will help clear the noise from the dataset before I can go on to figuring out the actual diagnoses of these cases. The value that tells us if the diagnosis is present is 1. Therefore, for simplicity, I will replace NA for all:

-   8s (no diagnosis of dementia)

-   -4s (not applicable)

-   0 (No)

-   7s (impaired but no syndromic diagnosis)

-   9s (unknown subtype)

```{r}

# there is probably a cleaner/faster code to write (using the operator !1 rather than listing each number to exclude, but this process helps me examine each iteration of the dataset once I exclude each number)
nacc_diag_investigate_2 <- nacc_diag_investigate_1 %>%
  mutate(across(c(6:40), na_if, -4)) %>% # replace all -4s with NA
  mutate(across(c(6:40), na_if, 8)) %>% # replace all 8s with NA
  mutate(across(c(6:40), na_if, 0)) %>% # replace all 0s with NA
  mutate(across(c(6:40), na_if, 7)) %>% # replace all 7s with NA
  mutate(across(c(6:40), na_if, 9)) # replace all 9s with NA
  
# now, for all columns between 6:40, I remove rows that have NAs across the board. In this code, I tally how many NAs need to be between these column indices (i.e., 40-6 = 34) and tell rowsums to pull out all rows where there are at least 34 NAs and remove them using is.na and !. Note: i tried using dplyr::filter_if but this code isn't working
nacc_diag_investigate_3 <- nacc_diag_investigate_2[rowSums(is.na(nacc_diag_investigate_2[,6:40]))!=34,]

nrow(nacc_diag_investigate_3)
# there are now 160 rows left. 

# From here, I will remove those cases with vascular diagnoses
nacc_diag_investigate_4 <- subset(nacc_diag_investigate_3, !(VASC %in% 1) &
                                    !(VASCPS %in% 1) &
                                    !(CVD %in% 1))
                                
# Many of the remaining cases have PSP, some have AD so let us recode their diagnosis and then we will fix the rest

nacc_diag_investigate_5 <- nacc_diag_investigate_4 %>%
  group_by(NACCID) %>%
  mutate(ClinDiagDementia = case_when( # recode diagnoses
    NACCUDSD == 3 & NACCALZD == 1 ~ "ADMCI", 
    NACCUDSD == 4 & NACCALZD == 1 ~ "ADDementia", 
    NACCUDSD == 3 & PSP == 1 ~ "PSPMCI",
    NACCUDSD == 4 & PSP == 1 ~ "PSPDementia",
    NACCUDSD == 3 & CORT == 1 ~ "CORTMCI",
    NACCUDSD == 4 & CORT == 1 ~ "CORTDementia",
    TRUE ~ ClinDiagDementia # no one emerges with AD/CORT diagnosis as they all have NACCUDSD == 2 which is impaired but not MCI
    )
  ) %>%
  subset(!(ClinDiagDementia %in% "OtherDx")) %>% # remove anyone else who has "OtherDx" as the diagnoses. I checked all their Dx columns and they have no other diagnosis.
  filter(!grepl("NACC577598", NACCID)) # remove NACC577598 as this person has only one time point

# now I'll make a version of the nacc_diag_investigate_5 dataset with only the ID and ClinDiagDementia columns so left_join/merge becomes easier

nacc_diag_investigate_6 <- nacc_diag_investigate_5 %>% 
  select(NACCID, NACCVNUM, ClinDiagDementia)
```

With this dataset, I will match it back to the `nacc_3` dataset and substitute their diagnoses and remove NACC577598.

```{r}

nacc_4 <- nacc_3 %>%
  mutate(index = row_number()) %>% # keep a row number index to facilitate matching
  left_join(nacc_diag_investigate_6, by = c("NACCID", "NACCVNUM"), keep = T) %>% # left_join the nacc diag dataset using NACCiD and visit num as index variables
  mutate(ClinDiagDementia.y = coalesce(ClinDiagDementia.y, ClinDiagDementia.x)) %>% # combine the values from nacc_diag_investigate$ClinDiagDementia with the ClinDiagDementia column of the nacc_3 dataset
  mutate(ClinDiagDementia.y = case_when(
    ClinDiagDementia.y == "OtherDx" ~ NA_character_,
    TRUE ~ ClinDiagDementia.y
    )
    ) %>% # before we can merge the MCI diagnosis back into this column, we need to replace the OtherDx string with NA - this makes it easy for dplyr::coalesce to do its job
  mutate(ClinDiagDementia.y = 
           coalesce(ClinDiagDementia.y, paste(ClinDiagMCI,"-",ClinDiagMCISubtype))) %>% # now for the diagnoses that were labelled as "OtherDx" but had an MCI diag, I paste their respective diagnosis from the ClinDiagMCI and ClinDiagMCISubtype into the ClinDiagDementia.y column
  select(-c(index,NACCID.y, NACCVNUM.y)) %>% # remove the extra columns that were generated during left join
  rename(Group = ClinDiagDementia.y, ClinDiagDementia = ClinDiagDementia.x, NACCVNUM = NACCVNUM.x, NACCID = NACCID.x) %>% # rename these variables and remove the .x/.y suffix
  select(c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), everything()) # reorder the first few columns to bring the diagnosis variables to the front
```

In the next step, I will remove all those cases who have primary contributions from alcohol//drug abuse and other primary psychiatric/neurological illnesses at baseline. For this, I will go through the DD-UDS (section D1) and remove all IDs where there is evidence for primary contribution from psychiatric/alcohol variables.

```{r}

nacc_5 <- nacc_4 %>%
  group_by(NACCID) %>% 
  mutate(ExclNeurolPsychContrib = case_when(
    NACCUDSD == 3 & NACCVNUM == 1 & ALCDEMIF == 1 ~ "Yes", #primary contributions to cogn. impairment from alcohol abuse
    NACCUDSD == 3 & NACCVNUM == 1 & IMPSUBIF == 1 ~ "Yes", #primary contributions to cogn. impairment from substance abuse
    NACCUDSD == 3 & NACCVNUM == 1 & DOWNSIF == 1 ~ "Yes", #primary contributions to cogn. impairment from Downs syndrome
    NACCUDSD == 3 & NACCVNUM == 1 & BRNINJIF == 1 ~ "Yes", #primary contributions to cogn. impairment from brain injury
    NACCUDSD == 3 & NACCVNUM == 1 & HYCEPHIF == 1 ~ "Yes", #primary contributions to cogn. impairment from NPH
    NACCUDSD == 3 & NACCVNUM == 1 & DEPIF == 1 ~ "Yes", #primary contributions to cogn. impairment from depression
    NACCUDSD == 3 & NACCVNUM == 1 & SCHIZOIF == 1 ~ "Yes", #primary contributions to cogn. impairment from schizophrenia
    NACCUDSD == 3 & NACCVNUM == 1 & BIPOLDIF == 1 ~ "Yes", #primary contributions to cogn. impairment from Bipolar Disorder
    NACCUDSD == 3 & NACCVNUM == 1 & DELIRIF == 1 ~ "Yes", #primary contributions to cogn. impairment from Delirium
    TRUE ~ "No"
    )) %>%
   filter(!any(grepl("Yes", ExclNeurolPsychContrib))) # remove all cases that have an occurrence of "Yes" in the ExclNeurolPsychContrib column. This step will remove all those cases that have a history of alcohol/substance abuse or primary psychiatric illnesses.
```

In the next step, I will only keep all variants of AD, bvFTD, MCI, and PPA, PSP and CBS. I will remove all others.

```{r}

nacc_5$Group <- as.factor(nacc_5$Group)

unique(nacc_5$Group) # what are all the diagnoses in this dataset

# count the number of occurrences of any VBI, OtherDx, MSA, and DLB diagnoses
length(grep("VBI", nacc_5$Group)) 
length(grep("Other", nacc_5$Group)) 
length(grep("MSA", nacc_5$Group)) 
length(grep("DLB", nacc_5$Group)) 


nacc_6 <- nacc_5 %>%
  group_by(NACCID) %>% # group by ID
  filter(!any(grepl("VBI|OtherDx|MSA|DLB|MND", Group))) # remove all occurrences of the string VBI, OtherDx, MSA, and DLB from the Group variable. Since I've used a group_by command, this will remove all rows where for any ID, the presence of these strings is there in any of their 3 longitudinal assessments

# lets see if it has worked and if the number of rows is still divisible by 3
unique(nacc_6$Group)
nrow(nacc_6)/3
# divides perfectly.
```

In the next step, I want to see how many patients have their diagnosis changing between each diagnostic assessment. This will help me weed out more patients if need be. For this, lets try one alluvial plot.

```{r}

nacc_6 %>%
  select(NACCID, NACCVNUM, Group) %>% # select only these variables
  ggplot(aes(x = as.factor(NACCVNUM), stratum = Group, # create the layers
             alluvium = NACCID, fill = Group)) +
  geom_flow() + # add the flow
  geom_stratum(alpha = .5) + # modify the transparence of the flow
  theme_bw() +
  theme_classic() +
  xlab("Visit number") +
  guides(fill=guide_legend(title="Group")) 
```

From the above plot, there are too many diagnoses to be able to understand what is happening. So in the next step, I'll create a more generalised diagnosis variable that simply codes people as having AD, MCI (all MCI), bvFTD, PPA (nfvPPA/svPPA/lvPPA) or FTLD (CBD/PSP).

```{r}

nacc_7 <- nacc_6 %>%
  mutate(GroupGeneral = case_when(
    Group == "ADDementia" ~ "AD",
    Group == "PCA" ~ "PCA",
    Group == "bvFTDDementia" ~ "bvFTD",
    Group == "lvPPA" ~ "PPA",
    Group == "nfvPPA" ~ "PPA",
    Group == "svPPA" ~ "PPA",
    Group == "PPANOS" ~ "PPA",
    Group == "PPADementia" ~ "PPA",
    Group == "FTLD-NOSDementia" ~ "FTLD",
    Group == "CBDDementia" ~ "FTLD",
    Group == "PSPDementia" ~ "FTLD",
    TRUE ~ "MCI"
  ))

# Let us examine how many cases in each diagnostic column at time 1
nacc_7 %>%
  group_by(GroupGeneral) %>%
  filter(NACCVNUM == 1) %>%
  summarise(count = n())

nacc_7 %>%
  select(NACCID, NACCVNUM, GroupGeneral) %>% # select only these variables
  ggplot(aes(x = as.factor(NACCVNUM), stratum = GroupGeneral, # create the layers
             alluvium = NACCID, fill = GroupGeneral)) +
  geom_flow() + # add the flow
  geom_stratum(alpha = .5) + # modify the transparence of the flow
  scale_fill_manual(values = colorBlindBlack_palette) +
  theme_bw() +
  theme_classic() +
  xlab("Visit number") +
  guides(fill = guide_legend(title = "Group"))
```

There are just too many AD patients, so I'll try this plot again without any ADs. This will help see the other cases better.

```{r}

nacc_7 %>%
  select(NACCID, NACCVNUM, GroupGeneral) %>% # select only these variables
  filter(!any(GroupGeneral == "AD")) %>%
  ggplot(aes(x = as.factor(NACCVNUM), stratum = GroupGeneral, # create the layers
             alluvium = NACCID, fill = GroupGeneral)) +
  geom_flow() + # add the flow
  geom_stratum(alpha = .5) + # modify the transparence of the flow
  scale_fill_manual(values = colorBlindBlack_palette) +
  theme_bw() +
  theme_classic()  +
  xlab("Visit number") +
  guides(fill = guide_legend(title = "Group"))
```

## **4. Missing data handling (round 1; on full dataset)**

The variables of interest are in section C of DD-UDS. I have gone through the whole DD-UDS using the search term "not available", which identified 438 entries (for 438 columns in DD-UDS that could have NA as a potential response). Going through the output, I noted NAs in NACC-UDS are coded using different codes, including "-4" (not available); "888" or "8" or "88.8" or "8888" (not applicable"/"untestable"/"not assessed"/"not reported"; 88.8 specifically for `HEIGHT`in inches; "8888" specifically for `TBIYEAR` and `NACCTIYR`); and "999" or "9" or "9999" (unknown; "9999" specifically for `TBIYEAR` and `NACCTIYR`). I will do NA removal/substitution in a few steps:

The -4 value is the common denominator for NA across the dataframe. It is often accompanied by the statement "UDS form submitted did not collect data in this way, or a skip pattern precludes response to this question". This means -4s are true NAs and can be substituted for NAs

There are many ways to do handle this missing data but I'm going to rely on the approach used by Qiu et al. 2019, Neurology (<https://n.neurology.org/content/93/8/e778>). They used the NACC dataset and their missing data statement reads: "*Missing test scores in the UDS dataset are designated as missing due to cognitive problems, other noncognitive problems, or not collected at that assessment. For scores missing due to cognitive problems, we imputed the worst possible score for the measure. For scores missing due to noncognitive reasons, we imputed a score using linear regression with predictors age, education, MMSE or MoCA score, CDR Sum of Boxes score, and all available neuropsychological measures, computed in the R package mice.^30^*"

This approach would be ideal but I am unsure how Qui et al. actually did this. The reason being the follows:

i)  there are a lot of -4s in the data. Retaining these would have skewed their linear model predictions. On the other hand, if they turned these to NAs prior to imputing, the software would have also imputed these in addition to the scores for cognitive/physical effort/

ii) a number of variables that are used for imputing (like MMSE etc.) themselves are missing. So the robustness of the predictive model can be questioned.

Therefore, this is my decision. For all cells with the value of 96 (cognitive reason for missing data), I will substitute it with the groupwise/time-wise minimum value. For all cells with missing values for non-cognitive reasons (95, 97; physical reasons for missing data), I will substitute it with the median values by group and time.

Lets handle these missing data step by step.

```{r}

# First, I will create a dataset with only clinical and neuropsychological variables. We no more require data on diagnosis etc., as all of those have been calculated already.

nacc_8 <- nacc_7 %>%
  select(1:48, # demographics and other clinical details 
         87:149, # CDR, NPI and GDS
         150:196, # clinical assessment of neurological fns. (Categorical)
         197:210, # clinical assessment of cogn. fns. (Categorical)
         212:253, # clinical assessment of cogn./neurol. fns. (Categ. & continuous)
         256:389 # cognitive/neuropsych assessment
         )
```

***Note:*** If "No meaningful changes" (B9CHG = 1) was selected, the clinician did not complete the rest of the form. (p202- DD-UDS)

```{r}

# Now, I'll write some code to keep replacing NAs, according to the DD-UDS for each variable. For this, the best way is to stagger a bunch of code to first pick out the common values across the entire dataframe and then the unique ones, based on each variables unique values.

nacc_9 <- nacc_8 %>% 
  # first I replace -4, 888, 8888, 999, 9999 from the entire dataset with NA
  mutate(across(everything(), ~replace(., . %in% c(-4, 888, 8888, 999, 9999), NA))) %>%
  # for the genetic variables, I replace 9s with NA
  mutate(across(35:48, ~replace(., . %in% 9, NA))) %>%
  # for the CBI/NPI/GDS variables, I replace 9s with NA
  mutate(across(61:100, ~replace(., . %in% c(9), NA))) %>%
  # for total GDS score, substitute 88 with NA
  mutate(NACCGDS = replace(NACCGDS, NACCGDS == 88, NA)) %>%
  # for NACC functional assessment scale, replace 9 (Unknown) and 8 (Not applicable) with NA
  mutate(across(102:111, ~replace(., . %in% c(8, 9), NA))) %>%
  # for clinical examination columns, replace 8 and 9 with NA
  mutate(across(112:158, ~replace(., . %in% c(8, 9), NA))) %>%
  # for clinical judgment of symptoms, replace 8 (could not be assessed) and 9 with NA
  mutate(across(159:172,  ~replace(., . %in% c(8, 9), NA))) %>%
  # for NACC- reported clinical variables, replace 99 (unknown) with NA
  mutate(across(173:175, ~replace(., . %in% 99, NA))) %>%
  # for clinical reports of mood, replace 9 with NA
  mutate(across(179:194, ~replace(., . %in% 9, NA))) %>%
  # for first behavioural symptom and mode of onset of behv sx, replace 99 with NA
  mutate(NACCBEHF = replace(NACCBEHF, NACCBEHF == 99, NA)) %>%
  mutate(BEMODE = replace(BEMODE, BEMODE == 99, NA)) %>%
  # for motor and other clinical symptoms, replace 9 and 99 with NA
  mutate(across(201:208, ~replace(., . %in% c(9, 99), NA))) %>%
  mutate(across(210:214, ~replace(., . %in% c(8, 9, 99), NA))) %>%
  # for MMSE and neuropsych tests, replace 88 with NA
  mutate(across(228:243, ~replace(., . %in% c(88, 99), NA))) %>%
  # for MOCA, replace 88 with NA
  mutate(across(292:293, ~replace(., .%in% 88, NA))) %>%
  # for MOCA and Rey, replace 88/99 with NA
  mutate(across(312:325, ~replace(., . %in% c(88, 99), NA))) %>%
  # for Rey, replace 88 with NA
  mutate(across(332:347, ~replace(., . %in% 88, NA)))
```

Now that I have substituted most missing data codes with NA, there are a few more test-specific changes that need to be made. For example, for the NPI, BEApathy etc, a score of 8 or 9 means not applicable (i.e., as per DD-UDS, this is stored as symptom not reported) so we can safely convert this to 0. For the GDS, a score of 9 means did not answer so this can be turned into NA. For MMSE/Logical Memory/Trails/Cogstat and all other neuropsych test, there are a number of codes related to refusal to participate so we can code these as NA as well.

```{r}

nacc_10 <- nacc_9 %>%
  # For NPI Del, I seem to have missed replacing score of 9 with NA
  mutate(DEL = replace(DEL, DEL==9, NA)) %>%
  # for NPI all variables, replace score of 8 (no symptom reported) with 0
  mutate(across(61:84, ~replace(., . %in% 8, 0))) %>%
  # for NACC Functional Assessment Scale, replace 8 (Not Applicable) and 9 (Unknown) with 0
  mutate(across(102:111, ~replace(., . %in% c(8, 9), 0))) %>%
  # For NACC Clinician Judgment of Symptoms - BE Apathy symptoms, replace 9 (unknown) with NA 
  mutate(BEAPATHY = replace(BEAPATHY, BEAPATHY == 9, NA)) %>%
  # for MOMOPARK and MOMOALS, replace 8 (no changes) with 0
  mutate(MOMOPARK = replace(MOMOPARK, MOMOPARK == 8, 0)) %>%
  mutate(MOMOALS = replace(MOMOALS, MOMOALS == 8, 0))

```

I will have to do another round of test-specific changes based on patterns of missing data due to low engagement/physical effort etc. Its best to do this when I'm pruning the dataframe even further, after selecting the groups I want to retain.

There are still tonnes of variables in this dataset. To make it easy for missing data culling and imputation, I will now create a version of this dataset with only the tests of interest. I will unselect many of the test meta-information and other qualitative variables (most of which are again, test meta-info).

```{r}

# There is a fast way to do this but it involves a long code. I can use dput to output the full names of all columns in a vector format, then copy that list from the console to this markdown file, delete the variables that I do not want to keep, save it as a vector and call that into dplyr::select. This is a faster way to manually delete the columns I don't want to keep, rather than typing out each column name.

nacc_10_allcols <- dput(names(nacc_10))

# Copying dput output into the markdown file and deleting ones that I don't want. I will write another line of code to show the columns that I deleted.

nacc_10_keeps <- c("NACCID", "NACCVNUM", "Group", "ClinDiagDementia", "ClinDiagMCI", "ClinDiagMCISubtype", "NACCADC", "NACCUDSD", "NACCDAYS", 
"NACCFDYS", "NACCDIED", "NACCMOD", "NACCYOD", "BIRTHYR", 
"SEX", "EDUC", "RACE", "PRIMLANG", "INDEPEND", "HANDED", 
"NACCAGEB", "NACCAGE",  "NORMCOG", "NACCFAM", "NACCMOM", "NACCDAD", "NACCAM", "NACCAMS", "NACCFM", "NACCFMS", "NACCOM",  "NACCFADM", 
"NACCFFTD", "MEMORY", "ORIENT", "JUDGMENT", "COMMUN", "HOMEHOBB", 
"PERSCARE", "CDRSUM", "CDRGLOB", "COMPORT", "CDRLANG", "DEL", "DELSEV", "HALL", "HALLSEV", "AGIT", "AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", "ELATSEV", "APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", "MOT", "MOTSEV", 
"NITE", "NITESEV", "APP", "APPSEV", "SATIS", "DROPACT", "EMPTY", "BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "NACCGDS", "BILLS", "TAXES", "SHOPPING", "GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", "TRAVEL", "FOCLDEF", "GAITDIS", "EYEMOVE", "PARKSIGN", "RESTTRL", "RESTTRR", "SLOWINGL", "SLOWINGR", "RIGIDL", "RIGIDR", "BRADY", "PARKGAIT", "POSTINST", "CVDSIGNS", "CORTDEF", "SIVDFIND", "CVDMOTL", "CVDMOTR", "CORTVISL", "CORTVISR", "SOMATL", "SOMATR", "POSTCORT", "PSPCBS", "EYEPSP", "DYSPSP", "AXIALPSP", "GAITPSP", "APRAXSP", "APRAXL", "APRAXR", "CORTSENL", "CORTSENR", "ATAXL", "ATAXR", "ALIENLML", "ALIENLMR", "DYSTONL", "DYSTONR", "MYOCLLT", "MYOCLRT", "ALSFIND", "GAITNPH", "OTHNEUR", "B9CHG", "DECSUB", "DECIN", "DECCLIN", "DECCLCOG", "COGMEM", "COGORI", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", "COGFLAGO", "COGOTHR", "NACCCOGF", "COGMODE", "DECAGE", "DECCLBE", "BEAPATHY",
"BEDEP", "BEVHALL", "BEVWELL", "BEVHAGO", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEREMAGO", "BEANX", "BEOTHR", "NACCBEHF", "BEMODE", "BEAGE", "DECCLMOT", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW", "NACCMOTF", "MOMODE", "MOMOPARK", "PARKAGE", "MOMOALS", "ALSAGE", "MOAGE", "COURSE", "FRSTCHG", "MMSEORDA", "MMSEORLO", "PENTAGON", "NACCMMSE", "LOGIMEM", "MEMUNITS", "UDSBENTC", "UDSBENTD", "UDSBENRS", "DIGIF", "DIGIB", "ANIMALS", "VEG", "TRAILA", "TRAILARR",
"TRAILALI", "TRAILB", "TRAILBRR", "TRAILBLI", "WAIS", "BOSTON", "UDSVERFC", "UDSVERFN", "UDSVERNF", "UDSVERLC", "UDSVERLR", "UDSVERLN",
"UDSVERTN", "UDSVERTE", "UDSVERTI", "COGSTAT", "MOCATOTS", "MOCATRAI", "MOCACUBE", "MOCACLOC", "MOCACLON", "MOCACLOH", "MOCANAMI", "MOCAREGI", "MOCADIGI", "MOCALETT", "MOCASER7", "MOCAREPE", "MOCAFLUE", "MOCAABST", "MOCARECN", "MOCARECC", "MOCARECR", "MOCAORDT", "MOCAORMO", "MOCAORYR", "MOCAORDY", "MOCAORPL", "MOCAORCT", "NACCMOCA", "MOCBTOTS", "NACCMOCB", "CRAFTVRS", "CRAFTURS", "DIGFORCT", "DIGBACCT", "CRAFTDVR", "CRAFTDRE", "CRAFTDTI", "CRAFTCUE", "MINTTOTS", "MINTTOTW", "MINTSCNG", "MINTSCNC", "MINTPCNG", "MINTPCNC", "REY1REC", "REY1INT", "REY2REC", "REY2INT", "REY3REC", "REY3INT", "REY4REC", "REY4INT", "REY5REC", "REY5INT", "REY6REC", "REY6INT", "OTRAILA", "OTRLARR", "OTRLALI", "OTRAILB", "OTRLBRR", "OTRLBLI", "REYDREC", "REYDINT", "REYTCOR", "REYFPOS", "VNTTOTW", "VNTPCNC")

# Here are all the columns I have removed
setdiff(nacc_10_allcols, nacc_10_keeps)


# I will call save these selected columns in a new df and call it voi = variables of interest
nacc_voi <- nacc_10 %>%
  select(c(nacc_10_keeps))
```

## **5. Retaining diagnostic groups of interest**

Now that I've recoded the diagnoses, kept variables of interest, and understood the extent of missing data, before I prune out tests and cases who have missing data, I will keep the diagnostic labels are relevant to this study. First let us see how many cases we have per group in this dataset at Time 1.

```{r}

print(nacc_voi %>% 
        filter (NACCVNUM == 1) %>% 
        group_by(Group) %>% 
        summarise(n = n()), 
      n=28)
```

Looks like there are just a handful of PCA, PPA, PSP Dementia etc. I seem to have lost the GroupGeneral variable that I made earlier so let me recode it.

```{r}
print(nacc_voi %>% 
        filter (NACCVNUM == 1) %>% 
        mutate(GroupGeneral = case_when(
          Group == "ADDementia" ~ "AD",
          Group == "PCA" ~ "PCA",
          Group == "bvFTDDementia" ~ "bvFTD",
          Group == "lvPPA" ~ "PPA",
          Group == "nfvPPA" ~ "PPA",
          Group == "svPPA" ~ "PPA",
          Group == "PPANOS" ~ "PPA",
          Group == "PPADementia" ~ "PPA",
          Group == "FTLD-NOSDementia" ~ "FTLD",
          Group == "CBDDementia" ~ "FTLD",
          Group == "PSPDementia" ~ "FTLD",
          TRUE ~ "MCI"
          )) %>%
        group_by(GroupGeneral) %>% 
        summarise(n = n()), 
      n=28)
```

Now, I'll create a new dataset without MCI.

```{r}

nacc_voi_dem <- nacc_voi %>%
  filter(!str_detect(Group, "MCI"))
```

I will take this dataframe forward.

## **6. Plot amount of missing data and prune IDs/tests with lots of missing data (only T1)**

For this step, my typical strategy for knowing which tests and patients to remove is to first make a raster/tile plot. Then, I first cull horizontally, i.e., remove tests that most patients have missing data on. Then I cull vertically, i.e., remove patients who have missing data on many tests. This is done in a supervised, iterative manner.

First, I will simply calculate the percentage of missing data for every column and plot that. I will only do this for Time 1.

```{r}

nacc_voi_dem %>%
  # choose only observations from Time 1
  filter(NACCVNUM == 1) %>%
  # calculate mean missing data for each column
  map(~ mean(is.na(.))) %>%
  # turn this into a data frame
  as.data.frame() %>%
  # reshape into long
  pivot_longer(everything(),
               names_to = "var_name",
               values_to = "Missing_value") %>%
  # add a variable number
  mutate(var_num = row_number()) %>%
  # label variables as being neuropsych or clinical
  mutate(var_categ = case_when(
    str_detect(var_name, "LOGI|MMSE|PENTA|UDS|DIGI|TRAIL|VEG|ANIM|MOC|MINT|REY|OTR|VNT") ~ "NPsy",
        TRUE ~ "Clin"
    )) %>%
  # recode NACCUDSD variable as clinical as it has been assigned as neuropsych based on string detect function above
  mutate(var_categ = case_when(
    var_name == "NACCUDSD" ~ "Clin",
    TRUE ~ var_categ
  )) %>%
  # feed into ggplot
  ggplot(., aes(x = Missing_value, y = var_num, colour = var_categ)) +
  geom_point(aes(colour = var_categ)) +
  # label the neuropsych variable with less than 50% missing data
  geom_text_repel(aes(label = var_name),
                  data = ~ subset(., var_categ %in% "NPsy" & Missing_value < .5), 
                  max.overlaps = Inf)
```

Next, I will plot a raster/tile plot to examine how much missing data there is for each ID across all variables. I will do this only for time 1 data as if there is no Time1 data, I don't really think I can keep these people on for the rest of the analyses.

```{r}

nacc_voi_dem %>%
  # filter only Time 1 data
  filter(NACCVNUM == 1) %>% 
  # reshape the data to long for ggplot
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), names_to = "Tests", values_to = "Value") %>% 
  # create index of missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # feed into ggplot
  ggplot(aes(x = NACCID, y = Tests, fill = Missing)) +
  geom_raster() +
  theme(axis.text.y = element_text(size=6)) +
  scale_color_manual(values = c("black", "white"))
```

Ok so this looks like a mess! To make more sense of this missing data, I will inspect all columns that have \>70% missing data.

```{r}

nacc_voi_dem %>%
  # filter only Time 1 data
  filter(NACCVNUM == 1) %>% 
  # select columns with >70% missing data
  select(which(colMeans(is.na(.)) > 0.7)) %>%
   # reshape the data to long for ggplot = group only by NACCID
  pivot_longer(!NACCID, names_to = "Tests", values_to = "Value") %>% 
  # create index of missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # feed into ggplot
  ggplot(aes(x = NACCID, y = Tests, fill = Missing)) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6))
```

For neuropsych variables, one of the patterns that stands out is that \>70% of cases don't have the Rey, Oral Trails, Craft, MOCA, UDS Benton and verbal fluency, VNT, MINT. I can have a detailed look at these tests by using `dplyr::select` to look just at them.

```{r}

nacc_voi_dem %>%
  # filter only Time 1 data
  filter(NACCVNUM == 1) %>%
  # select tests of interest
  select(starts_with(c("NACCID", "NACCVNUM", "Group", "Clin", "REY", "OTR", "MOCA", "CRAFT", "MINT", "UDS", "VNT"))) %>%
  # reshape the data to long for ggplot
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), names_to = "Tests", values_to = "Value") %>% 
  # create index of missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # feed into ggplot
  ggplot(aes(x = NACCID, y = Tests, fill = Missing)) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6))
```

Looks like MOCA, CRAFT and UDSVER have some data. Can we salvage it? I need to see how many people have data on these variables.

```{r}

# for UDSVER
(sum(!is.na(nacc_voi_dem$UDSVERFC))/length(nacc_voi_dem$UDSVERFC))*100

# for MOCATRAI (I can choose any MOCA variable)
(sum(!is.na(nacc_voi_dem$MOCATRAI))/length(nacc_voi_dem$MOCATRAI))*100

# for MOCATRAI (I can choose any MOCA variable)
(sum(!is.na(nacc_voi_dem$CRAFTVRS))/length(nacc_voi_dem$CRAFTVRS))*100
```

Only \~20% of participants at T1 have this data so I don't think we can keep these variables. Lets try removing those three tests.

```{r}

# remove tests that are not 
nacc_voi_dem %>%
  # filter only Time 1 data
  filter(NACCVNUM == 1) %>%
  # remove Rey, oral trails and MOCA 
  select(-starts_with(c("REY", "OTR", "MOCA", "CRAFT", "MINT", "UDS", "VNT"))) %>%
  # reshape the data to long for ggplot
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), names_to = "Tests", values_to = "Value") %>% 
  # create index of missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # feed into ggplot
  ggplot(aes(x = NACCID, y = Tests, fill = Missing)) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6))
```

Much better! I need to zoom into this plot more to understand which variables have the most missing data. For that, one quick way is to use `dplyr::select` to zoom into columns starting with the letters A, C, P, R, S, U as from looking at the y-axis of the plot, these variables seem to have most missing data.

```{r}

nacc_voi_dem %>%
  # filter only Time 1 data
  filter(NACCVNUM == 1) %>%
  # remove Rey, oral trails and MOCA 
  select(-starts_with(c("REY", "OTR", "MOCA", "CRAFT", "MINT", "UDSVE", "VNT"))) %>%
  select(starts_with(c("NACCID", "NACCVNUM", "Group", "Clin", "A", "C", "P", "R", "S", "U"))) %>%
  # reshape the data to long for ggplot
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), names_to = "Tests", values_to = "Value") %>% 
  # create index of missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # feed into ggplot
  ggplot(aes(x = NACCID, y = Tests, fill = Missing)) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6))
```

So it looks like most of the missing data is for the clinical variables.

## **7. Plot how many people are missing data across all three time points.**

Now, let us see % of missing data over each time period, divided by group. Note that, if I plot this using `ggalluvium,` this is a lot of data to plot, as x = each visit; y = each ID, `alluvium`/`stratum` will include whether data is missing or not for each test and `facet_grid` will include each diagnostic group. The easiest to do is to simply plot bar plots for each diagnostic group.

```{r}

nacc_voi_dem %>%
   # remove Rey, oral trails and MOCA 
  select(-starts_with(c("REY", "OTR", "MOCA", "CRAFT", "MINT", "UDSVE", "VNT"))) %>%
  # reshape the data to long for ggplot
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), names_to = "Tests", values_to = "Value") %>% 
  # Calculate missingness
  mutate(Missing = ifelse(
    is.na(Value), "Missing", "Present")) %>%
  # calculate % of missing data per time point per patient
  group_by(NACCID, Group, NACCVNUM) %>%
    summarise(total = n(),
            missing = sum(is.na(Value)),
            percentage_missing = (missing/total)*100) %>%
  # plot this value
  ggplot(., aes(x = NACCVNUM, y = percentage_missing, colour = percentage_missing, fill = percentage_missing)) +
    geom_bar(aes(colour = percentage_missing, fill = percentage_missing), stat = "identity", position = 'dodge') +
  # facet by Group
  facet_grid(~Group)
```

## **8. Filter out tests that don't have longitudinal data**

As this is a longitudinal project, it is important to retain tests that have longitudinal data. To understand the pattern of missing-ness better, I will first tally how many cases have missing data for all longitudinal time points, per test. This will further help me understand the number of cases missing tests across all time points.

```{r}

nacc_voi_dem %>%
  # reshape into long df
   pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID, visit number and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  summarise(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = missingamt)) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))
  
```

## **9. Check whether missing data are greater in some specific diagnostic groups**

It is important to check if the missing data are greater in one/two specific diagnostic group than the whole group. For this, a couple of wrangling steps are needed. One step is to simply add a `ggplot` `facet_grid` layer on to the data and facet by Group and examine the plots. Lets try that.

```{r}

nacc_voi_dem %>%
  # reshape into long df
   pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID, visit number and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = missingamt)) +
  geom_raster() +
  facet_wrap(~Group) +
  theme(axis.text.y = element_text(size = 6)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))
```

A variation of this code is to simply tally the missingamt variable by group as a percentage of the sample size of each group. We can also try to plot that value.

```{r}

nacc_voi_dem %>%
  # reshape into long df
   pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID, visit number and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ungroup() %>%
  # add in the grouping variables
  group_by(Group, Tests, missingamt) %>%
  summarise (n = n()) %>%
  # summarise how much missing data there are
  mutate(missingamtpercent = (n / sum(n)) * 100)

```

## **10. Decide which group you want to keep and focus the attention of the project on**

With so much missing data everywhere, this project has to be partially informed by where the data are available. At this point, I need to focus on which group I want to center the project on. One focus of the project can be looking at divergence of cognitive profiles in all patients, combining FTD, AD and PPA together. Another focus can be on typical AD whereas yet another can be on PPAs. For this, first let us look at what the missing data looks like in each of these groups.

### i. Meta

At this point, it is a good idea to create a meta-dataset. What this meta-dataset will contain will be just the demoraphics, age etc. that we can later left_join into the final datasaet that we will be using.

```{r}

nacc_voi_dem_meta <- nacc_voi_dem %>%
  # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) %>%
  ungroup() %>%
  select(1:33, NACCBEHF, B9CHG) %>%
  # calculate Age at death
  mutate(AgeDeath = NACCYOD - BIRTHYR)

# i Need visit year which is in nacc_9. So lets create a new df of just that variable with visit number and ID

nacc_9_meta <- nacc_9 %>%
   # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) %>%
  # select vars
  select(NACCID, NACCVNUM, VISITYR, CDRSUM, CDRGLOB) %>%
  ungroup()

# Finally, I need year permanently moved to nursing home which is in original nacc dataset. Lets mvoe that too

nacc_nursing_meta <- nacc %>%
   # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) %>%
  # select vars
  select(NACCID, NACCVNUM, NACCNRYR) %>%
  ungroup()
```

### ii. All groups

First, I will keep those patients with at least 3 time point data.

```{r}

nacc_voi_dem_all <- nacc_voi_dem %>%
  # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) %>%
  ungroup()

# check how many rows this leads to. This should be divisible by 3 now
nacc_voi_dem_all %>% nrow()/3

# now let us see how what the data look like if we chuck out tests that have over 70% missing data
nacc_voi_dem_all_2 <- nacc_voi_dem_all %>%
  # keep all columns where mean of missing data is less than 30% 
  select(where(~mean(is.na(.)) < .30))

# from this dataset, remove all rows that have at least 70% missing data
nacc_voi_dem_all_3 <- nacc_voi_dem_all_2[rowMeans(is.na(nacc_voi_dem_all_2)) < .3, ]

# again run the data through the pipeline to keep only those IDs with at least 3 observations
nacc_voi_dem_all_4 <- nacc_voi_dem_all_3 %>% 
  # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) %>%
  ungroup()

# look at what the missingness plot looks like.
nacc_voi_dem_all_4 %>%
     # reshape into long df
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = as.factor(missingamt))) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))
```

### **iii. In AD**

I will plot the missingness plot from above but focusing only on AD.

```{r}

nacc_voi_dem %>%
  filter(Group == "ADDementia") %>%
  # reshape into long df
   pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ungroup() %>%
  # add in the grouping variables
  group_by(Tests, missingamt) %>%
  summarise (n = n()) %>%
  # summarise how much missing data there are
  mutate(missingamtpercent = (n / sum(n)) * 100) %>%
  ungroup()

```

Now its time to see how many people can we keep in this dataset if we remove those tests that have tonnes of missing data

```{r}

# first let us see how many people we have in this AD dataset

nacc_voi_dem %>%
  filter(Group == "ADDementia") %>%
  nrow()/3

# looks like there are a couple of cases for whom one timepoint may have been removed. These will likely have been MCI to begin with, so I may have removed them in an earlier iteration of data cleaning. We can safely remove them from this dataset

nacc_voi_dem_AD <- nacc_voi_dem %>%
  # keep only those with AD dementia diagnosis
  filter(Group == "ADDementia") %>%
  # group by ID
  group_by(NACCID) %>%
  # keep only those IDs that repeat 3 times
  filter(n() == 3) 

# check how many rows this leads to. This should be divisible by 3 now
nacc_voi_dem_AD %>% nrow()/3

# now let us see how what the data look like if we chuck out tests that have over 70% missing data
nacc_voi_dem_AD %>%
  # select all columns where mean of missing data is less than 30% 
  select(where(~mean(is.na(.)) < .3)) %>%
  # reshape into long df
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = as.factor(missingamt))) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))
```

Lets now make a new df chucking out test that have missing data across the board and IDs that have a lot of missing data.

```{r}

# save into a new df
nacc_voi_dem_AD_1 <- nacc_voi_dem_AD %>%
  # remove tests that have 70% missing data
  select(where(~mean(is.na(.)) < .3)) %>%
  # reshape into long df
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID
  group_by(NACCID) %>%
  # keep only IDs who have a mean missing % of less than 30%
  filter(mean(is.na(Value)) < .3)

# lets check if this step worked
# first lets see who is the first ID in this df
nacc_voi_dem_AD_1[1,1]

# now for this ID, lets see how much % missing data they have
nacc_voi_dem_AD_1 %>%
  filter(NACCID == "NACC051179") %>%
  summarise(mean(is.na(Value)))
# 19% missing = not bad

# now lets look at the pattern of missing data in this group
nacc_voi_dem_AD_1 %>%
   # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = as.factor(missingamt))) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))

# lets make a version of this dataset
nacc_voi_dem_AD_2 <- nacc_voi_dem_AD %>%
  # remove tests that have 70% missing data
  select(where(~mean(is.na(.)) < .3))
```

### **iv. In PPA**

It is important to remember that some patients would not have had a diagnosis of PPA in the beginning but later will. For example, NACC959301 has a diagnosis of CBD at baseline and then lvPPA at the next two followups. By filtering out the string "PPA", this may lead to exclusion of many people. Therefore, the best thing to do is to (i) group the data by NACCID and NACCVNUM, and then retain any person who may have had a diagnosis of PPA at one of their time points, OR (ii) filter all IDs with even one occurrence of the string "PPA" in Group, save these IDs as a new vector, and subset them from the nacc_voi_dem datasheet (my preference).

```{r}

# select all IDs with even one occurrence of the string "PPA" in their Group column

nacc_PPA_ID_keeps <- nacc_voi_dem %>%
  # group by ID
  group_by(NACCID) %>%
  # filter any occurrence of PPA in the grouping variable (irrespective of NACCVNUM)
  filter(str_detect(Group, "PPA")) %>%
  # select the ID column
  select(NACCID) %>%
  # extract all unique occurrences of each ID (i.e., pull each ID number out that remains after this subsetting and save it into a new vector)
  unique() %>%
  as.data.frame()

# from nacc_voi_dem, keep rows that match the nacc_PPA_ID_keeps vector using dplyr::semi_join 
nacc_voi_dem_PPA <- semi_join(nacc_voi_dem, nacc_PPA_ID_keeps, by = "NACCID")
```

In the new df nacc_voi_dem_PPA, some individuals have only 2 time point data (e.g., NACC559440). I looked at their observations from the nacc_voi df and turns out, they were MCI to start with. When I removed all MCIs in my earlier steps, I had therefore removed their baseline datapoint. Since I'm not keeping MCIs in this study, I can safely remove all those who don't have at least 3 time points

```{r}

# select out all those who don't have 3 time points
nacc_voi_dem_PPA_1 <- nacc_voi_dem_PPA %>%
# group by ID
  group_by(NACCID) %>%
  # keep only those IDs that have at least 3 observations
  filter(n() == 3)

# do the total number of observations in nacc_voi_dem_PPA_1 divide by 3?
nrow(nacc_voi_dem_PPA_1)/3
# nice!

# now I will see how much missing data this whole PPA group has

nacc_voi_dem_PPA_1 %>%
  # reshape into long df
   pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = as.factor(missingamt))) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))
```

Now, lets remove the tests that most participants are missing and see how many individuals we are left with. I will set a threshold of 70% and keep only columns where at less than 30% have missing data.

```{r}

nacc_voi_dem_PPA_1 %>%
  # select all columns where the mean of missing data is less than 30%
  select(where(~mean(is.na(.)) < .3)) %>%
  # reshape into long df
  pivot_longer(!c(NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype), 
     names_to = "Tests", 
     values_to = "Value") %>%
  # group by ID and test
  group_by(NACCID, Tests) %>%
  # tell dplyr to return a new variable called missing amount, where if the person has NA for more than 2 values of each test (i.e., more than two assessments), it is coded as "missing three"; "missing two" for if they have more than one missing value across assessments (as 3 missing will have already been coded in the line before); "missing one" is they have one missing value; and "missing none" if none of these conditions are met.
  # see below one small change: in the previous version of this code (above) I had used summarise() instead of mutate(). Summarise does not allow you to recover the dataset, which means I cannot pipe in a grouping function for plotting after that. On the other hand, mutate() allows me to do that (essentially does the same thing as summarise) so I've changed summarise() to mutate() below.
  mutate(missingamt = case_when(
    sum(is.na(Value)) == 3 ~ "MissingThree",
    sum(is.na(Value)) == 2 ~ "MissingTwo",
    sum(is.na(Value)) == 1 ~ "MissingOne",
    sum(is.na(Value)) == 0 ~ "MissingNone"
  )) %>% # note that this step takes quite some time to run as I have grouped by three variables. It is computationally intensive.
  # I can plot this using a geom_raster plot.
  ggplot(., aes(x = NACCID, y = Tests, fill = as.factor(missingamt))) +
  geom_raster() +
  theme(axis.text.y = element_text(size = 6)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(values = c("#FFF7FB", # missing none
                               "#A6BDDB", # missing one
                               "#034E7B", # missing three
                               "#3690C0" # missing two
                               ))

# lets make a version of this dataset and save it
nacc_voi_dem_PPA_2 <- nacc_voi_dem_PPA_1 %>%
  ungroup() %>%
  # select all columns where the mean of missing data is less than 30%
  select(where(~mean(is.na(.)) < .3))
```

## 11. Missing data handling (round 2; on culled dataset)

Given the amount of missing data, I will have to impute these missing values while grouping the data by visit number. Normally, I would have used a data-driven method like probabilistic PCA (like in my earlier papers), but the data are a mix of categorical and continuous so I cannot impute using PPCA as it will generate a decimal value for variables that are categorised as No(0) or Yes (1). So I will use [MICE](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/) to impute (using `mice` in R).

Some musings on using `mice` to impute: The most important thing to do when using `mice` to impute is to set the predictor matrix. This is a matrix that tells that algorithm what all vars you want to use to predict the missing data. Sometimes, there may be two variables, that you are not interested in using as predictors, but that are collinear/highly correlated. If you don't set these vars to a non-predictor status, then `mice` will not know what to do with them and will continue to spit out NAs for these vars as `mice` is not good with handling highly collinear variables. See for example, [this thread](https://stackoverflow.com/questions/64359275/r-imputation-with-mice).

In my case, I will be setting the ***following vars as predictors of missing data***: Age, Sex, Education, Group, Time of assessment and CDR SOB.

Now, in each dataset, I need to run another round of missing data imputation. A number of tests have scores coded as 95 (physical problem), 96 (cognitive/behaviour problem), 97 (other problem) and 98 (verbal refusal). For all cells with the value of 96, I will substitute it with the groupwise/time-wise minimum value. For all cells with missing values for non-cognitive reasons (95, 97), I will substitute it with the median values by group and time.

### i. For all groups

There is a bit of preparation of the data needed before imputing it. First, I have to deal with the 95, 96 etc. and then create six versions of the dataset, one for each time point.

```{r}

# I'm thinking that since i've already culled so many variables down, there's no harm in simply now replacing all missings with NA. So I'm thinking of now replacing all 995, 996 etc with NA. Then split the data into three time points, run a MICE at each time point, one separately for linear data and one separately for binary data, then put them altogether (see https://stackoverflow.com/questions/57390643/imputing-based-on-specific-columns)

# is there a 95 or 995 in any column in this dataset?
which(nacc_voi_dem_all_4 == c("95", "995"), arr.ind = T)
# there are quite a few 95s & 995s. For these, I'll impute the lowest score


# Next, let us replace all 96, 996 etc. with NA. I will impute in these variables in the next step
nacc_voi_dem_all_5 <- nacc_voi_dem_all_4 %>%
  group_by(Group, NACCVNUM) %>%
  # group data by Dx and visit number
  # substitute all 96 and 996 with NA so we can impute them
  mutate(across(
    everything(), 
    ~replace(., . %in% c(96, 97, 98, 99, 996, 997, 998, 999), 
             NA)
    )) %>% 
# for trails, replace with 300  
  mutate(across(
    TRAILA:TRAILBRR, 
    ~replace(., . %in% c(995), 
             300)
    )) %>%
  # for trails ARR:BRR, replace with 
  mutate(across(
    TRAILA:TRAILBRR, 
    ~replace(., . %in% c(95), 
             40)
    )) %>%
  # for everything else, replace 95 and 995 wiht the minimum value
  mutate(across(
    everything(), 
    ~ case_when(. == 95 ~ min(.),
      . == 995 ~ min(.),
      TRUE ~ .))) %>%
  #replace 9 in MOGAIT with NA
  replace_with_na_at(.vars = c("MOGAIT"), 
                     condition = ~.x == 9) %>%
  ungroup()

```

I will now impute missing data using predictive mean matching (pmm) approach in `mice`. Note that I will use Age, Education, Sex, Group, NACCVNUM and CDR SOB as predictive variables. Also note that, I previously ran versions of this imputation by dividing up the patient group into different time points (NACCVNUM = 1, 2 etc.), then splitting the data into binary and continuous variables, then running imputation via logistic regression on binary vars and pmm on continuous variables, then merging the data together. What I noticed was that, once the predictors of your imputation function are set, then there is hardly any difference in splitting the data and running logistic/pmm vs. running pmm alone on the entire dataset. Pmm handles binary vars by imputing binary values and continuous vars by imputing continuous values. The code for my previous approach is in the extra code section of this document.

```{r}

# first, I will have to set the predictor matrix. There is a bit of circularity to this step as to extract the predictor matrix, you need to have run MICE. So I will run MICE with 0 iterations.

test <- mice(nacc_voi_dem_all_5,
                     maxit = 0)

pred_mat_all <- test$predictorMatrix

dput(colnames(pred_mat_all))

# now copy all colnames except the six predictors set above, and set the remaining colnames to 0 (not predictors) leaving the six predictors as set to 1 (predictors)

pred_mat_all[, c("NACCID", "ClinDiagDementia", "ClinDiagMCI", 
"ClinDiagMCISubtype", "NACCADC", "NACCUDSD", "NACCDAYS", "NACCFDYS", 
"NACCDIED", "NACCMOD", "BIRTHYR", "RACE", "PRIMLANG", 
"INDEPEND", "HANDED", "NACCAGEB", "NORMCOG", "NACCFAM", 
"NACCMOM", "NACCDAD", "NACCFADM", "NACCFFTD", "MEMORY", "ORIENT", 
"JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", "CDRGLOB", 
"COMPORT", "CDRLANG", "DEL", "DELSEV", "HALL", "HALLSEV", "AGIT", 
"AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", "ELATSEV", 
"APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", "MOT", "MOTSEV", 
"NITE", "NITESEV", "APP", "APPSEV", "SATIS", "DROPACT", "EMPTY", 
"BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", 
"MEMPROB", "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", 
"BILLS", "TAXES", "SHOPPING", "GAMES", "STOVE", "MEALPREP", "EVENTS", 
"PAYATTN", "REMDATES", "TRAVEL", "DECSUB", "DECIN", "DECCLIN", 
"COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", 
"COGOTHR", "NACCCOGF", "COGMODE", "DECAGE", "BEAPATHY", "BEDEP", 
"BEVHALL", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", 
"BEPERCH", "BEREM", "BEOTHR", "NACCBEHF", "BEMODE", "MOGAIT", 
"MOFALLS", "MOTREM", "MOSLOW", "NACCMOTF", "MOMODE", "COURSE", 
"FRSTCHG", "ANIMALS", "VEG", "TRAILA", "TRAILARR", "TRAILBRR", 
"COGSTAT")] <- 0

# check what the predictor matrix looks like
#pred_mat_all

# looks good - only the vars we want are set to 1 (predictors)

# run mice with pmm on the data

nacc_voi_dem_all_5_predict <- mice(nacc_voi_dem_all_5,
                                   m = 5,
                                   maxit = 20,
                                   method = "pmm",
                                   pred = pred_mat_all)


nacc_voi_dem_all_6 <- mice::complete(nacc_voi_dem_all_5_predict, 1)

# finally, remove the string "Dementia" from the Group name

nacc_voi_dem_all_7 <- nacc_voi_dem_all_6 %>%
  mutate(Group = str_remove(Group, "Dementia"))
  

# the final dataset to use for the all group analysis is nacc_voi_dem_all_7
```

### ii. For AD

```{r}


# # First, I will replace all trails values > 990 to 300 (max score)
# 
# nacc_voi_dem_AD_3 <- nacc_voi_dem_AD_2 %>%
#   # turn to wide format
#   pivot_wider(names_from = "Tests", 
#      values_from = "Value") %>%
#   # group data by visit number
#   group_by(NACCVNUM) %>%
#   # replace values with 96 and 996 with minimum value of those columns
#   apply(2, function(x) {round(impute(., median))})
#   
#   
#   mutate(across(28:126), ~replace(., . %in% c(96, 996), min(.)))
# 
# nacc_11 <- nacc_10 %>%
#   # group data by Dx and visit number
#   group_by(Group, NACCVNUM) %>%
#   # substitute all 96 and 996 with minimum number of those columns
#   mutate(across(221:336), ~replace(., . %in% c(96, 996), min(.)))
# 
# # g <- nacc_11
# 
# g %>% impute_lm(GAMES ~ NACCAGE + NACCMMSE + EDUC + CDRSUM | Group + NACCVNUM)
# 
# 
# 
# nacc_12 <- nacc_11 %>%
#   # group data by Dx and visit number
#   group_by(Group, NACCVNUM) %>%
#   # substitute 95, 995, 97, 997 with the median values 
#   mutate(is.numeric(across(221:336, ~replace(., . %in% c(95, 995, 97, 997), median(.)))))
# 
# g <- nacc_voi_dem_AD_1 %>%
#   # reshape into long df
#   pivot_wider(names_from = "Tests", values_from = "Value")
# 
# 
# 
# 
# # create a new dataset with imputed values
# nacc_voi_dem_AD_imp <- nacc_voi_dem_AD_1 %>%
# 
#   
#   
#   
#   # group by visit number (as all are AD)
#   group_by(NACCVNUM) %>%
#   # impute all missing data with the median value of that column, based on each person's group membership and visit number
#   mutate_all(funs(ifelse(is.na(.), median(., na.rm = T),. )))
# 
# # check if any NAs exist
# summary(is.na(nacc_voi_dem_AD_imp))
# # all missing data imputed

```

### iii. For PPA

There is a bit of preparation of the data needed before imputing it. First, I have to deal with the 95, 96 etc. and then create six versions of the dataset, one for each time point.

```{r}

# I'm thinking that since i've already culled so many variables down, there's no harm in simply now replacing all missings with NA. So I'm thinking of now replacing all 995, 996 etc with NA.

# is there a 95 or 995 in any column in this dataset?
which(nacc_voi_dem_PPA_2 == c("95", "995"), arr.ind = T)
# nope, so I don't need to worry about imputing these details

# Next, let us replace all 96, 996 etc. with NA. I will impute in these variables in the next step
nacc_voi_dem_PPA_3 <- nacc_voi_dem_PPA_2 %>%
  # group data by Dx and visit number
  # substitute all 96 and 996 with NA so we can impute them
  mutate(across(everything(), ~replace(., . %in% c(96, 97, 98, 99, 996, 997, 998, 999), NA)))

```

Imputing missing data using Age, Education, Sex, Group, NACCVNUM and CDR SOB as predictive variables.

```{r}

# first, I will have to set the predictor matrix. There is a bit of circularity to this step as to extract the predictor matrix, you need to have run MICE. So I will run MICE with 0 iterations.

test_ppa <- mice(nacc_voi_dem_PPA_3,
                     maxit = 0)

pred_mat_ppa <- test_ppa$predictorMatrix

dput(colnames(pred_mat_ppa))

# now copy all colnames except the six predictors set above, and set the remaining colnames to 0 (not predictors) leaving the six predictors as set to 1 (predictors)

pred_mat_ppa[, c("NACCID", "ClinDiagDementia", "ClinDiagMCI", 
"ClinDiagMCISubtype", "NACCADC", "NACCUDSD", "NACCDAYS", "NACCFDYS", 
"NACCDIED", "NACCMOD", "BIRTHYR",  "RACE", "PRIMLANG", 
"INDEPEND", "HANDED", "NACCAGEB","NORMCOG", "NACCFAM", 
"NACCMOM", "NACCDAD", "NACCFADM", "NACCFFTD", "MEMORY", "ORIENT", 
"JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", "CDRGLOB", 
"COMPORT", "CDRLANG", "DEL", "DELSEV", "HALL", "HALLSEV", "AGIT", 
"AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", "ELATSEV", 
"APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", "MOT", "MOTSEV", 
"NITE", "NITESEV", "APP", "APPSEV", "BILLS", "TAXES", "SHOPPING", 
"GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", 
"TRAVEL", "DECSUB", "DECIN", "COGMEM", "COGJUDG", "COGLANG", 
"COGVIS", "COGATTN", "COGFLUC", "COGOTHR", "NACCCOGF", "COGMODE", 
"DECAGE", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", "BEDEL", 
"BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEOTHR", 
"NACCBEHF", "BEMODE", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW", 
"NACCMOTF", "MOMODE", "COURSE", "FRSTCHG", "TRAILA", "TRAILBRR", 
"COGSTAT")] <- 0

# check what the predictor matrix looks like
#pred_mat_ppa

# looks good - only the vars we want are set to 1 (predictors)

# run mice with pmm on the data

nacc_voi_dem_PPA_3_predict <- mice(nacc_voi_dem_PPA_3,
                                   m = 5,
                                   maxit = 20,
                                   method = "pmm",
                                   pred = pred_mat_ppa)


nacc_voi_dem_PPA_4 <- mice::complete(nacc_voi_dem_PPA_3_predict, 1)

# now remove the string "Dementia" from the Grouping variable

nacc_voi_dem_PPA_5 <- nacc_voi_dem_PPA_4 %>%
  mutate(Group = str_remove(Group, "Dementia"))

# the final dataset to use for the PPA analysis is nacc_voi_dem_PPA_5
```

## **12. Merge CSF and imaging with final behavioural dataset**

With NACC, I am lucky that a number of these patients may have accompanying imaging and CSF data. I've already read in these data earlier. First, I will consult with the DD-CSF and DD-Imaging to check whether I am matching the data with all possible ID variables.

### i. For all groups

```{r}

# check colnames for CSF and imaging data to see if they too have NACCID and NACCVNUM

colnames(nacc_csf)
colnames(nacc_imaging)

# merge CSF and imaging data to PPA df by ID

nacc_voi_dem_all_8 <- nacc_voi_dem_all_7 %>%
  # join in CSF data by ID (CSF was only taken once I think so NACCVNUM doesn't matter)
    left_join(nacc_csf, by = "NACCID") %>%
  # join in MRI data by ID and visit number
  left_join(nacc_imaging, by = c("NACCID", "NACCVNUM"))


# Note, if you want to keep only those people who have MRI or CSF, then run the code below. The problem is that this retains mostly just AD patients. There are hardly any of these values anyways.

#test <- nacc_voi_dem_all_final %>%
#  group_by(NACCID, NACCVNUM) %>%
  # keep only those who have MRI (replace MRIMO with CSFABETA to keep only those with CSF)
#  filter(!is.na(MRIMO)) %>%
#  ungroup()

```

### ii. For PPA

```{r}

# check colnames for CSF and imaging data to see if they too have NACCID and NACCVNUM

colnames(nacc_csf)
colnames(nacc_imaging)

# merge CSF and imaging data to PPA df by ID

nacc_voi_dem_PPA_6 <- nacc_voi_dem_PPA_5 %>%
  # join in CSF data by ID (CSF was only taken once I think so NACCVNUM doesn't matter)
    left_join(nacc_csf, by = "NACCID") %>%
  # join in MRI data by ID and visit number
  left_join(nacc_imaging, by = c("NACCID", "NACCVNUM"))

```

## 13. Finalise dataframes for analyses

In the final part of the data preparation steps, I will have to convert all the data into percentage, including inverting the Trails and NPI subscales data. Note that normally, I would have derived an NPI Frequency\*Severity score for each subdomain, but here, the frequency variable is not scored in the traditional manner (0, 1, 2, 3), rather is just a present/absent. So in this case, I can use just the severity scores but I have to invert them because higher scores are worse.

### i. For all groups

```{r}


nacc_voi_dem_all_9 <- nacc_voi_dem_all_8 %>%
  # BILLS anomalous data had to be replaced with 0
  mutate(BILLS = replace(BILLS, BILLS == 9, 0)) %>%
  mutate(BILLS = replace(BILLS, BILLS == 8, 0)) %>%
  # group data
  group_by(Group, NACCVNUM) %>%
  # reverse code all NPI severity columns and modify to percentage
  mutate(across(ends_with("SEV"), ~ ((3-.)/3)*100, .names = "{col}REVPRCNT")) %>%
  # reverse code TRAILS, TRAILSARR, and TRAILS using maximum score
  mutate(across(starts_with("TRAIL"), ~ ((max(.)-.)/max(.))*100, .names = "{col}REVPRCNT")) %>%
  # reverse and scale all GDS variables
  mutate(across(SATIS:BETTER, ~((1-.)/1)*100, .names = "{col}REVPRCNT")) %>%
  # reverse and scale all FAS variables
  mutate(across(BILLS:TRAVEL, ~((3-.)/3)*100, .names = "{col}REVPRCNT")) %>%
  # reverse and scale all Cog (COGMEM etc.), Behavioural (BEApathy etc.), adn motor varibles (MOFALLS etc)
  mutate(across(COGMEM:COGOTHR, ~((1-.)/1)*100, .names = "{col}REVPRCNT")) %>%
  mutate(across(BEAPATHY:BEOTHR, ~((1-.)/1)*100, .names = "{col}REVPRCNT")) %>%
   mutate(across(MOGAIT:MOSLOW, ~((1-.)/1)*100, .names = "{col}REVPRCNT")) %>%
  # scale fluency scores
  mutate(across(ANIMALS:VEG, ~(./max(.))*100, .names = "{col}REVPRCNT")) %>%
  # organise these columns to have all the demographic data first, 
  select(NACCID:Group, NACCUDSD:CDRLANG, DEL:APPSEV, contains("SEV"), SATIS:COGSTAT, ends_with("REVPRCNT"), everything()) %>%
    # now lets drop the ClinDiagMCI and ClinDiagDementia columns 
  select(-c(ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype, NACCADC.y))
```

This dataset is extremely imbalanced in terms of group membership. This will affect our data-driven analyses. So we will have to remove some ADs and balance this dataset out more. Let's inspect this more.

```{r}

nacc_voi_dem_all_9 %>%
  # filter out only first time point
  filter(NACCVNUM == 1) %>%
  # group by diagnosis
  group_by(Group) %>%
  # summarise diagnostic groups
  summarise(n = n(),
            # extract CDR mean and SD
            mean = mean(CDRSUM), 
            sd = sd(CDRSUM),
            age = mean(NACCAGE),
            education = mean(EDUC))
```

Few things that we need to do here:

(i) Remove the PCA patient as there is N=1

(ii) Create a new grouping variable for FTLD (CBD, PSP, FTLD-NOS), PPA, bvFTD and AD

(iii) Sample a subset of the AD group who meet the range of `CDRSUM` score to the overall group.

```{r}

nacc_voi_dem_all_10 <- nacc_voi_dem_all_9 %>%
  ungroup() %>%
  # remove the PCA patient - NACC311188
  filter(NACCID != "NACC311188") %>%
  # create a new grouping variable
  mutate(Dx_Group = case_when(Group == "AD" ~ "AD",
                              # keep bvFTDs as separate group
                              Group == "bvFTD" ~ "bvFTD",
                              # combine CBD and PSP as FTLD-motor
                              Group == "CBD" ~ "FTLD-motor",
                              Group == "PSP" ~ "FTLD-motor",
                              # if FTLD NOS, then FTLD NOS
                              Group == "FTLD-NOS" ~ "FTLD-NOS",
                              # if the Dx contains PPA in the label, call them PPA
                              grepl("PPA", Group) ~ "PPA"
                              ))

# there seem to be two NANs that have crept up. I will first replace the NAN with NA
nacc_voi_dem_all_10$TRAILARRREVPRCNT[is.nan(nacc_voi_dem_all_10$TRAILARRREVPRCNT)]<-NA

nacc_voi_dem_all_10$TRAILBRRREVPRCNT[is.nan(nacc_voi_dem_all_10$TRAILBRRREVPRCNT)]<-NA

# then I will replace it with the group/time median
nacc_voi_dem_all_10a <- nacc_voi_dem_all_10 %>%
  group_by(Dx_Group, NACCVNUM) %>%
  mutate(across(where(is.numeric), ~ coalesce(., median(., na.rm = TRUE)))) %>%
  ungroup()


# Now I will create a vector of all the AD cases you want to retain
nacc_AD_retain_vec <- nacc_voi_dem_all_10a %>%
# check number of patient groups falling into each of these categories at T1
  group_by(Dx_Group) %>%
  # at time 1
  filter(NACCVNUM == 1) %>%
  # pick out only ADs
  filter(Dx_Group == "AD") %>%
  # now we add a few filters to picking out AD patients meeting some specific variable ranges
  # pick out ADs with Age between 63-67, Educ between 14-16, and CDR SUM between 2-5
  filter(NACCAGE > 65 & NACCAGE < 75) %>%
  filter(EDUC < 16) %>%
  ungroup() %>%
  select(NACCID) %>%
  as.data.frame()

# how many ADs do we have in this df?
nrow(nacc_AD_retain_vec)

# we need only 100ish, so I'm going to randomly sample 100 from this
nacc_AD_retain_vec_120sample <- sample_n(nacc_AD_retain_vec, 120)
nacc_AD_retain_vec_120sample <- as.data.frame(nacc_AD_retain_vec_120sample)
  
# create a new df at T1 with non-AD diagnosis
nacc_nonAD_retain_vec <- nacc_voi_dem_all_10a %>%
# check number of patient groups falling into each of these categories at T1
  group_by(Dx_Group) %>%
  # at time 1
  filter(NACCVNUM == 1) %>%
  # pick out only ADs
  filter(!Dx_Group == "AD") %>%
  ungroup() %>%
  select(NACCID) %>%
  as.data.frame()

# bind both dfs into a new df
nacc_all_retain_vec <- rbind(nacc_AD_retain_vec_120sample, nacc_nonAD_retain_vec)
nacc_all_retain_vec <- as.data.frame(nacc_all_retain_vec)

# check if any values are duplicated
summary(duplicated(nacc_all_retain_vec))

# who are these duplicates?
#nacc_all_retain_vec$NACCID[duplicated(nacc_all_retain_vec$NACCID)]

# check how many rows this df has
nrow(nacc_all_retain_vec)

# save as a new df
nacc_voi_dem_all_11 <- nacc_voi_dem_all_10a %>%
  # keep all IDs from the above keeps vector
  filter(NACCID %in% nacc_all_retain_vec$NACCID) %>%
  group_by(NACCID) %>%
  # remove duplicated IDs whose rows occur more than 3 times 
  filter(! n() > 3)

# are the nrows/3?
nrow(nacc_voi_dem_all_11)/3

# summarise number of patients per group
nacc_voi_dem_all_11 %>%
  filter(NACCVNUM == 1) %>%
  group_by(Dx_Group) %>%
  summarise(n = n())

names(nacc_voi_dem_all_11)
  
```

If you are running this analysis in specific groups, then repeat for AD and PPA.

# Analyses

Lets run some analyses. Below, [**I'm running analyses only for all groups combined**]{.underline}.

## 1. Group differences

I can use APA tables and other packages for this data.

### i) Check floor/ceiling effects

This is important to determining what tests can go into the PCA. Tests that have a lot of floor/ceiling effects cannot usually go into the PCA.

```{r}

# First, create a dataset to work off of

nacc_voi_dem_all_11 %>%
  ungroup() %>%
  select(NACCID, Dx_Group, NACCVNUM, DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT) %>% 
  pivot_longer(DELSEVREVPRCNT:VEGREVPRCNT, names_to = "Test", values_to = "Value") %>%
  as.data.frame() %>%
  ggplot(aes(x = Value)) +
  facet_wrap(~Test, scales = "free") +
  geom_histogram()

# saved as 2000*1000 dim

```

### ii) Plot number of patients changing diagnosis

```{r}

nacc_alluvium_full <- test_1 %>%
  dplyr::select(NACCID, NACCVNUM, Dx_Group) %>% # select only these variables
  ggplot(aes(x = as.factor(NACCVNUM), stratum = Dx_Group, # create the layers
             alluvium = NACCID, fill = Dx_Group)) + 
  geom_flow() + # add the flow
  geom_stratum(alpha = .5) + # modify the transparence of the flow
  theme_bw() +
  theme_classic() +
  xlab("Visit number") +
  ylab ("Count") +
  guides(fill = guide_legend(title="Group")) +
  scale_fill_manual(values = group.colours) +
  # for some reason, I'm struggling to add in the numbers of each patient group at each time point so I'm going to manually add them in
  # for Time 1
  annotate("text", x = 1, y = 340.5, label = "114") + # AD
  annotate("text", x = 1, y = 228.5, label = "107") + # bvFTD
  annotate("text", x = 1, y = 151, label = "42") + # FTKD-m
  annotate("text", x = 1, y = 115, label = "24") + # FTLD-NOS
  annotate("text", x = 1, y = 51.5, label = "103") + # PPA
   # for time 2
  annotate("text", x = 2, y = 327, label = "132") + # AD
  annotate("text", x = 2, y = 220, label = "82") + # bvFTD
  annotate("text", x = 2, y = 161, label = "36") + # FTKD-m
  annotate("text", x = 2, y = 124, label = "38") + # FTLD-NOS
  annotate("text", x = 2, y = 52.5, label = "105") + # PPA
  # for time 3
  annotate("text", x = 3, y = 306, label = "134") + # AD
  annotate("text", x = 3, y = 220, label = "75") + # bvFTD
  annotate("text", x = 3, y = 164, label = "40") + # FTKD-m
  annotate("text", x = 3, y = 124, label = "40") + # FTLD-NOS
  annotate("text", x = 3, y = 52, label = "104") +  # PPA
   # change the text size for axes
  theme(axis.text= element_text(size=12), axis.title = element_text(size=12)) +
  ggtitle("Full group") +
  theme(plot.title = element_text(hjust = 0.5))
# saved as 700*500

```

### iii) Calculate group differences

Now, I will check group differences in demographics and other test variables.

```{r}

# create a T1 df for group difference analyses
nacc_voi_dem_all_11_t1 <- nacc_voi_dem_all_11 %>% 
  group_by(NACCID) %>% 
  filter(NACCVNUM == 1) %>% 
  ungroup()


# first, count N
nacc_voi_dem_all_11 %>%
  group_by(Dx_Group) %>%
  filter(NACCVNUM == 1) %>%
  tally()

# sex differences = 1 is male; 2 is female
nacc_voi_dem_all_11 %>%
  group_by(Dx_Group, SEX) %>%
  filter(NACCVNUM == 1) %>%
  tally()

# check sex differences (T1)
chisq.test(nacc_voi_dem_all_11_t1$SEX, nacc_voi_dem_all_11_t1$Dx_Group)


# Age differences (T1)
aggregate(nacc_voi_dem_all_11_t1$NACCAGEB, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = mean)
aggregate(nacc_voi_dem_all_11_t1$NACCAGEB, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = sd)

summary(aov(NACCAGEB ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
eta_squared(aov(NACCAGEB ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
TukeyHSD(aov(NACCAGEB ~ Dx_Group, data = nacc_voi_dem_all_11_t1))


# Educ differences (T1)
aggregate(nacc_voi_dem_all_11_t1$EDUC, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = mean)
aggregate(nacc_voi_dem_all_11_t1$EDUC, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = sd)

summary(aov(EDUC ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
eta_squared(aov(EDUC ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
TukeyHSD(aov(EDUC ~ Dx_Group, data = nacc_voi_dem_all_11_t1))



# CDR SOB differences (T1)
aggregate(nacc_voi_dem_all_11_t1$CDRSUM, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = mean)
aggregate(nacc_voi_dem_all_11_t1$CDRSUM, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = sd)

summary(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
eta_squared(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
TukeyHSD(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))


# time between first nad second visit
# CDR SOB differences (T1)
aggregate(nacc_voi_dem_all_11_t1$NACCFDYS, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = mean)
aggregate(nacc_voi_dem_all_11_t1$CDRSUM, by = list(nacc_voi_dem_all_11_t1$Dx_Group), FUN = sd)

summary(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
eta_squared(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))
TukeyHSD(aov(CDRSUM ~ Dx_Group, data = nacc_voi_dem_all_11_t1))


# examine changes in each time point in B9CHG, FRSTCHG and COURSE variable. The idea is to plot stacked bar graphs, faceted by group, showing the number of patients who have vlaues for each of these. For this, I create a new df with just NACCVNUM, NACCID adn the above three vars and then left-join it into the nacc_voi_dem_all_11 df
# 
# nacc_frstchg <- nacc_11 %>% 
#   group_by(NACCID) %>%
#   dplyr::select(NACCID, NACCVNUM, B9CHG)
# 
# nacc_frstchg_1 <- nacc_frstchg %>%
#   left_join(nacc_voi_dem_all_11, by = c("NACCID", "NACCVNUM"))


# assess changes in course of decline (COURSE variable) by group
# nacc_voi_dem_all_11 %>% 
#   mutate(COURSE_n = case_when(COURSE == 1 ~ "Gradual",
#                               COURSE == 2 ~ "Stepwise",
#                               COURSE == 3 ~ "Static",
#                               COURSE == 4 ~ "Fluctuating")) %>%
#   group_by(NACCVNUM, Dx_Group, COURSE_n) %>% 
#   summarise(n = n()) %>%
#   summarise(freq = (n/sum(n)*100)) %>%
#   ggplot(., aes(x = COURSE_n, y = freq)) +
#   geom_bar(position = "stack", stat = "identity") +
#   facet_grid(NACCVNUM ~ Dx_Group)


```

## 2. Uncover dimensional structure of data

Now, I'll aim to uncover the dimensional structure of the data. For this, I'll run a number of analyses.

### i) Cross-validated PCA component selection

Prior to running PCA, I need to select the number of components that underlie the data. For this, we have used a pca_compsel algorithm in the past, implemented in MATLAB. I will move the data into MATLAB to do this. Unfortunately, this affects this workflow for now, but I will bring the data back in to R later.

First, I will have to save each time point data as a new dataframe.

```{r}

# create an ID df that I can paste into the output dfs later

nacc_voi_dem_all_ID_indtimepoint <- nacc_voi_dem_all_11 %>%
  # choose any one time point
  filter(NACCVNUM == 1) %>%
  # keep ID column
  select(NACCID)

nacc_voi_dem_all_ID_alltimepoint <- nacc_voi_dem_all_11 %>%
  # keep ID column
  select(NACCID)

# create DFs for each time point for only variables of interest

nacc_voi_dem_all_11_time1 <- nacc_voi_dem_all_11 %>%
  ungroup() %>%
  filter(NACCVNUM == 1) %>%
  select(DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT)

nacc_voi_dem_all_11_time2 <- nacc_voi_dem_all_11 %>%
  ungroup() %>%
  filter(NACCVNUM == 2) %>%
  select(DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT)

nacc_voi_dem_all_11_time3 <- nacc_voi_dem_all_11 %>%
  ungroup() %>%
  filter(NACCVNUM == 3) %>%
  select(DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT)
```

Now, create three dfs, for each time point, and save it to wd. I will move a copy of this adn the ID frames to the cluster.

```{r}

# save ID dfs

write.xlsx(nacc_voi_dem_all_ID_indtimepoint, file = "nacc_voi_dem_all_ID_indtimepoint.xlsx")

write.xlsx(nacc_voi_dem_all_ID_alltimepoint, file = "nacc_voi_dem_all_ID_alltimepoint.xlsx")

# save other dfs

write.xlsx(nacc_voi_dem_all_11_time1, file = "nacc_voi_dem_all_11_time1.xlsx")

write.xlsx(nacc_voi_dem_all_11_time2, file = "nacc_voi_dem_all_11_time2.xlsx")

write.xlsx(nacc_voi_dem_all_11_time3, file = "nacc_voi_dem_all_11_time3.xlsx")
```

In parallel, I have finished running component selection for PCA using MATLAB. Its time to read in that code and make those plots. I have manually combined all component selection findings into one excel sheet.

```{r}

nacc_voi_dem_all_rmsecvCompsel <- read.xlsx("RMSECV_allTs_combined.xlsx")

nacc_voi_dem_all_rmsecvCompsel_1 <- nacc_voi_dem_all_rmsecvCompsel %>%
  # turn data into long format
  pivot_longer(!Components, names_to = "Iteration", values_to = "CVRMSE")

 # create factors with iteration and Time point
nacc_voi_dem_all_rmsecvCompsel_1$Timepoint <- rep(1:3, each = 10, times = 63)
nacc_voi_dem_all_rmsecvCompsel_1$Iteration_no <- rep(1:10, each = 1, times = 189)

# add string "Time" to Time point column
nacc_voi_dem_all_rmsecvCompsel_1$Timepoint <- sub("^", "Time ", nacc_voi_dem_all_rmsecvCompsel_1$Timepoint)

# make Component, Iteration, and Time point variables factors
nacc_voi_dem_all_rmsecvCompsel_1$Iteration_no <- as.factor(nacc_voi_dem_all_rmsecvCompsel_1$Iteration_no)
nacc_voi_dem_all_rmsecvCompsel_1$Timepoint <- as.factor(nacc_voi_dem_all_rmsecvCompsel_1$Timepoint)


# the CV-RMSE column has a number of values that are extraordinarily high. Its best to truncate all values > 300 for visualisation

nacc_voi_dem_all_rmsecvCompsel_2 <- nacc_voi_dem_all_rmsecvCompsel_1 %>%
# for this step, I'll simply recode all values above 280 to "280" and then will set limits and breaks for scale_fill in the ggplot 
    mutate(CVRMSE = 
      case_when(
        CVRMSE >= 280 ~ 280,
        CVRMSE < 280 ~ CVRMSE))

# I don't want to plot all 63 components, so I'm going to select just a few specific components to plot
comps_keep <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "15", "20", "30", "40", "60", "63")

# make and save ploteigen

# MAKE A NOTE THAT VALUES >=280 ARE TRUNCATED FOR PLOTTING
nacc_fig_compsel_alltimepoints <- nacc_voi_dem_all_rmsecvCompsel_2 %>%
  # select only the components from comps_keep
  filter(Components %in% comps_keep) %>%
  # make the plot
  ggplot(aes(x=as.factor(Components), 
           y=as.factor(Iteration_no), 
           fill=CVRMSE)) +
  # facet by time points
  facet_grid(~Timepoint) +
  # make the tile plot
  geom_raster() +
  # change the backgrounds
  theme_bw() +
  theme_classic() +
  # add the colour scale
  scale_fill_viridis() +
  # change the text size for axes
  theme(axis.text= element_text(size=12), axis.title = element_text(size=12)) +
 # labs(caption = "Note: values above 280 truncated") +
  # change x, y axis labels, legend label
  labs(fill='CV-RMSE value') +
  xlab("Component number") +
  ylab("Iteration number") +
  # change strip text size and put a box around the legend.
  theme(strip.text = element_text(size=12),
        legend.box.background = element_rect(colour = "black"),
        legend.background = element_blank())

# saved as 1400*550 dim  

```

### ii) Principal Component Analysis

In conversation with Dan Akarca, we thought of some different approaches to these analyses. We thought to take all subjects at all time points and do one PCA. Then I extract the PCA scores and treat them as a coordinate within that multidimensional space. For select PCs, Then I can calculate the euclidean distance between subjects and time points, giving me a matrix of scores of changes between each subject over time and each time point over time. Within this plot, I'll know who has changed how much, how similar they are to themselves, their group members, and other group members. On this data, I can further try glasso etc.

Lets try this out. First, I will run a PCA on the full dataset. For low/non-loading variables, I will go back to the original distribution figure and check to see if the distribution is skewed/floor/ceiling effects. If so, I'll remove those variables and run the PCA on the rest of the data.

#### First pass PCA on all variables

```{r}

# first, create a toy dataset to play with

test_1 <- nacc_voi_dem_all_11 %>%
  ungroup() %>%
  select(NACCID, Dx_Group, NACCVNUM, DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT) %>%
  as.data.frame()

# next, arrange the data frame so that all Time 1s appear in the first rows, followed by Time 2s and Time 3s. This helps plot the dissimilarity matrix
test_2 <- test_1 %>%
  group_by(NACCID) %>%
  arrange(NACCVNUM, Dx_Group) %>%
  ungroup()

# once the distance matrix is created, I'll need to replace each index with the participant's ID value.
# For this, I will create a composite of each NACCID and diagnosis and visit number
test_2$concat <- paste(test_2$NACCID, test_2$Dx_Group, test_2$NACCVNUM, sep = "_")

# I will now turn this into a separate df and give it a sequential ID number that I can use to index each person's position on the dissimilarity matrix
ID_Diag_VNUM_vec <- as.data.frame(test_2$concat)
# change the colname to ID
colnames(ID_Diag_VNUM_vec)[colnames(ID_Diag_VNUM_vec) == "test_2$concat"] <- "PID"

# add a sequential integer count. I need to call it the same name as what the column will be called (Var1 and Var2) when the dataset is reshaped
ID_Diag_VNUM_vec$Var1 <- seq.int(nrow(ID_Diag_VNUM_vec))

# create a copy of this column and call it Var2
ID_Diag_VNUM_vec$Var2 <- ID_Diag_VNUM_vec$Var1

# For the PCA, create a version of this dataset with only test scores
test_3 <- test_2 %>% select(DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT) %>%
  as.data.frame()

# now run the PCA with a 6 component solution
fit_pca <- psych::principal(test_3, nfactors = 6, rotate = "varimax", scores = T)

# for trial, let us also create three versions of this dataset to see whether the same factor structure emerges at all three time points
test_3_v1 <- test_2 %>% filter(NACCVNUM == 1) %>% dplyr::select(DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT) %>%
  as.data.frame()

```

What do key metrics (loadings, variance explained etc) look like?

```{r}

# extract loadings into a df
loadings_df <- as.data.frame.matrix(fit_pca$loadings)

# there are too many low level loadings so lets clean them up
# set a 0.5 threshold
loadings_threshold <- 0.5

# set all loadings below this threshold to NA
loadings_df[abs(loadings_df) < loadings_threshold] <- NA

# view the loadings table again
loadings_df

# what does varexp look like
fit_pca$Vaccounted

# calculate KMO
psych::KMO(test_3)
# extremely high - NICE!


# merge in PCA scores with ID variables

# create a matrix of the PCA scores
fit_scores <- as.matrix(fit_pca$scores)

# create a df version also
fit_scores_df <- as.data.frame(fit_scores)

fit_scores_df$Var1 <- seq.int(nrow(fit_scores_df))

# merge with IDs
fit_scores_df_ID <- fit_scores_df %>%
    # sequentially join in the PID df
  # First, join in the PID column but by Var1
  left_join(ID_Diag_VNUM_vec, by=c("Var1")) %>%
  # select out unnecessary cols (Var 2 andVar1)
  select(-c(Var1, Var2)) %>%
  #parse the ID variable
  separate(PID, c("ID", "Dx", "Visit"), "_")

```

One of the issues in this above PCA analyses were that there were many non-loading/low-loading variables. What happens if we remove these variables? Does the PCA model improve? Lets test this below.

#### Second pass PCA on select variables (removing non-loading tests)

First, I will create a list of non-loading/low-loading variables from the above PCA and remove them from a new toy dataset. I will call this noLL = no Low Loading.

```{r}

# I will call this new dataset as noLL = no Low Loading

test_3_noLL <- test_3 %>%
  select(-c("DELSEVREVPRCNT",  "NITESEVREVPRCNT", "TRAILARRREVPRCNT", "TRAILBRRREVPRCNT", "SATISREVPRCNT", "DROPACTREVPRCNT", "BOREDREVPRCNT", "SPIRITSREVPRCNT", "AFRAIDREVPRCNT", "HAPPYREVPRCNT", "STAYHOMEREVPRCNT", "MEMPROBREVPRCNT", "WONDRFULREVPRCNT", "ENERGYREVPRCNT", "BETTERREVPRCNT", "COGLANGREVPRCNT", "COGVISREVPRCNT", "COGATTNREVPRCNT", "COGFLUCREVPRCNT", "COGOTHRREVPRCNT", "BEAPATHYREVPRCNT",  "BEAGITREVPRCNT", "BEREMREVPRCNT", "BEOTHRREVPRCNT"))

# run PCA

# now run the PCA with a 6 component solution
fit_pca_noLL <- psych::principal(test_3_noLL, nfactors = 6, rotate = "varimax", scores = T)

# extract loadings into a df
loadings_df_noLL <- as.data.frame.matrix(fit_pca_noLL$loadings)

# there are too many low level loadings so lets clean them up
# set a 0.5 threshold
loadings_threshold <- 0.5

# set all loadings below this threshold to NA
loadings_df_noLL[abs(loadings_df_noLL) < loadingsu_threshold] <- NA

# view the loadings table again
loadings_df_noLL

# what does varexp look like
fit_pca_noLL$Vaccounted
# better than previous one

# calculate KMO
psych::KMO(test_3_noLL)
# extremely high - NICE!

# create a matrix of the PCA scores
fit_pca_noLL_scores <- as.matrix(fit_pca_noLL$scores)

# create a df version also
fit_pca_noLL_scores_df <- as.data.frame(fit_pca_noLL_scores)

fit_pca_noLL_scores_df$Var1 <- seq.int(nrow(fit_pca_noLL_scores_df))

# merge with IDs
fit_pca_noLL_scores_df_ID <- fit_pca_noLL_scores_df %>%
    # sequentially join in the PID df
  # First, join in the PID column but by Var1
  left_join(ID_Diag_VNUM_vec, by=c("Var1")) %>%
  # select out unnecessary cols (Var 2 andVar1)
  select(-c(Var1, Var2)) %>%
  #parse the ID variable
  separate(PID, c("ID", "Dx", "Visit"), "_")

```

#### Plot PCA results

Here, I will make a plot of the variance explained, loadings etc.

```{r}

# First, make a plot of the LOADINGS

# save the loadings matrix as a new DF
loadings_df_noLL <- as.data.frame.matrix(fit_pca_noLL$loadings) 

# the rownames need to be a separate column so I can use them to pivot the dataset to long
loadings_df_noLL$Measures <- rownames(loadings_df_noLL)

# reshape this dataset to long
loadings_df_noLL_long <- reshape2::melt(loadings_df_noLL, id="Measures")

# now rename the test names for plotting purposes # you can get the rownames via dput(rownames(loadings_df_noLL))
# first remove the string REVPRCNT from all rows
loadings_df_noLL_long[] <- lapply(loadings_df_noLL_long, gsub, pattern='REVPRCNT', replacement='')
 
# rename vars for Measures
loadings_df_noLL_long$Measures <- recode(loadings_df_noLL_long$Measures,
                                         "HALLSEV" = "NPI.Hallucinations",
                                         "AGITSEV" =  "NPI.Agitation",
                                         "DEPDSEV" = "NPI.Depression",
                                         "ANXSEV" = "NPI.Anxiety",
                                         "ELATSEV" = "NPI.Elation",
                                         "APASEV" = "NPI.Apathy",
                                         "DISNSEV" = "NPI.Dishinhibit",
                                         "IRRSEV" = "NPI.Irritab",
                                         "MOTSEV" = "NPI.Motor",
                                         "APPSEV" = "NPI.Appetite",
                                         "EMPTY" = "GDS.Empty",
                                         "HELPLESS" = "GDS.Helpless",
                                         "WRTHLESS" = "GDS.Worthless",
                                         "HOPELESS" = "GDS.Hopeless",
                                         "BILLS" = "FAQ.Bills",
                                         "TAXES" = "FAQ.Taxes",
                                         "SHOPPING" = "FAQ.Shopping",
                                         "GAMES" = "FAQ.Games",
                                         "STOVE" = "FAQ.Stove",
                                         "MEALPREP" = "FAQ.MealPrep",
                                         "EVENTS" = "FAQ.Events",
                                         "PAYATTN" = "FAQ.PayAttn",
                                         "REMDATES" = "FAQ.RemDates",
                                         "TRAVEL" = "FAQ.Travel",
                                         "COGMEM" = "Clin.CogMemory",
                                         "COGJUDG" = "Clin.CogExec",
                                         "BEDEP" = "Clin.BehDepress",
                                         "BEVHALL" = "Clin.BehVisHalluc", 
                                         "BEAHALL" = "Clin.BehAudHalluc", 
                                         "BEDEL" = "Clin.BehDelusions",
                                         "BEDISIN" = "Clin.BehDisinhibit",
                                         "BEIRRIT" = "Clin.BehIrritab",
                                         "BEPERCH" = "Clin.BehPersChange",
                                         "MOGAIT" = "MotorGait",
                                         "MOFALLS" = "MotorFalls",
                                         "MOTREM" = "MotorTremors",
                                         "MOSLOW" = "MotorSlowing",
                                         "ANIMALS" = "Cog.AnimalFlu",
                                         "VEG" = "Cog.VegFlu")

# reset these vars as factor/numeric
loadings_df_noLL_long$Measures <- as.factor(loadings_df_noLL_long$Measures)
loadings_df_noLL_long$value <- as.numeric(loadings_df_noLL_long$value)


# rename vars for Measures
loadings_df_noLL_long$variable <- recode(loadings_df_noLL_long$variable,
                                         "RC1" = "PC1: Func. Stat.",
                                         "RC2" = "PC2: Apat./impuls.",
                                         "RC3" = "PC3: Motor",
                                         "RC4" = "PC4: Psychosis",
                                         "RC5" = "PC5: Affective",
                                         "RC6" = "PC6: Depression"
                                         )

# create plot
fig1p1a <- loadings_df_noLL_long %>%
  ggplot(aes(x = value, y = Measures)) +
  geom_bar(stat = 'identity', position = 'identity') +
  scale_x_continuous(limits=c(-1, 1)) +
  facet_wrap(~variable, nrow=1) +
  scale_y_discrete(limits = unique(rev(loadings_df_noLL_long$Measures))) + 
  geom_vline(xintercept = .5, colour='red', linetype='dashed') +
  geom_vline(xintercept = -.5, colour='red', linetype='dashed') +
  theme_bw() +
  theme(strip.background = element_rect(colour = "black"), strip.text = element_text(size=12)) +
  xlab('Component Loadings') + 
  ylab('Measures') +
  theme(text = element_text(size=12), 
        axis.text=element_text(size=12))
# saved as 1600(w) * 800


# plot KMO

kmo_df_noLL <- as.data.frame(KMO(test_3_noLL)$MSAi) # for each item

# # the rownames need to be a separate column so I can use them to pivot the dataset to long
kmo_df_noLL$Measures <- rownames(kmo_df_noLL)

# reshape this dataset to long
kmo_df_noLL_long <- reshape2::melt(kmo_df_noLL, id="Measures")

# now rename the test names for plotting purposes # you can get the rownames via dput(rownames(loadings_df_noLL))
# first remove the string REVPRCNT from all rows
kmo_df_noLL_long[] <- lapply(kmo_df_noLL_long, gsub, pattern='REVPRCNT', replacement='')
 
# rename vars for Measures
kmo_df_noLL_long$Measures <- recode(kmo_df_noLL_long$Measures,
                                         "HALLSEV" = "NPI.Hallucinations",
                                         "AGITSEV" =  "NPI.Agitation",
                                         "DEPDSEV" = "NPI.Depression",
                                         "ANXSEV" = "NPI.Anxiety",
                                         "ELATSEV" = "NPI.Elation",
                                         "APASEV" = "NPI.Apathy",
                                         "DISNSEV" = "NPI.Dishinhibit",
                                         "IRRSEV" = "NPI.Irritab",
                                         "MOTSEV" = "NPI.Motor",
                                         "APPSEV" = "NPI.Appetite",
                                         "EMPTY" = "GDS.Empty",
                                         "HELPLESS" = "GDS.Helpless",
                                         "WRTHLESS" = "GDS.Worthless",
                                         "HOPELESS" = "GDS.Hopeless",
                                         "BILLS" = "FAQ.Bills",
                                         "TAXES" = "FAQ.Taxes",
                                         "SHOPPING" = "FAQ.Shopping",
                                         "GAMES" = "FAQ.Games",
                                         "STOVE" = "FAQ.Stove",
                                         "MEALPREP" = "FAQ.MealPrep",
                                         "EVENTS" = "FAQ.Events",
                                         "PAYATTN" = "FAQ.PayAttn",
                                         "REMDATES" = "FAQ.RemDates",
                                         "TRAVEL" = "FAQ.Travel",
                                         "COGMEM" = "Clin.CogMemory",
                                         "COGJUDG" = "Clin.CogExec",
                                         "BEDEP" = "Clin.BehDepress",
                                         "BEVHALL" = "Clin.BehVisHalluc", 
                                         "BEAHALL" = "Clin.BehAudHalluc", 
                                         "BEDEL" = "Clin.BehDelusions",
                                         "BEDISIN" = "Clin.BehDisinhibit",
                                         "BEIRRIT" = "Clin.BehIrritab",
                                         "BEPERCH" = "Clin.BehPersChange",
                                         "MOGAIT" = "MotorGait",
                                         "MOFALLS" = "MotorFalls",
                                         "MOTREM" = "MotorTremors",
                                         "MOSLOW" = "MotorSlowing",
                                         "ANIMALS" = "Cog.AnimalFlu",
                                         "VEG" = "Cog.VegFlu")

# reset these vars as factor/numeric
kmo_df_noLL_long$Measures <- as.factor(kmo_df_noLL_long$Measures)
kmo_df_noLL_long$value <- as.numeric(kmo_df_noLL_long$value)


# rename vars for Measures
kmo_df_noLL_long$variable <- recode(kmo_df_noLL_long$variable,
                                         "RC1" = "PC1",
                                         "RC2" = "PC2",
                                         "RC3" = "PC3",
                                         "RC4" = "PC4",
                                         "RC5" = "PC5",
                                         "RC6" = "PC6"
                                         )

# create plot
kmo_df_noLL_long %>%
  ggplot(aes(x = value, y = Measures, label = round(value, digits = 2))) +
  geom_segment(aes(yend = Measures), xend = 0, linewidth = 1, colour = "grey50") +
  geom_point(size = 4) +
  theme_bw() +
  scale_x_continuous(limits=c(0.5, 1)) +
  xlab('Value') + 
  ylab('Measures') +
  theme(text = element_text(size=12), 
        axis.text=element_text(size=12),
        panel.grid.major.y = element_blank()) +
  geom_text(hjust = -0.4)
# saved as 1k * 700



# Plot VARIANCE EXPLAINED
varexp_df_noLL <- as.data.frame(fit_pca_noLL$Vaccounted)

# the rownames need to be a separate column so I can use them to pivot the dataset to long
varexp_df_noLL$VarExp <- rownames(varexp_df_noLL)

# keep only proportion and cumulative variance
varexp_df_noLL <- varexp_df_noLL[2:3, ]

# reshape this dataset
varexp_df_noLL_long <-reshape2::melt(varexp_df_noLL, id.vars ='VarExp')

# rename vars for Measures
varexp_df_noLL_long$variable <- recode(varexp_df_noLL_long$variable,
                                         "RC1" = "PC1",
                                         "RC2" = "PC2",
                                         "RC3" = "PC3",
                                         "RC5" = "PC4",
                                         "RC4" = "PC5",
                                         "RC6" = "PC6"
                                         ) 

varexp_df_noLL_long <- varexp_df_noLL_long %>%
  mutate(value = case_when(VarExp == "Cumulative Variance" & value == 0.18844018) ~ 0, TRUE ~ value)

# replace the cumulative variance for the first PC with 0
varexp_df_noLL_long$value[which(varexp_df_noLL_long$VarExp == "Cumulative Var" & varexp_df_noLL_long$variable == "PC1")] <- NA

varexp_df_noLL_long <- na.omit(varexp_df_noLL_long)


# load a colorblind friendly pallete
cbp1 <- c("#E69F00", "#56B4E9", "#009E73", "#999999", 
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# create stacked bar chart
ggplot(varexp_df_noLL_long, aes(x = variable, y = value, fill = VarExp)) +
  geom_col() +
  geom_text(aes(label = round(value, digits = 2)),
            size=5, vjust=-1) + 
  scale_y_continuous(breaks = seq(0, 0.6, 0.2)) +
  theme_bw() + 
  theme(panel.grid.major=element_blank()) + 
  theme(axis.text= element_text(size=12), axis.title = element_text(size=12)) +
  xlab('Component number') + 
  ylab('Variance explained')+
  labs(fill = "Variance type") +
  scale_fill_manual(values=cbp1, labels=c("Cumulative Variance", 'Component Variance')) +
  theme(legend.title = element_blank(),
        legend.spacing.y = unit(0, "mm"), 
        panel.border = element_rect(colour = "black", fill=NA),
        aspect.ratio = 1, axis.text = element_text(colour = 1, size = 12),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black"), 
        legend.position = c(0.15, 0.8))
# saved as 950*650


# Plot PARALLEL ANALYSIS plot

paran_fit_PCA <- fa.parallel(test_3_noLL)


# Plot SCREEPLOTS

# save eigen values as a vector with a row number
fit_pca_noLL_scores_eigen <- as.data.frame(fit_pca_noLL$values)

# rename this column
colnames(fit_pca_noLL_scores_eigen)[1] = "Values"

# create a column for component number
fit_pca_noLL_scores_eigen$Component <- seq.int(nrow(fit_pca_noLL_scores_eigen))

# plot
fit_pca_noLL_scores_eigen %>%
  ggplot(aes(x = Component, y = Values)) +
  geom_point(size = 3) +
  geom_line() +
  geom_hline(yintercept = 1, colour = "black", linetype = 2) +
  geom_vline(xintercept = 6.7, colour = "red") +
  geom_text(data = subset(fit_pca_noLL_scores_eigen, Values > 1), 
    aes(label = round(Values, digits = 1)), 
            vjust = -1, 
    hjust = -0.1,
            size = 3) +
  theme_bw() +
  xlab('Component number') + 
  ylab('Eigenvalue') +
  theme(text = element_text(size=12), 
        axis.text=element_text(size=12))
# saved as 750*500


```

#### Group differences on principal components for noLL dataset

First, I will visualise the group differences at each time point and at all time points.

```{r}

# measure group differences using ANOVA with Tukey post-hoc corrections

# create a T1 object to examine group differences at T1
fit_pca_noLL_scores_df_ID_t1 <- fit_pca_noLL_scores_df_ID %>% filter(Visit == 1)


# PC 1
m1 <- aov(RC1 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m1)
TukeyHSD(m1)

# PC 2
m2 <- aov(RC2 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m2)
TukeyHSD(m2)

# PC 3
m3 <- aov(RC3 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m3)
TukeyHSD(m3)

# PC 4
m4 <- aov(RC4 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m4)
TukeyHSD(m4)

# PC 5
m5 <- aov(RC5 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m5)
TukeyHSD(m5)

# PC 6
m6 <- aov(RC6 ~ Dx, data = fit_pca_noLL_scores_df_ID_t1)
summary(m6)
TukeyHSD(m6)


# plot for T1 only
fit_pca_noLL_scores_df_ID %>% 
  filter(Visit == 1) %>%
   rename("PC1" = "RC1",
         "PC2" = "RC2",
         "PC3" = "RC3",
         "PC4" = "RC4",
         "PC5" = "RC5",
         "PC6" = "RC6"
         ) %>%
  dplyr::select(-c(EucBetween, Visit)) %>%
  pivot_longer(!c(ID, Dx), 
               names_to = "Components",
               values_to = "values") %>%
  ggplot(aes(x = values,
             y = Dx)) +
  facet_wrap(~Components) +
  geom_density_ridges(aes(colour = Dx,
                          fill = Dx),
                      jittered_points = TRUE, 
                      scale = 0.6, 
                      alpha = 0.6,
                      point_size = 2, 
                      point_alpha = 0.6,
                      position = "raincloud") +
  theme_bw() +
  theme(panel.grid.major=element_blank()) +
  theme(legend.title = element_text(size=12),
        legend.text = element_text(size = 12), 
        legend.background = element_rect(colour='black', linetype = "solid"),
        axis.text= element_text(size=12), 
        axis.title = element_text(size=12),
        strip.text = element_text(size = 12), 
        legend.position = 'none') +
  xlab('Component score') + 
  ylab('Group') +
  scale_colour_manual(values = group.colours) +
  scale_fill_manual(values = group.colours) +
  scale_y_discrete(limits = rev)
# savd as 900*650


# plot for all time points
fit_pca_noLL_scores_df_ID %>% 
   rename("PC1" = "RC1",
         "PC2" = "RC2",
         "PC3" = "RC3",
         "PC4" = "RC4",
         "PC5" = "RC5",
         "PC6" = "RC6"
         ) %>%
  pivot_longer(!c(ID, Dx, Visit), 
               names_to = "Components",
               values_to = "values") %>%
  ggplot(aes(x = values,
             y = Dx)) +
  facet_wrap(~Components) +
  geom_density_ridges(aes(colour = Dx,
                          fill = Dx),
                      jittered_points = TRUE, 
                      scale = 0.6, 
                      alpha = 0.6,
                      point_size = 2, 
                      point_alpha = 0.6,
                      position = "raincloud") +
  theme_bw() +
  theme(panel.grid.major=element_blank()) +
  theme(legend.title = element_text(size=12),
        legend.text = element_text(size = 12), 
        legend.background = element_rect(colour='black', linetype = "solid"),
        axis.text= element_text(size=12), 
        axis.title = element_text(size=12),
        strip.text = element_text(size = 12), 
        legend.position = 'none') +
  xlab('Component score') + 
  ylab('Group') +
  scale_colour_manual(values = group.colours) +
  scale_fill_manual(values = group.colours) +
  scale_y_discrete(limits = rev)
# savd as 900*650


  
```

#### **Estimate change in PC scores over time**

```{r}

# plot these values

fit_pca_noLL_scores_df_ID %>% 
   rename("PC1" = "RC1",
         "PC2" = "RC2",
         "PC3" = "RC3",
         "PC4" = "RC4",
         "PC5" = "RC5",
         "PC6" = "RC6"
         ) %>%
  pivot_longer(!c(ID, Dx, Visit), 
               names_to = "Components",
               values_to = "values") %>%
    ggplot(aes(x = Visit,
             y = values,
             colour = Dx,
             group = Dx)) +
  geom_point(position = "jitter",
             alpha = 0.2) +
  stat_smooth() + # method=lm, se=T, fullrange=TRUE) +
  facet_wrap(~Components, 
             scales = "free") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(axis.text = element_text(size = 15), 
        axis.title = element_text(size = 15),
        strip.text = element_text(size = 15)) +
  ylab("Scores") +
  guides(colour=guide_legend(title="Group"))
# saved as 750*500

# looks like changes might be non linear
```

#### Euclidean distance between all PCA components (noLL dataset)

Two ways to do this.

First, as we have 6 components, I can compute a Euclidean distance value for every person across all 6 componets for each time point. This will give a value which is indicative of every person's profile (i.e., one value representing a mix of their 6 PCs). I can then compare this value between people andwithin/between groups to see who changes more with time. I will do this for all groups and for those whose Dx changes.

Update: in MALR meeting on 1/2/23, he said what this approach does is showcase how different they are from each other but in abstract terms because we are not calculating the difference to a fixed region. Instead he suggested we calculate (i) distance from centroid of the 6D space (i.e., all PCs) to each point and (ii) examine how tightly packed these individuals are at each time point (we can also do this by subgroup and by Dx change patients). This will tell us how different each person is to the whole group across time points. If we do this by subgroup, we can evaluate how different each person is from their own group at each time point. I do this analysis in a separate chunk below.

```{r}


# I will write some code where we calculate the Euclidean distance of each consecutive set of rows and add them togehter. I got this code from here: https://stackoverflow.com/questions/55885900/r-calculate-distance-between-first-and-current-row-of-grouped-dataframe

test <- fit_pca_noLL_scores_df_ID %>%
  # group by ID
  group_by(ID) %>%
  # arrange in the order of Visit
  arrange(Visit) %>%
  # calculate Euclidean distance between consecutive rows
  mutate(EucDist = sqrt(
    (RC1 - lag(RC1))^2 +
      (RC2 - lag(RC2))^2 +
      (RC3 - lag(RC3))^2 +
      (RC4 - lag(RC4))^2 +
      (RC5 - lag(RC5))^2 +
      (RC6 - lag(RC6))^2)) %>%
  # replace NAs in EucDist with 0
  mutate(EucDist = replace_na(EucDist, 0)) %>%
  ungroup()


# now plot this
fig1p2 <- test %>%
     ggplot(aes(x = Visit, 
                y = EucDist)) +
     facet_wrap(~Dx, nrow = 1) +
     geom_line(aes(group = ID),
               alpha = 0.2) +
     geom_point(size=3, 
                position=position_jitter(width=0.05), 
                alpha=0.2) +
     geom_boxplot(aes(group = Visit), alpha = 0.8) +
     theme_bw() +
   theme(panel.grid.major=element_blank(),
           axis.text.y = element_text(size = 12),
           axis.text.x = element_text(size = 12),
           axis.title.x = element_text(size = 12),
           axis.title.y = element_text(size = 12),
           strip.text = element_text(size = 12)) +
  ylab("Euclidean distance") +
  ggtitle("Magnitude of change in multidimensional PC space") +
  theme(plot.title = element_text(hjust = 0.5))
# saved as 550*550


# do significance testing (either by Visit (bettween group) or by Dx (within group))
test1 <- test %>%
  group_by(ID) %>%
  filter(Visit == 2) 

summary(aov(EucDist ~ as.factor(Dx), data = test1))  
eta_squared(aov(EucDist ~ as.factor(Dx), data = test1))
TukeyHSD(aov(EucDist ~ as.factor(Dx), data = test1))

# if running a linear model, best to remove T1 data as this is all 0 so it will cause model to not work (overfit)
test2 <- test %>%
  group_by(ID) %>%
  filter(!Visit == 1) 

# Between group comparisons For Visit 2 and 3 with ANOVA?



# Run a linear model (standard parameters) - ignore this bit of code - not used for main analysis

# diagnosis as fixed effects
# time as random and fixed effect
# random intercept for individual performance

#mod0 <- lmer(EucDist ~  Visit*Dx + (1 | ID), data = test2)

# extract coefficients
#coefs <- data.frame(coef(summary(mod0)))

# use normal distribution to approximate p-value
#coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
#coefs


# first, I will create a new column which calculates the Euclidean distance for every row of the dataset fit_pca_noLL_scores_df_ID. By specifying to calculate Euclidean distance foe the first 6 cols (i.e., the 6 PCs) I will find a value that represents the Euclidean distance of all 6PCs for each assessment time point


fit_pca_noLL_scores_df_ID$EucBetween <- apply(fit_pca_noLL_scores_df_ID[,1:6], 1, function(x) mean(dist(x, method = "euclidean"))) # applies Euclidean distance to the first 6 columns

# conduct anovas between visits for each patient group. Here I am doing this independnetly. Rewrite this bit of code for each patient group and as you find significnat differences, keep making a note in your manuscript.

fit_pca_scores_ppa <- test %>%
  group_by(ID) %>%
  filter(Dx == "PPA") %>%
  filter(n() == 3) 

summary(aov(EucBetween ~ Visit, data = fit_pca_scores_ppa))  
eta_squared(aov(EucBetween ~ Visit, data = fit_pca_scores_ppa))
TukeyHSD(aov(EucBetween ~ Visit, data = fit_pca_scores_ppa))


# calculate group differnces on euclidean distance by group and time
fit_pca_noLL_scores_df_ID %>%
  ggplot(aes(x = Visit, 
             y = EucBetween)) +
  facet_wrap(~Dx) +
  geom_line(aes(group = ID),
            alpha = 0.2) +
  geom_point(size=3, 
             position=position_jitter(width=0.05), 
             alpha=0.2) +
  geom_boxplot(alpha = 0.8) +
  theme_bw() +
   theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  ylab ("Euclidean distance across Components")
# saved as 700*700


#### For thosse whose Dx changes ####

# Below, I have created a new df with those whose Dx changes. I need to retain these IDs in the fit_pca_noLL_scores_df_ID df

fit_pca_noLL_scores_df_ID$Visit <- as.integer(fit_pca_noLL_scores_df_ID$Visit)

nacc_voi_dem_all_11_dxchange_pcscores <- nacc_voi_dem_all_11_dxchange %>% 
  left_join(fit_pca_noLL_scores_df_ID, by = c("ID", "Visit"))

# create a label with whatever was their first Dx
nacc_voi_dem_all_11_dxchange_pcscores_2 <- nacc_voi_dem_all_11_dxchange_pcscores %>%
  group_by(ID) %>% 
  # for some reason, some NAs have crept up so lets remove them.
  drop_na() %>%
  mutate(FirstDx = case_when(Visit == 1 & Dx == "AD" ~ "FirstDx - AD",
                             Visit == 1 & Dx == "bvFTD" ~ "FirstDx - bvFTD",
                             Visit == 1 & Dx == "FTLD-motor" ~ "FirstDx - FTLD-Motor",
                             Visit == 1 & Dx == "FTLD-NOS" ~ "FirstDx - FTLD-NOS",
                             Visit == 1 & Dx == "PPA" ~ "FirstDx - PPA")) %>%
  # fill this down into Group membership
  fill(FirstDx, .direction = "down")


# Divide this by group

fit_pca_scores_dxchange_firstdxftldm <- nacc_voi_dem_all_11_dxchange_pcscores_2 %>%
  group_by(ID) %>%
  filter(FirstDx == "FirstDx - FTLD-Motor") %>%
  filter(n() == 3) 

summary(aov(EucBetween ~ Visit, data = fit_pca_scores_dxchange_firstdxppa))  
eta_squared(aov(EucBetween ~ Visit, data = fit_pca_scores_dxchange_firstdxppa))
TukeyHSD(aov(EucBetween ~ Visit, data = fit_pca_scores_dxchange_firstdxppa))

# all
summary(aov(EucBetween ~ Visit, data = nacc_voi_dem_all_11_dxchange_pcscores_2))  
eta_squared(aov(EucBetween ~ Visit, data = nacc_voi_dem_all_11_dxchange_pcscores_2))
TukeyHSD(aov(EucBetween ~ Visit, data = nacc_voi_dem_all_11_dxchange_pcscores_2))

# plot these
nacc_voi_dem_all_11_dxchange_pcscores_2 %>%
  ggplot(aes(x = as.factor(Visit), 
             y = EucBetween)) +
  facet_wrap(~FirstDx) +
  geom_line(aes(group = ID),
            alpha = 0.2) +
  geom_point(size=3, 
             position=position_jitter(width=0.05), 
             alpha=0.2) +
  geom_boxplot(alpha = 0.8) +
  theme_bw() +
   theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  ylab("Euclidean distance across Components") +
  xlab("Visit")
# saved as 700*700


# also plot the alluvial plots for those whose Dx chnages

nacc_alluvium_dx_change <- nacc_voi_dem_all_11_dxchange_pcscores %>%
  # remove 2 NA
  drop_na() %>%
  dplyr::select(ID, Visit, Dx) %>% # select only these variables
  ggplot(aes(x = as.factor(Visit), stratum = Dx, # create the layers
             alluvium = ID, fill = Dx)) + 
  geom_flow() + # add the flow
  geom_stratum(alpha = .5) + # modify the transparence of the flow
  theme_bw() +
  theme_classic() +
  xlab("Visit number") +
  ylab ("Count") +
  guides(fill = guide_legend(title="Group")) +
  scale_fill_manual(values = group.colours) +
  # for some reason, I'm struggling to add in the numbers of each patient group at each time point so I'm going to manually add them in
  # for Time 1
  annotate("text", x = 1, y = 87, label = "2") + # AD
  annotate("text", x = 1, y = 64, label = "38") + # bvFTD
  annotate("text", x = 1, y = 42, label = "13") + # FTKD-m
  annotate("text", x = 1, y = 34, label = "2") + # FTLD-NOS
  annotate("text", x = 1, y = 17, label = "33") + # PPA
   # for time 2
  annotate("text", x = 2, y = 72, label = "19") + # AD
  annotate("text", x = 2, y = 62, label = "13") + # bvFTD
  annotate("text", x = 2, y = 53, label = "6") + # FTKD-m
  annotate("text", x = 2, y = 42, label = "15") + # FTLD-NOS
  annotate("text", x = 2, y = 17.5, label = "35") + # PPA
  # for time 3
  annotate("text", x = 3, y = 74, label = "21") + # AD
  annotate("text", x = 3, y = 64, label = "6") + # bvFTD
  annotate("text", x = 3, y = 56, label = "10") + # FTKD-m
  annotate("text", x = 3, y = 42.5, label = "17") + # FTLD-NOS
  annotate("text", x = 3, y = 18, label = "34") +  # PPA
   # change the text size for axes
  theme(axis.text= element_text(size=12), axis.title = element_text(size=12)) +
  ggtitle("Individuals with diagnosis change") +
  theme(plot.title = element_text(hjust = 0.5))
# saved as 700*500

# multiplot with the other alluvial plot
ggpubr::ggarrange(nacc_alluvium_full,
                   nacc_alluvium_dx_change,
                  labels = "AUTO",
                  ncol = 2,
                  nrow = 1, 
                  common.legend = T, 
                  legend = "bottom")

# saved as 1200*800
```

#### Centroid based calculation of distance

```{r}

# the centroid is essentially the mean of each coordinate space. I found a data table code here: https://stackoverflow.com/questions/38699761/getting-the-centroids-of-lat-and-longitude-in-a-data-frame. Tried modifying this code but it doesn't work.


# For all pts and all time points

# I'll calculate the mean of each coordinate space and then take the mean of those means. That will be the centre value of the entire 6d PC space. Then I'll calculate the difference for every value from that value

# lets first calcualte the mean of those means
fit_pca_noLL_scores_df_ID %>%
  # compute colwise mean
    summarise(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6)) %>%
  # compute centroid of all cols (i.e., grand mean)
  summarise(grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) 
  # grandmean/centroid is 2.291839e-17

# now find the distance of for all PCS combined from this value
fig1p1 <- fit_pca_noLL_scores_df_ID %>%
  group_by(ID) %>%
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-2.291839e-17) %>%
  # plot this on ggplot 
  ggplot(aes(x = Visit, y = Dispersion, group = Visit)) + 
  geom_point(aes(fill = Dispersion), 
             size=3, 
             position=position_jitter(width=0.05), 
             alpha=0.1) +
  geom_boxplot(alpha = .9) +
  facet_wrap(~Dx, nrow=1) +
  geom_line(aes(group = ID),
            alpha = .1) +
   # draw line at centroid of all values
  geom_hline(yintercept = -2.291839e-17, 
             colour = "#E69F00",
             size = 1) +
  theme_bw() +
  theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12),
        legend.position = "none") +
  ylab("Global dispersion index") +
  ggtitle("Global dispersion in multidimensional PC space") +
  theme(plot.title = element_text(hjust = 0.5))
  
# saved as 700*700


# I will save this as a new df to be able to do mixed modeling on this data
nacc_pca_dispersion_all <- fit_pca_noLL_scores_df_ID %>%
  group_by(ID) %>%
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-2.291839e-17)

# plot all in one group
nacc_pca_dispersion_all %>%
 ggplot(aes(x = Visit,
             y = Dispersion,
             colour = Dx,
             group = Dx)) +
  geom_point(position = "jitter",
             alpha = 0.1,
             size = 2) +
  stat_smooth(method = lm, se = T, fullrange = TRUE) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(axis.text = element_text(size = 15), 
        axis.title = element_text(size = 15),
        strip.text = element_text(size = 15)) +
  guides(colour=guide_legend(title="Group"))  + 
  # draw line at centroid of all values
  geom_hline(yintercept = -2.291839e-17, 
             colour = "#E69F00",
             size = 1,
             lty = 2)

# fit a mixed model
# diagnosis as fixed effect
# time as random and fixed effect
# random intercept for individual performance
# random slope for Visit

mod1 <- lmer(Dispersion ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# now, I split this nacc_pca_dispersion_all dataset inot those with Dx change and no Dx change. Then I will model two separate lmers and do an anova between the lmers to see these models are different.

# for this, I use the Dx change patients in nacc_voi_dem_all_11_dxchange_pcscores
# lets keep just an ID variable for nacc_voi_dem_all_11_dxchange_pcscores people and then subset the nacc_pca_dispersion_all df for these dx change individuals

nacc_dxchange_ID <- nacc_voi_dem_all_11_dxchange_pcscores %>% 
  # group by ID
  group_by(ID) %>% 
# keep only the ID variable
  dplyr::select(ID) %>%
  # keep only one occurrence of each ID
  unique() %>%
  # turn this into a df
  as.data.frame()

nacc_pca_dispersion_all_dxchange <- nacc_pca_dispersion_all %>%
# group by ID
  group_by(ID) %>%
  # keep only those IDs from nacc_dxchange_ID df
  filter(ID %in% unique(nacc_dxchange_ID$ID)) %>%
  # make a new grouping to delineate what their first Dx was
  mutate(FirstDx = case_when(Visit == 1 & Dx == "AD" ~ "FirstDx - AD",
                             Visit == 1 & Dx == "bvFTD" ~ "FirstDx - bvFTD",
                             Visit == 1 & Dx == "FTLD-motor" ~ "FirstDx - FTLD-Motor",
                             Visit == 1 & Dx == "FTLD-NOS" ~ "FirstDx - FTLD-NOS",
                             Visit == 1 & Dx == "PPA" ~ "FirstDx - PPA")) %>%
  # fill this down into Group membership
  fill(FirstDx, .direction = "down")


# now also build a df with those who have stable dx
nacc_pca_dispersion_all_dxstable <- nacc_pca_dispersion_all %>%
# group by ID
  group_by(ID) %>%
  # remove those IDs from nacc_dxchange_ID df
  anti_join(nacc_dxchange_ID, by = "ID")


# build linear models for dispersion indices in both groups and compare them

# stable group
mod_stable1 <- lmer(Dispersion ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_all_dxstable)

# extract coefficients
coefs_mod_stable1 <- data.frame(coef(summary(mod_stable1)))

# use normal distribution to approximate p-value
coefs_mod_stable1$p.z <- 2 * (1 - pnorm(abs(coefs_mod_stable1$t.value)))
coefs_mod_stable1


# dx change group (with random effect for group also)
mod_dxchange1 <- lmer(Dispersion ~  Visit*FirstDx + (1 + Visit|ID) + (0 + FirstDx), data = nacc_pca_dispersion_all_dxchange)

# extract coefficients
coefs_mod_dxchange1 <- data.frame(coef(summary(mod_dxchange1)))

# use normal distribution to approximate p-value
coefs_mod_dxchange1$p.z <- 2 * (1 - pnorm(abs(coefs_mod_dxchange1$t.value)))
coefs_mod_dxchange1


anova(mod_stable1, mod_dxchange1)


# repeat using group-wise centroids
# calculate groupwise means
fit_pca_noLL_scores_df_ID %>%
  group_by(Dx) %>%
  # compute colwise mean and grand mean of each row
  mutate(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6), 
           grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) %>%
           # now calculate a new score with the rowwise PC means
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-grandmean) %>%
  # plot the dispersion value
   # plot this on ggplot 
  ggplot(aes(x = Visit, y = Dispersion, group = Visit)) + 
  geom_point(aes(fill = Dispersion), 
             size=3, 
             position=position_jitter(width=0.05), 
             alpha=0.1) +
  geom_boxplot(alpha = .9) +
  facet_wrap(~Dx) +
  geom_line(aes(group = ID),
            alpha = .1) +
  # the geom_h line estiamtes are mentioned below in the df tmp
  geom_hline(data = tmp, 
             aes(yintercept = grandmean),
             colour = "#E69F00",
             size = 1) +
# draw line at centroid of all values
  theme_bw() +
  theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12),
        legend.position = "none") 
# saved as 700*700


tmp <- data.frame(Dx = c("AD", "bvFTD", "FTLD-motor", "FTLD-NOS", "PPA"),
                  grandmean = c(0.0494, -0.102, -0.244, -0.0880, 0.147)) 
  

# create a DF of this data to explore changes groupwise
nacc_pca_dispersion_groupwise <- fit_pca_noLL_scores_df_ID %>%
  group_by(Dx) %>%
  # compute colwise mean and grand mean of each row
  mutate(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6), 
           grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) %>%
           # now calculate a new score with the rowwise PC means
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-grandmean)

# split this df into their own groups
nacc_pca_dispersion_groupwise_AD <- nacc_pca_dispersion_groupwise %>% group_by(ID) %>% filter (Dx == "AD")
nacc_pca_dispersion_groupwise_bv <- nacc_pca_dispersion_groupwise %>% group_by(ID) %>% filter (Dx == "bvFTD")
nacc_pca_dispersion_groupwise_ftldm <- nacc_pca_dispersion_groupwise %>% group_by(ID) %>% filter (Dx == "FTLD-motor")
nacc_pca_dispersion_groupwise_ftldnos <- nacc_pca_dispersion_groupwise %>% group_by(ID) %>% filter (Dx == "FTLD-NOS")
nacc_pca_dispersion_groupwise_PPA <- nacc_pca_dispersion_groupwise %>% group_by(ID) %>% filter (Dx == "PPA")

# run mixed models within each group
# time as random and fixed effect
# random intercept for individual performance
# random slope for Visit

# For AD
mod1 <- lmer(Dispersion ~  Visit + (1 + Visit|ID), data = nacc_pca_dispersion_groupwise_AD)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# For bv
mod1 <- lmer(Dispersion ~  Visit + (1 + Visit|ID), data = nacc_pca_dispersion_groupwise_bv)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# For FTLDm
mod1 <- lmer(Dispersion ~  Visit + (1 + Visit|ID), data = nacc_pca_dispersion_groupwise_ftldm)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# For ftldnos
mod1 <- lmer(Dispersion ~  Visit + (1 + Visit|ID), data = nacc_pca_dispersion_groupwise_ftldnos)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# For PPA
mod1 <- lmer(Dispersion ~  Visit + (1 + Visit|ID), data = nacc_pca_dispersion_groupwise_PPA)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# Examine changes in Dispersion with CDR and visit

# first left join in CDR for both stable and dx change dfs. There is a df below called umap_df_rank_withMeta_1, which I'll merge in here for simplicity
nacc_pca_dispersion_all_CDR <-  nacc_pca_dispersion_all %>%
  left_join(umap_df_rank_withMeta_1, by = c("ID", "Visit")) %>% 
  select(ID, Visit, Dx, Dispersion, CDRSUM)

# correlations
nacc_pca_dispersion_all_CDR %>% group_by(Dx) %>% summarise(cor(CDRSUM, Dispersion))
# p value for correlation
nacc_pca_dispersion_all_CDR %>% group_by(Dx) %>% summarise(cor.test(CDRSUM, Dispersion)[["p.value"]])




# REPEAT CENTROID ANALYSIS FOR Dx CHANGE using groupwise centroids

nacc_voi_dem_all_11_dxchange_pcscores %>%
  group_by(ID) %>%
  # for some reason, some NAs have crept up so lets remove them.
  drop_na() %>%
  # create a new Dx col
  mutate(FirstDx = case_when(Visit == 1 & Dx == "AD" ~ "FirstDx - AD",
                             Visit == 1 & Dx == "bvFTD" ~ "FirstDx - bvFTD",
                             Visit == 1 & Dx == "FTLD-motor" ~ "FirstDx - FTLD-Motor",
                             Visit == 1 & Dx == "FTLD-NOS" ~ "FirstDx - FTLD-NOS",
                             Visit == 1 & Dx == "PPA" ~ "FirstDx - PPA")) %>%
  # fill this down into Group membership
  fill(FirstDx, .direction = "down") %>%
  ungroup() %>%
  group_by(FirstDx) %>%
  # compute colwise mean and grand mean of each row
  mutate(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6), 
           grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) %>%
           # now calculate a new score with the rowwise PC means
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-grandmean) %>%
  # plot the dispersion value
   # plot this on ggplot 
  ggplot(aes(x = Visit, y = Dispersion, group = Visit)) + 
  geom_point(aes(fill = Dispersion), 
             size=3, 
             position=position_jitter(width=0.05), 
             alpha=0.1) +
  geom_boxplot(alpha = .9) +
  facet_wrap(~FirstDx) +
  geom_line(aes(group = ID),
            alpha = .1) +
  # the geom_h line estiamtes are mentioned below in the df tmp
  geom_hline(data = tmp_dxchange, 
             aes(yintercept = grandmean),
             colour = "#E69F00",
             size = 1) +
# draw line at centroid of all values
  theme_bw() +
  theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12),
        legend.position = "none") 
# saved as 700*700


tmp_dxchange <- data.frame(FirstDx = c("FirstDx - AD", "FirstDx - bvFTD", "FirstDx - FTLD-Motor", "FirstDx - FTLD-NOS", "FirstDx - PPA"),
                  grandmean = c(0.247 , -0.130, -0.0824, 0.0552, 0.0858))


# explore changes in these people as well
nacc_pca_dispersion_dxchange <- nacc_voi_dem_all_11_dxchange_pcscores %>%
  group_by(ID) %>%
  # for some reason, some NAs have crept up so lets remove them.
  drop_na() %>%
  # create a new Dx col
  mutate(FirstDx = case_when(Visit == 1 & Dx == "AD" ~ "FirstDx - AD",
                             Visit == 1 & Dx == "bvFTD" ~ "FirstDx - bvFTD",
                             Visit == 1 & Dx == "FTLD-motor" ~ "FirstDx - FTLD-Motor",
                             Visit == 1 & Dx == "FTLD-NOS" ~ "FirstDx - FTLD-NOS",
                             Visit == 1 & Dx == "PPA" ~ "FirstDx - PPA")) %>%
  # fill this down into Group membership
  fill(FirstDx, .direction = "down") %>%
  ungroup() %>%
  group_by(FirstDx) %>%
  # compute colwise mean and grand mean of each row
  mutate(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6), 
           grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) %>%
           # now calculate a new score with the rowwise PC means
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-grandmean)


# Fit linear models
# time as random and fixed effect
# Dx as random effect and fixed effect
# random intercept for individual performance
# random slope for Visit

mod1 <- lmer(Dispersion ~  Visit*FirstDx + (1|Visit:FirstDx) + (1 + Visit|ID), data = nacc_pca_dispersion_dxchange)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs



```

#### Euclidean distance within each component and PC specific centroid (noLL dataset)

I can now calculate the Euclidean distance within each component for each person between their visits and see which group moves the most. This seems a bit complex to do as ther is no function written for calculating distance between two values (not points) in consecutive rows so I'll have to improvise.

I also calcualte centroids for each PC and model group changes on that.

```{r}

fit_pca_noLL_scores_df_ID_distmatrix_within <- fit_pca_noLL_scores_df_ID %>%
  arrange(ID) %>%
  group_by(ID) %>%
  # take the difference between every consecutive row, within the grouped data and turn it into an absolute value 
  mutate(diffRC1 = RC1 - lag(RC1),
         diffRC2 = RC2 - lag(RC2),
         diffRC3 = RC3 - lag(RC3),
         diffRC4 = RC4 - lag(RC4),
         diffRC5 = RC5 - lag(RC5),
         diffRC6 = RC6 - lag(RC6)) %>% 
  # replace 0 with NA
  mutate(diffRC1 = replace_na(diffRC1, 0),
         diffRC2 = replace_na(diffRC2, 0),
         diffRC3 = replace_na(diffRC3, 0),
         diffRC4 = replace_na(diffRC4, 0),
         diffRC5 = replace_na(diffRC5, 0),
         diffRC6 = replace_na(diffRC6, 0)
         ) %>% 
  # turn it into absolute value by squaring then deriving sqrt (could have also done abs)
  mutate(distRC1 = sqrt(diffRC1^2),
         distRC2 = sqrt(diffRC2^2),
         distRC3 = sqrt(diffRC3^2),
         distRC4 = sqrt(diffRC4^2),
         distRC5 = sqrt(diffRC5^2),
         distRC6 = sqrt(diffRC6^2))

# ggplot these values
fit_pca_noLL_scores_df_ID_distmatrix_within %>%
  dplyr::select(ID, Dx, Visit, starts_with("dist")) %>%
  rename(PC1 = distRC1,
         PC2 = distRC2,
         PC3 = distRC3,
         PC4 = distRC4,
         PC5 = distRC5,
         PC6 = distRC6) %>%
  pivot_longer(!c(ID, Dx, Visit),
               names_to = "Components",
               values_to = "Value") %>%
  ggplot(aes(x = Visit, y = Value, colour = Dx)) +
  geom_point(aes(fill=Dx), size=5, shape=21, position=position_jitter(width=0.2), alpha=0.05) +
  facet_wrap(~Components,scales = "free") +
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.y = mean, geom = "line", aes(group = Dx), size = 1) + 
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 1) +
  theme_bw() + 
  # change x and y axis coordinates
  theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
#  labs(title=expression('Euclidean distance between visits'))  +
#  theme(plot.title = element_text(hjust = 0.5, size=22)) +
  scale_colour_manual(values=group.colours) +
  scale_fill_manual(values=group.colours) +
  guides(fill=guide_legend(title="Group")) +
  guides(colour=guide_legend(title="Group"))
# saved as 1600*800


# PC specfiic centroid
nacc_pca_dispersion_PCspecific_all <- fit_pca_noLL_scores_df_ID %>%
  # calculate mean of each of the RC columns
  mutate(meanRC1 = mean(RC1),
         meanRC2 = mean(RC2),
         meanRC3 = mean(RC3),
         meanRC4 = mean(RC4),
         meanRC5 = mean(RC5),
         meanRC6 = mean(RC6)) %>%
  # compute difference between everyone's PC score with the mean of the overall PC
  mutate(RC1Dist = RC1-meanRC1,
         RC2Dist = RC2-meanRC2,
         RC3Dist = RC3-meanRC3,
         RC4Dist = RC4-meanRC4,
         RC5Dist = RC5-meanRC5,
         RC6Dist = RC6-meanRC6) %>%
  group_by(ID) %>%
  # calculate the average distance travelled from Centroid (i.e, mean dispersion per person over time)
  mutate(MeanRC1Dist = mean(RC1Dist),
            MeanRC2Dist = mean(RC2Dist),
            MeanRC3Dist = mean(RC3Dist),
            MeanRC4Dist = mean(RC4Dist),
            MeanRC5Dist = mean(RC5Dist),
            MeanRC6Dist = mean(RC6Dist)) %>%
  dplyr::select(ID, Dx, Visit, RC1Dist:RC6Dist) %>%
  rename("PC1: Func. stat." = "RC1Dist",
         "PC2: Apat./impuls." = "RC2Dist",
         "PC3: Motor" = "RC3Dist",
         "PC4: Psychosis" = "RC4Dist",
         "PC5: Affective" = "RC5Dist",
         "PC6: Depression" = "RC6Dist") %>%
  pivot_longer(!c(ID, Dx, Visit), names_to = "Components", values_to = "Value")

# create a DF with the mean of each PC so that you can plot it

tmp_pc_dispersion <- data.frame(Components = c("PC1: Func. stat.", "PC2: Apat./impuls.", "PC3: Motor", "PC4: Psychosis", "PC5: Affective", "PC6: Depression"), Value = c(-6.343129e-17, 2.137995e-16, -5.163548e-17, -4.055861e-17, -1.129109e-16, -8.277355e-17))

# plot this
fig1p3 <- nacc_pca_dispersion_PCspecific_all %>%
  ggplot(aes(x = as.factor(Visit), y = Value, colour = Dx)) +
  geom_point(aes(fill=Dx), size=5, shape=21, position=position_jitter(width=0.2), alpha=0.05) +
  facet_wrap(~Components, scales = "free") +
  # plot PC specific centroid
  geom_hline(data = tmp_pc_dispersion, 
             aes(yintercept = Value),
             colour = "#E69F00",
             size = 1) +
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.y = mean, geom = "line", aes(group = Dx), size = 1) + 
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 1) +
  theme_bw() + 
  theme(panel.grid.major=element_blank(),
        axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12),
        legend.position = "bottom") +
  labs(title=expression('PC-specific dispersion index'))  +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_colour_manual(values=group.colours) +
  scale_fill_manual(values=group.colours) +
  ylab("PC-specific dispersion") +
  xlab("Visit") +
  guides(fill=guide_legend(title="Group")) +
  guides(colour=guide_legend(title="Group"))
  

# multiplot a Fig 1 panels A and B so they don't take up all of the aspect ratio of the final figure
Fig1pAB <- cowplot::plot_grid(fig1p1a,  
                              fig1p1,
                              fig1p2,
                              fig1p3,
                              labels="AUTO",
                   ncol=2)
# saved as 2100*1300



# now calculate linear models on each PC by visit and group

# fit a mixed model
# diagnosis as fixed effect
# time as random and fixed effect
# random intercept for individual performance
# random slope for Visit

# PC1
mod11 <- lmer(RC1Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod11)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# PC2
mod12 <- lmer(RC2Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod12)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# PC3
mod13 <- lmer(RC3Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod13)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


# PC4
mod14 <- lmer(RC4Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod14)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs



# PC5
mod15 <- lmer(RC5Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod15)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs



# PC6
mod16 <- lmer(RC6Dist ~  Visit*Dx + (1 + Visit|ID), data = nacc_pca_dispersion_PCspecific_all)

# extract coefficients
coefs <- data.frame(coef(summary(mod16)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs


```

### Multidimensional visualisation with UMAP (noLL dataset)

I spent some time wondering if tSNE or UMAP are better. There is now lot of literature that UMAP may be better for large dataset and produces more consistent results and its hyperparameters are easier to understand (<https://blog.bioturing.com/2022/01/14/umap-vs-t-sne-single-cell-rna-seq-data-visualization/>). It also is computationally faster than tSNE. See some more resources here (<https://distill.pub/2016/misread-tsne/>).

First I thought I'll use raw data (pre-PCA), i.e., test_2. UMAP typically does a PCA like reduction with distance counting (euclidean) - and it is suggested to initialise the data with PCA data. So I'm entering the PCA scores into the UMAP.

```{r}

# first, select the rotated PC scores for the UMAP
test_umap <- fit_pca_noLL_scores_df_ID %>%
  # select scores of interest
  select(1:6) %>%
  # create a row number ID so we can left-join in the meta-data later
  mutate(ID.x=row_number()) 

# create a meta-dataset with ID and identifiers that we can left-join in later
test_umap_meta <- fit_pca_noLL_scores_df_ID %>%
  select(ID, Dx, Visit) %>%
  mutate(ID.x=row_number()) 

# run the UMAP
umap_fit <- test_umap %>% 
  column_to_rownames("ID.x") %>%
  umap(input = "data" # inputting a data matrix, not a distance matrix
       )
       #) # note that I am using the default parameters with k nearest neighbours as 15

# extract the UMAP dimensions and merge the scores in
umap_df <- umap_fit$layout %>% 
  as.data.frame() %>%
  mutate(ID.x=row_number())

# join in the meta-data
umap_df <- umap_df %>%
  inner_join(test_umap_meta, by="ID.x")

# make a first plot of the UMAP results
umap_p1 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point() +
  geom_line(aes(group=ID)) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = "bottom", #c(0.15,0.2),
        legend.background = element_rect(colour = "black")) +
  guides(colour=guide_legend(title="Group")) +
  ggtitle("All groups") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("All groups") +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
# wow! Great structure! it looks like the center is a "black hole" of sorts but lets see if this is true if we code it by shape for each visit

# lets do this plot again but add shape to geom point to code each visit 
# to make this clearer, lets highlight only one group at a time.

# umap_df %>%
#   ggplot(aes(x = V1, 
#              y = V2,
#              color = Dx)) +
#   geom_point(aes(shape = factor(Visit))) +
#   geom_line(aes(group=ID)) +
#   scale_colour_manual(values = group.colours) +
#   theme_bw() +
#   theme(legend.position = c(0.1,0.2),
#         legend.background = element_rect(colour = "black"),
#         legend.title = element_text(size = 12),
#         legend.text = element_text(size = 12)) +
#   guides(colour=guide_legend(title="Group")) +
#   ggtitle("All groups") +
#   theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# lets now do a few versions of this. First we can see where everyone is at Time 1

umap_v1plot <- umap_df %>%
  filter(Visit == 1) %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position =  "none") + #c(0.15,0.2),
      #  legend.background = element_rect(colour = "black")) +
  guides(colour=guide_legend(title="Group")) +
  ggtitle("First visit") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# at time 2

umap_v2plot <- umap_df %>%
  filter(Visit == 2) %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position =  "none") + #c(0.15,0.2),
      #  legend.background = element_rect(colour = "black")) +
  guides(colour=guide_legend(title="Group")) +
  ggtitle("Second visit") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# at time 3

umap_v3plot <- umap_df %>%
  filter(Visit == 3) %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position =  "none") + #c(0.15,0.2),
      #  legend.background = element_rect(colour = "black")) +
  guides(colour=guide_legend(title="Group")) +
  ggtitle("Third visit") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))


# make a multiplot of the three time point movement
fig2p2 <- cowplot::plot_grid(umap_v1plot,
                   umap_v2plot,
                   umap_v3plot,
                   labels = c("B", "C", "D"),
                   ncol = 3)

# now highlight the movement of each group across the time periods

# AD
umap_p2 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(aes(shape = Visit), size = 2) +
  geom_line(aes(group=ID)) +
  gghighlight(Dx == "AD") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("AD") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# bvFTD
umap_p3 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(aes(shape = Visit), size = 2) +
  geom_line(aes(group=ID)) +
  gghighlight(Dx == "bvFTD") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("bvFTD") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# FTLD-motor
umap_p4 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(aes(shape = Visit), size = 2) +
  geom_line(aes(group=ID)) +
  gghighlight(Dx == "FTLD-motor") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-motor") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# FTLD-NOS
umap_p5 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(aes(shape = Visit), size = 2) +
  geom_line(aes(group=ID)) +
  gghighlight(Dx == "FTLD-NOS") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-NOS") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

# PPA
umap_p6 <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(aes(shape = Visit), size = 2) +
  geom_line(aes(group=ID)) +
  gghighlight(Dx == "PPA") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PPA") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
  
# create a multiplot for all groups and group-wise
cowplot::plot_grid(umap_p1, 
          umap_p2,
          umap_p3,
          umap_p4,
          umap_p5,
          umap_p6,
          labels = c("A", "B", "C", "D", "E", "F"))

#saved as 2000*1200 dim

# create a multiplot for movement in each timepoint
cowplot::plot_grid(umap_p1,
  umap_v1plot, 
          umap_v2plot,
          umap_v3plot,
          labels = c("A", "B", "C", "D"),
          ncol = 4)
# saved as 1600*550 dim


# each group's single plot saved with 550*550 dim

# 
# umap_df %>%
#   ggplot(aes(x = V1, 
#              y = V2,
#              color = Dx)) +
#   geom_point(data = ~subset(., Dx == "AD"), 
#              aes(shape = factor(Visit))) +
#   geom_line(data = ~subset(., Dx == "AD"), 
#             aes(group=ID)) +
#   theme(legend.position="bottom")

```

#### Animate movement with time on UMAP

A good way of visualising the movement is to animate it for each patient group. For this, I can use the `gganimate` package.

```{r}

# for all groups. I will simply use the images created before and add an animation layer to it.
# first, I use the code from above to create a plot of all patients and all time points without any connections.
umap_animate_all <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.15,0.2),
        legend.background = element_rect(colour = "black")) +
  guides(colour=guide_legend(title="Group")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_all, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("output.gif")


# Per group
# AD
umap_animate_AD <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) + Commenting this out as I don't want to show connecting lines
  gghighlight(Dx == "AD") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("AD") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_AD, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("umap_animate_AD.gif")


# bvFTD
umap_animate_bvFTD <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
 # geom_line(aes(group=ID)) +
  gghighlight(Dx == "bvFTD") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("bvFTD") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_bvFTD, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("umap_animate_bvFTD.gif")


# FTLD-motor
umap_animate_ftldm <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  gghighlight(Dx == "FTLD-motor") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-motor") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_ftldm, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("umap_animate_ftldm.gif")


# FTLD-NOS
umap_animate_ftldnos <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  gghighlight(Dx == "FTLD-NOS") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-NOS") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_ftldnos, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("umap_animate_ftldnos.gif")


# PPA
umap_animate_PPA <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Dx)) +
  geom_point(size = 2) +
#  geom_line(aes(group=ID)) +
  gghighlight(Dx == "PPA") +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PPA") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    # animate the plot
  transition_time(as.numeric(Visit)) +
  labs(subtitle = "Visit: {frame_time}")

animate(umap_animate_PPA, width = 550, height = 550, duration = 10, fps = 6, end_pause = 15, renderer = gifski_renderer())
anim_save("umap_animate_PPA.gif")

```

#### Calculate density of distribution on 2D plot for each patient group

To understand the distribution of each patient group's data on this UMAP, we can calculate 2D kernel density estimates using `geom_density_2d`. More on this here: <https://gis.stackexchange.com/questions/389739/how-to-interpret-kernel-density-maps> and <https://stackoverflow.com/questions/38761453/confusion-on-2-dimension-kernel-density-estimation-in-r>

```{r}

# For all groups

umap_kdensity_all <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  stat_density2d_filled(contour_var = "ndensity") + # contour_var = "ndensity" displays the density estimate scaled to a max of 1
  # say I chuck in an ellipse for each time point 
  facet_wrap(~Dx, scales = "free", ncol=5) +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  # shift legend into the FTLD motor panel
  theme(legend.position = c(0.93,0.77),
        legend.background = element_rect(colour = "white"),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9)) +
  guides(fill = guide_legend(title = "Scaled density")) +
  ggtitle("All time points") +
  theme(plot.title = element_text(hjust = 0.5))
# sved as 1200*800

# stratified by visit

umap_kdensity_bytime <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  stat_density2d_filled(contour_var = "ndensity") + # contour_var = "ndensity" displays the density estimate scaled to a max of 1
  # say I chuck in an ellipse for each time point 
   facet_grid("Visit" + Visit ~ Dx, scales = "free") + # the "Visit" adds the text Visit before Visit number +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  # shift legend into the FTLD motor panel
  theme(#legend.position = c(0.93,0.77),
        legend.background = element_rect(colour = "white"),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9)) +
  guides(fill = guide_legend(title = "Scaled density")) +
  ggtitle("Stratified by visit") +
  theme(plot.title = element_text(hjust = 0.5))

# make this a multiplot
cowplot::plot_grid(umap_kdensity_all,
                   umap_kdensity_bytime,
                   ncol=1,
                   labels = "AUTO")

# make another multiplot with the UMAP main on top, UMAP time wise under that, and kdensity_by_time under that
fig2a <- plot_grid(umap_p1,
                  fig2p2, 
                  labels = c("A", ""),
                  rel_heights = c(2,1),
                  rel_widths = c(1,2),
                  ncol = 1)

fig2 <- plot_grid(fig2a,
                  umap_kdensity_bytime, 
                  labels = c("", "E"),
                 # rel_heights = c(2,1),
                #  rel_widths = c(1,2),
                  ncol = 1)
# sved 800*1600


# for All groups - probabilty maps

# The issue with the above plot is that it doesn't allow us to calculate probability. That's because what stat_density2d_filled does is find the intersections between the height of each density curve (in 3d) and the area it occupies. Instead, geom_hdr from ggdensity can circumvent this problem by calculating probabilities of the data lying in that space. We can tweak the probability using the probs argument within geom_hdr. I will plot this by Visit

umap_kdensity_all_probs <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  facet_grid("Visit" + Visit ~ Dx, scales = "free") + # the "Visit" adds the text Visit before Visit number
  geom_hdr() + 
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        strip.text.x = element_text(size = 12),
        strip.text.y = element_text(size = 12))
# saved as

# redo the same plot with ellipses for each visit point

umap_kdensity_byvisit <- umap_df %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  stat_density2d_filled(contour_var = "ndensity") + # contour_var = "ndensity" displays the density estimate scaled to a max of 1
  # say I chuck in an ellipse for each time point 
  facet_wrap(~Dx, scales = "free") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  # shift legend into the FTLD motor panel
  theme(legend.position = c(0.93,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9)) +
  guides(fill = guide_legend(title = "Scaled density")) +
  stat_ellipse(aes(colour = Visit),
               linetype = 2,
               size = 1,
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  ggtitle("Stratified by visit") +
  theme(plot.title = element_text(hjust = 0.5))
# sved as 1200*800 

```

#### Clustering/dispersion of points within UMAP

This UMAP space gives a nice opportunity to evaluate wheterh groups are clustered with time or not. I can do this using spatial point pattern analysis (<https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html>). Lets try this out.

```{r}

# first, let me play around with a temporary df for the full group
temp <- umap_df_rank_withMeta_1 %>% 
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# repeat the same for each Dx group
# AD
temp_ad <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>%
  filter(Dx.x == "AD") %>%
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# bvFTD
temp_bvftd <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>% 
  filter(Dx.x == "bvFTD") %>%
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# FTLD-Motor
temp_ftldm <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>% 
  filter(Dx.x == "FTLD-motor") %>%
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# FTLD-NOS
temp_ftldnos <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>%
  filter(Dx.x == "FTLD-NOS") %>%
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# PPA
temp_ppa <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>% 
  filter(Dx.x == "PPA") %>%
  dplyr::select(V1, V2, ID, Dx.x, Visit) %>%
  dplyr::rename(x = V1, y = V2)


# now I have to make a temporary outline of the UMAP space. This is because, when I biuld a null model to understand the spread of the data, I want the data to be constrained within the UMAP polygon
# for this, I can use the ggbiuld function (https://stackoverflow.com/questions/65313262/can-you-get-polygon-coordinates-from-a-ggplot2geom-density-2d-filled-plot)

# first, I need to find the outer parameters of the polygon that encloses the full UMAP space

# for this, I find the convex hull of the UMAP space. These are the outermost points of this polygon - literally, the outline of the polygon
chull(temp_coords$x, temp_coords$y)
# the output is the row numbers that lie on the convex hull. take the X & Y coordinates of these row numbers save them as X-coord and Y-coord vectors and then create a polygon with the code below.

# I'll call these row numbers and pull out the x&y coords and save them in the vector below

umap_chull_x_coord <- c(3.964064, # row 762
                        3.30197, # row 723
                        2.623577, # row 774
                        2.113068, # row 1165
                        -0.9987727, # row 946
                        -5.611988, # row 988
                        -6.117606, # row 247
                        -6.069356, # row 1002
                        -5.479685, # row 636
                        -2.866324, # row 866
                        -2.40756, # row 84
                        4.453479, # row 802
                        4.654693, # row 206
                        4.779662, # row 824
                        5.284508, # row 746
                        5.408121, # row 801
                        5.510033 # # row 1135
                        )

umap_chull_y_coord <- c(-2.128761, # row 762
                        -3.890932, # row 723
                        -5.321749, # row 774
                        -6.341354, # row 1165
                        -3.692227, # row 946
                        0.4421464, # row 988
                        1.106894, # row 247
                        1.271868, # row 1002
                        2.349177, # row 636
                        4.369899, # row 866
                        4.440421, # row 84
                        3.554285, # row 802
                        3.501049, # row 206
                        3.455073, # row 824
                        2.875181, # row 746
                        2.704847, # row 801
                        2.50476 # row 1135
                        )

# then i turn this into a spatial points object using the code below:
xym <- cbind(umap_chull_x_coord, umap_chull_y_coord) # bind these coords into a df
p = Polygon(xym) # create a polygon
ps = Polygons(list(p),1) # polygon creation
sps = SpatialPolygons(list(ps)) # create a spatial image out of this polygon
plot(sps) # plot the polygon


# for whole group

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp <- st_as_sf(temp, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp.sp <- as(spatial.temp, "Spatial")  # turn it into a spatial df
class(spatial.temp.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp1 <- as_Spatial(spatial.temp)
ppp.spatial.temp <- as.ppp(spatial.temp1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann.p <- mean(nndist(ppp.spatial.temp))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann.r > ann.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann.r, main= "Full group (all time points)", las=1, breaks=40, col="grey", xlim=range(ann.p, ann.r), xlab = "ANN value")
abline(v=ann.p, col="black", lwd = 4)
# saved as 400*400


####### NOTE ######
### The code below is not being used anymore because I decided to not run a null model that is randomyl distributed. Note, I am not using this analysis anymore because, the first finding that the whole dataset has some clustering pattern that is not randomly distributed means that I cannot assume the null model is random anymore. Therefore, this part of the text below is not useful as the comparison null model was randomly dispersed points within the convex hull of the UMAP.
#### Instead, skip to the next code chunk because the null model is made using Dx shuffling of the group....


# AD

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_AD <- st_as_sf(temp_ad, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_AD.sp <- as(spatial.temp_AD, "Spatial")  # turn it into a spatial df
class(spatial.temp_AD.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_AD1 <- as_Spatial(spatial.temp_AD)
ppp.spatial.temp_AD <- as.ppp(spatial.temp_AD1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_AD.p <- mean(nndist(ppp.spatial.temp_AD))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_AD.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_AD$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_AD.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_AD.r > ann_AD.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_AD.r, main= "AD (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_AD.p, ann_AD.r), xlab = "ANN value")
abline(v=ann_AD.p, col="#D55E00", lwd = 4)



# bvFTD

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_bvftd <- st_as_sf(temp_bvftd, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_bvftd.sp <- as(spatial.temp_bvftd, "Spatial")  # turn it into a spatial df
class(spatial.temp_bvftd.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_bvftd1 <- as_Spatial(spatial.temp_bvftd)
ppp.spatial.temp_bvftd <- as.ppp(spatial.temp_bvftd1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_bvftd.p <- mean(nndist(ppp.spatial.temp_bvftd))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_bvftd.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_bvftd$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_bvftd.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_bvftd.r > ann_bvftd.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_bvftd.r, main= "BvFTD (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_bvftd.p, ann_bvftd.r), xlab = "ANN value")
abline(v=ann_bvftd.p, col= "#009E73", lwd = 4)



# FTLD-motor

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_ftldm <- st_as_sf(temp_ftldm, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_ftldm.sp <- as(spatial.temp_ftldm, "Spatial")  # turn it into a spatial df
class(spatial.temp_ftldm.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_ftldm1 <- as_Spatial(spatial.temp_ftldm)
ppp.spatial.temp_ftldm <- as.ppp(spatial.temp_ftldm1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_ftldm.p <- mean(nndist(ppp.spatial.temp_ftldm))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_ftldm.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_ftldm$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_ftldm.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_ftldm.r > ann_ftldm.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_ftldm.r, main= "FTLD-motor (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ftldm.p, ann_ftldm.r), xlab = "ANN value")
abline(v=ann_ftldm.p, col= "#0072B2", lwd = 4)



# FTLD-NOS

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_ftldnos <- st_as_sf(temp_ftldnos, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_ftldnos.sp <- as(spatial.temp_ftldnos, "Spatial")  # turn it into a spatial df
class(spatial.temp_ftldnos.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_ftldnos1 <- as_Spatial(spatial.temp_ftldnos)
ppp.spatial.temp_ftldnos <- as.ppp(spatial.temp_ftldnos1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_ftldnos.p <- mean(nndist(ppp.spatial.temp_ftldnos))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_ftldnos.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_ftldnos$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_ftldnos.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_ftldnos.r > ann_ftldnos.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_ftldnos.r, main= "FTLD-NOS (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ftldnos.p, ann_ftldnos.r), xlab = "ANN value")
abline(v=ann_ftldnos.p, col= "#56B4E9", lwd = 4)



# PPA

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_ppa <- st_as_sf(temp_ppa, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_ppa.sp <- as(spatial.temp_ppa, "Spatial")  # turn it into a spatial df
class(spatial.temp_ppa.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_ppa1 <- as_Spatial(spatial.temp_ppa)
ppp.spatial.temp_ppa <- as.ppp(spatial.temp_ppa1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_ppa.p <- mean(nndist(ppp.spatial.temp_ppa))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_ppa.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_ppa$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_ppa.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_ppa.r > ann_ppa.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_ppa.r, main= "PPA (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ppa.p, ann_ppa.r), xlab = "ANN value")
abline(v=ann_ppa.p, col= "#CC79A7", lwd = 4)

```

In my meeting with MALR on 15/3/23, he liked the null model finding but suggested the following. Given that the null model result from th ewhol group says that the data are not randomly dispersed, it doesnt make sense to run the group-wise ANN models based on random dispersion in the UMAP space. Instead, what he suggested was:

1)  We first shuffle the Dx (grouped by ID).

2)  From this shuffled dataset, we subset the patient group of interest and run the ANN on them. Repeat this 1000x. This forms the null model as it places the patients in locations that are already established to be occupied (as compared to placing pts in places where there are already no patients).

3)  Then we compare this new null model to each group's derived model from the code above.

4)  We repeat this by time to show convergence or divergence. Using the same data above, we simply subset the patietns by Time and run the models.

```{r}

# First, let us randomly shuffle the Dx for all groups (by ID), subset each Dx, and run the ANN analysis 1000 times to derive the null model.

# I will do this for each Dx group - all time points and each time point


#### AD ######

# For AD (all time points)
# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_AD.r <- vector(length = n)


# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for AD
  temp_AD <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ADs from this shuffled Dx column 
    filter(Dx.x == "AD")
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_AD <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ADs from this shuffled Dx column 
    filter(Dx_shuff == "AD")
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_AD <- st_as_sf(temp_AD, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_AD <- as.ppp(spatial.temp_AD)
  
  
  # for shuffled df
    spatial.temp_shuff_AD <- st_as_sf(temp_shuff_AD, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_AD_t11 <- as_Spatial(spatial.temp_shuff_AD_t1)
  ppp.spatial.temp_shuff_AD <- as.ppp(spatial.temp_shuff_AD)
  
  # run the ANN on the standard DF
  ann_AD.p <- mean(nndist(ppp.spatial.temp_AD))
 
 # run the ANN on the shuffled df 
  ann_shuff_AD.r[i] <- mean(nndist(ppp.spatial.temp_shuff_AD))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_AD.r > ann_AD.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_AD.r, main= "AD (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_AD.p, ann_shuff_AD.r), xlab = "ANN value")
abline(v=ann_AD.p, col="#D55E00", lwd = 4)
# saved as 500*300(h)


# AD T1 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_AD_t1.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for AD
  temp_AD_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ADs from this shuffled Dx column 
    filter(Dx.x == "AD") %>%
    filter(Visit == 1)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_AD_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ADs from this shuffled Dx column 
    filter(Dx_shuff == "AD") %>%
    filter(Visit == 1)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_AD_t1 <- st_as_sf(temp_AD_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_AD_t1 <- as.ppp(spatial.temp_AD_t1)
  
  
  # for shuffled df
    spatial.temp_shuff_AD_t1 <- st_as_sf(temp_shuff_AD_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_AD_t11 <- as_Spatial(spatial.temp_shuff_AD_t1)
  ppp.spatial.temp_shuff_AD_t1 <- as.ppp(spatial.temp_shuff_AD_t1)
  
  # run the ANN on the standard DF
  ann_AD_t1.p <- mean(nndist(ppp.spatial.temp_AD_t1))
 
 # run the ANN on the shuffled df 
  ann_shuff_AD_t1.r[i] <- mean(nndist( ppp.spatial.temp_shuff_AD_t1))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_AD_t1.r > ann_AD_t1.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_AD_t1.r, main= "AD (Visit 1)", las=1, breaks=40, col="grey", xlim=range(ann_AD_t1.p, ann_shuff_AD_t1.r), xlab = "ANN value")
abline(v=ann_AD_t1.p, col="#D55E00", lwd = 4)


# AD T2 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_AD_t2.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t2 data for AD
  temp_AD_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ADs from this shuffled Dx column 
    filter(Dx.x == "AD") %>%
    filter(Visit == 2)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_AD_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ADs from this shuffled Dx column 
    filter(Dx_shuff == "AD") %>%
    filter(Visit == 2)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_AD_t2 <- st_as_sf(temp_AD_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_AD_t2 <- as.ppp(spatial.temp_AD_t2)
  
  
  # for shuffled df
    spatial.temp_shuff_AD_t2 <- st_as_sf(temp_shuff_AD_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_AD_t21 <- as_Spatial(spatial.temp_shuff_AD_t2)
  ppp.spatial.temp_shuff_AD_t2 <- as.ppp(spatial.temp_shuff_AD_t2)
  
  # run the ANN on the standard DF
  ann_AD_t2.p <- mean(nndist(ppp.spatial.temp_AD_t2))
 
 # run the ANN on the shuffled df 
  ann_shuff_AD_t2.r[i] <- mean(nndist( ppp.spatial.temp_shuff_AD_t2))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_AD_t2.r > ann_AD_t2.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_AD_t2.r, main= "AD (Visit 2)", las=1, breaks=40, col="grey", xlim=range(ann_AD_t2.p, ann_shuff_AD_t2.r), xlab = "ANN value")
abline(v=ann_AD_t2.p, col="#D55E00", lwd = 4)


# AD t3 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_AD_t3.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t3 data for AD
  temp_AD_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ADs from this shuffled Dx column 
    filter(Dx.x == "AD") %>%
    filter(Visit == 3)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_AD_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ADs from this shuffled Dx column 
    filter(Dx_shuff == "AD") %>%
    filter(Visit == 3)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_AD_t3 <- st_as_sf(temp_AD_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_AD_t3 <- as.ppp(spatial.temp_AD_t3)
  
  
  # for shuffled df
    spatial.temp_shuff_AD_t3 <- st_as_sf(temp_shuff_AD_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_AD_t31 <- as_Spatial(spatial.temp_shuff_AD_t3)
  ppp.spatial.temp_shuff_AD_t3 <- as.ppp(spatial.temp_shuff_AD_t3)
  
  # run the ANN on the standard DF
  ann_AD_t3.p <- mean(nndist(ppp.spatial.temp_AD_t3))
 
 # run the ANN on the shuffled df 
  ann_shuff_AD_t3.r[i] <- mean(nndist( ppp.spatial.temp_shuff_AD_t3))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_AD_t3.r > ann_AD_t3.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_AD_t3.r, main= "AD (Visit 3)", las=1, breaks=40, col="grey", xlim=range(ann_AD_t3.p, ann_shuff_AD_t3.r), xlab = "ANN value")
abline(v=ann_AD_t3.p, col="#D55E00", lwd = 4)




##### bvFTD #####

# For bvFTD (all time points)
# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_bvFTD.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for bvFTD
  temp_bvFTD <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx.x == "bvFTD")
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_bvFTD <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx_shuff == "bvFTD")
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_bvFTD <- st_as_sf(temp_bvFTD, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_bvFTD <- as.ppp(spatial.temp_bvFTD)
  
  
  # for shuffled df
    spatial.temp_shuff_bvFTD <- st_as_sf(temp_shuff_bvFTD, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_bvFTD_t11 <- as_Spatial(spatial.temp_shuff_bvFTD_t1)
  ppp.spatial.temp_shuff_bvFTD <- as.ppp(spatial.temp_shuff_bvFTD)
  
  # run the ANN on the standard DF
  ann_bvFTD.p <- mean(nndist(ppp.spatial.temp_bvFTD))
 
 # run the ANN on the shuffled df 
  ann_shuff_bvFTD.r[i] <- mean(nndist(ppp.spatial.temp_shuff_bvFTD))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_bvFTD.r > ann_bvFTD.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_bvFTD.r, main= "bvFTD (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_bvFTD.p, ann_shuff_bvFTD.r), xlab = "ANN value")
abline(v=ann_bvFTD.p, col="#009E73", lwd = 4)



# bvFTD T1 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_bvFTD_t1.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for bvFTD
  temp_bvFTD_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx.x == "bvFTD") %>%
    filter(Visit == 1)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_bvFTD_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx_shuff == "bvFTD") %>%
    filter(Visit == 1)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_bvFTD_t1 <- st_as_sf(temp_bvFTD_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_bvFTD_t1 <- as.ppp(spatial.temp_bvFTD_t1)
  
  
  # for shuffled df
    spatial.temp_shuff_bvFTD_t1 <- st_as_sf(temp_shuff_bvFTD_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_bvFTD_t11 <- as_Spatial(spatial.temp_shuff_bvFTD_t1)
  ppp.spatial.temp_shuff_bvFTD_t1 <- as.ppp(spatial.temp_shuff_bvFTD_t1)
  
  # run the ANN on the standard DF
  ann_bvFTD_t1.p <- mean(nndist(ppp.spatial.temp_bvFTD_t1))
 
 # run the ANN on the shuffled df 
  ann_shuff_bvFTD_t1.r[i] <- mean(nndist( ppp.spatial.temp_shuff_bvFTD_t1))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_bvFTD_t1.r > ann_bvFTD_t1.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_bvFTD_t1.r, main= "bvFTD (Visit 1)", las=1, breaks=40, col="grey", xlim=range(ann_bvFTD_t1.p, ann_shuff_bvFTD_t1.r), xlab = "ANN value")
abline(v=ann_bvFTD_t1.p, col="#009E73", lwd = 4)


# bvFTD T2 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_bvFTD_t2.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t2 data for bvFTD
  temp_bvFTD_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx.x == "bvFTD") %>%
    filter(Visit == 2)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_bvFTD_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx_shuff == "bvFTD") %>%
    filter(Visit == 2)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_bvFTD_t2 <- st_as_sf(temp_bvFTD_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_bvFTD_t2 <- as.ppp(spatial.temp_bvFTD_t2)
  
  
  # for shuffled df
    spatial.temp_shuff_bvFTD_t2 <- st_as_sf(temp_shuff_bvFTD_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_bvFTD_t21 <- as_Spatial(spatial.temp_shuff_bvFTD_t2)
  ppp.spatial.temp_shuff_bvFTD_t2 <- as.ppp(spatial.temp_shuff_bvFTD_t2)
  
  # run the ANN on the standard DF
  ann_bvFTD_t2.p <- mean(nndist(ppp.spatial.temp_bvFTD_t2))
 
 # run the ANN on the shuffled df 
  ann_shuff_bvFTD_t2.r[i] <- mean(nndist( ppp.spatial.temp_shuff_bvFTD_t2))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_bvFTD_t2.r > ann_bvFTD_t2.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_bvFTD_t2.r, main= "bvFTD (Visit 2)", las=1, breaks=40, col="grey", xlim=range(ann_bvFTD_t2.p, ann_shuff_bvFTD_t2.r), xlab = "ANN value")
abline(v=ann_bvFTD_t2.p, col="#009E73", lwd = 4)


# bvFTD t3 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_bvFTD_t3.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t3 data for bvFTD
  temp_bvFTD_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx.x == "bvFTD") %>%
    filter(Visit == 3)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_bvFTD_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the bvFTDs from this shuffled Dx column 
    filter(Dx_shuff == "bvFTD") %>%
    filter(Visit == 3)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_bvFTD_t3 <- st_as_sf(temp_bvFTD_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_bvFTD_t3 <- as.ppp(spatial.temp_bvFTD_t3)
  
  
  # for shuffled df
    spatial.temp_shuff_bvFTD_t3 <- st_as_sf(temp_shuff_bvFTD_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_bvFTD_t31 <- as_Spatial(spatial.temp_shuff_bvFTD_t3)
  ppp.spatial.temp_shuff_bvFTD_t3 <- as.ppp(spatial.temp_shuff_bvFTD_t3)
  
  # run the ANN on the standard DF
  ann_bvFTD_t3.p <- mean(nndist(ppp.spatial.temp_bvFTD_t3))
 
 # run the ANN on the shuffled df 
  ann_shuff_bvFTD_t3.r[i] <- mean(nndist( ppp.spatial.temp_shuff_bvFTD_t3))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_bvFTD_t3.r > ann_bvFTD_t3.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_bvFTD_t3.r, main= "bvFTD (Visit 3)", las=1, breaks=40, col="grey", xlim=range(ann_bvFTD_t3.p, ann_shuff_bvFTD_t3.r), xlab = "ANN value")
abline(v=ann_bvFTD_t3.p, col="#009E73", lwd = 4)




##### FTLD motor #####

# all time points
# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldm.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ftldm
  temp_ftldm <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "FTLD-motor")
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldm <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldms from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-motor")
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldm <- st_as_sf(temp_ftldm, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldm <- as.ppp(spatial.temp_ftldm)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldm <- st_as_sf(temp_shuff_ftldm, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldm_t11 <- as_Spatial(spatial.temp_shuff_ftldm_t1)
  ppp.spatial.temp_shuff_ftldm <- as.ppp(spatial.temp_shuff_ftldm)
  
  # run the ANN on the standard DF
  ann_ftldm.p <- mean(nndist(ppp.spatial.temp_ftldm))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldm.r[i] <- mean(nndist(ppp.spatial.temp_shuff_ftldm))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldm.r > ann_ftldm.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldm.r, main= "FTLD-motor (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ftldm.p, ann_shuff_ftldm.r), xlab = "ANN value")
abline(v=ann_ftldm.p, col= "#0072B2", lwd = 4)



# ftldm T1 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldm_t1.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ftldm
  temp_ftldm_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "FTLD-motor") %>%
    filter(Visit == 1)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldm_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldms from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-motor") %>%
    filter(Visit == 1)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldm_t1 <- st_as_sf(temp_ftldm_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldm_t1 <- as.ppp(spatial.temp_ftldm_t1)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldm_t1 <- st_as_sf(temp_shuff_ftldm_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldm_t11 <- as_Spatial(spatial.temp_shuff_ftldm_t1)
  ppp.spatial.temp_shuff_ftldm_t1 <- as.ppp(spatial.temp_shuff_ftldm_t1)
  
  # run the ANN on the standard DF
  ann_ftldm_t1.p <- mean(nndist(ppp.spatial.temp_ftldm_t1))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldm_t1.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldm_t1))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldm_t1.r > ann_ftldm_t1.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldm_t1.r, main= "FTLD-motor (Visit 1)", las=1, breaks=40, col="grey", xlim=range(ann_ftldm_t1.p, ann_shuff_ftldm_t1.r), xlab = "ANN value")
abline(v=ann_ftldm_t1.p, col= "#0072B2", lwd = 4)


# ftldm T2 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldm_t2.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t2 data for ftldm
  temp_ftldm_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "FTLD-motor") %>%
    filter(Visit == 2)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldm_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldms from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-motor") %>%
    filter(Visit == 2)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldm_t2 <- st_as_sf(temp_ftldm_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldm_t2 <- as.ppp(spatial.temp_ftldm_t2)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldm_t2 <- st_as_sf(temp_shuff_ftldm_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldm_t21 <- as_Spatial(spatial.temp_shuff_ftldm_t2)
  ppp.spatial.temp_shuff_ftldm_t2 <- as.ppp(spatial.temp_shuff_ftldm_t2)
  
  # run the ANN on the standard DF
  ann_ftldm_t2.p <- mean(nndist(ppp.spatial.temp_ftldm_t2))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldm_t2.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldm_t2))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldm_t2.r > ann_ftldm_t2.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldm_t2.r, main= "FTLD-motor (Visit 2)", las=1, breaks=40, col="grey", xlim=range(ann_ftldm_t2.p, ann_shuff_ftldm_t2.r), xlab = "ANN value")
abline(v=ann_ftldm_t2.p, col= "#0072B2", lwd = 4)


# ftldm t3 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldm_t3.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t3 data for ftldm
  temp_ftldm_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "FTLD-motor") %>%
    filter(Visit == 3)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldm_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldms from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-motor") %>%
    filter(Visit == 3)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldm_t3 <- st_as_sf(temp_ftldm_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldm_t3 <- as.ppp(spatial.temp_ftldm_t3)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldm_t3 <- st_as_sf(temp_shuff_ftldm_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldm_t31 <- as_Spatial(spatial.temp_shuff_ftldm_t3)
  ppp.spatial.temp_shuff_ftldm_t3 <- as.ppp(spatial.temp_shuff_ftldm_t3)
  
  # run the ANN on the standard DF
  ann_ftldm_t3.p <- mean(nndist(ppp.spatial.temp_ftldm_t3))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldm_t3.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldm_t3))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldm_t3.r > ann_ftldm_t3.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldm_t3.r, main= "FTLD-motor (Visit 3)", las=1, breaks=40, col="grey", xlim=range(ann_ftldm_t3.p, ann_shuff_ftldm_t3.r), xlab = "ANN value")
abline(v=ann_ftldm_t3.p, col= "#0072B2", lwd = 4)



#### FTLD NOS #####

# all time points
# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldnos.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ftldm
  temp_ftldnos <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "FTLD-NOS")
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldnos <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-NOS")
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldnos <- st_as_sf(temp_ftldnos, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldnos <- as.ppp(spatial.temp_ftldnos)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldnos <- st_as_sf(temp_shuff_ftldnos, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldnos_t11 <- as_Spatial(spatial.temp_shuff_ftldnos_t1)
  ppp.spatial.temp_shuff_ftldnos <- as.ppp(spatial.temp_shuff_ftldnos)
  
  # run the ANN on the standard DF
  ann_ftldnos.p <- mean(nndist(ppp.spatial.temp_ftldnos))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldnos.r[i] <- mean(nndist(ppp.spatial.temp_shuff_ftldnos))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldnos.r > ann_ftldnos.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldnos.r, main= "FTLD-NOS (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ftldnos.p, ann_shuff_ftldnos.r), xlab = "ANN value")
abline(v=ann_ftldnos.p, col= "#56B4E9", lwd = 4)



# ftldnos T1 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldnos_t1.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ftldnos
  temp_ftldnos_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx.x == "FTLD-NOS") %>%
    filter(Visit == 1)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldnos_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-NOS") %>%
    filter(Visit == 1)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldnos_t1 <- st_as_sf(temp_ftldnos_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldnos_t1 <- as.ppp(spatial.temp_ftldnos_t1)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldnos_t1 <- st_as_sf(temp_shuff_ftldnos_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldnos_t11 <- as_Spatial(spatial.temp_shuff_ftldnos_t1)
  ppp.spatial.temp_shuff_ftldnos_t1 <- as.ppp(spatial.temp_shuff_ftldnos_t1)
  
  # run the ANN on the standard DF
  ann_ftldnos_t1.p <- mean(nndist(ppp.spatial.temp_ftldnos_t1))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldnos_t1.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldnos_t1))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldnos_t1.r > ann_ftldnos_t1.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldnos_t1.r, main= "FTLD-NOS (Visit 1)", las=1, breaks=40, col="grey", xlim=range(ann_ftldnos_t1.p, ann_shuff_ftldnos_t1.r), xlab = "ANN value")
abline(v=ann_ftldnos_t1.p, col= "#56B4E9", lwd = 4)


# ftldnos T2 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldnos_t2.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t2 data for ftldnos
  temp_ftldnos_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx.x == "FTLD-NOS") %>%
    filter(Visit == 2)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldnos_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-NOS") %>%
    filter(Visit == 2)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldnos_t2 <- st_as_sf(temp_ftldnos_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldnos_t2 <- as.ppp(spatial.temp_ftldnos_t2)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldnos_t2 <- st_as_sf(temp_shuff_ftldnos_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldnos_t21 <- as_Spatial(spatial.temp_shuff_ftldnos_t2)
  ppp.spatial.temp_shuff_ftldnos_t2 <- as.ppp(spatial.temp_shuff_ftldnos_t2)
  
  # run the ANN on the standard DF
  ann_ftldnos_t2.p <- mean(nndist(ppp.spatial.temp_ftldnos_t2))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldnos_t2.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldnos_t2))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldnos_t2.r > ann_ftldnos_t2.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldnos_t2.r, main= "FTLD-NOS (Visit 2)", las=1, breaks=40, col="grey", xlim=range(ann_ftldnos_t2.p, ann_shuff_ftldnos_t2.r), xlab = "ANN value")
abline(v=ann_ftldnos_t2.p, col= "#56B4E9", lwd = 4)


# ftldnos t3 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ftldnos_t3.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t3 data for ftldnos
  temp_ftldnos_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx.x == "FTLD-NOS") %>%
    filter(Visit == 3)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ftldnos_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ftldnoss from this shuffled Dx column 
    filter(Dx_shuff == "FTLD-NOS") %>%
    filter(Visit == 3)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ftldnos_t3 <- st_as_sf(temp_ftldnos_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ftldnos_t3 <- as.ppp(spatial.temp_ftldnos_t3)
  
  
  # for shuffled df
    spatial.temp_shuff_ftldnos_t3 <- st_as_sf(temp_shuff_ftldnos_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ftldnos_t31 <- as_Spatial(spatial.temp_shuff_ftldnos_t3)
  ppp.spatial.temp_shuff_ftldnos_t3 <- as.ppp(spatial.temp_shuff_ftldnos_t3)
  
  # run the ANN on the standard DF
  ann_ftldnos_t3.p <- mean(nndist(ppp.spatial.temp_ftldnos_t3))
 
 # run the ANN on the shuffled df 
  ann_shuff_ftldnos_t3.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ftldnos_t3))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ftldnos_t3.r > ann_ftldnos_t3.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ftldnos_t3.r, main= "FTLD-NOS (Visit 3)", las=1, breaks=40, col="grey", xlim=range(ann_ftldnos_t3.p, ann_shuff_ftldnos_t3.r), xlab = "ANN value")
abline(v=ann_ftldnos_t3.p, col= "#56B4E9", lwd = 4)





##### PPA #######

# all time points
# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ppa.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ftldm
  temp_ppa <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ftldms from this shuffled Dx column 
    filter(Dx.x == "PPA")
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ppa <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ppas from this shuffled Dx column 
    filter(Dx_shuff == "PPA")
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ppa <- st_as_sf(temp_ppa, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ppa <- as.ppp(spatial.temp_ppa)
  
  
  # for shuffled df
    spatial.temp_shuff_ppa <- st_as_sf(temp_shuff_ppa, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ppa_t11 <- as_Spatial(spatial.temp_shuff_ppa_t1)
  ppp.spatial.temp_shuff_ppa <- as.ppp(spatial.temp_shuff_ppa)
  
  # run the ANN on the standard DF
  ann_ppa.p <- mean(nndist(ppp.spatial.temp_ppa))
 
 # run the ANN on the shuffled df 
  ann_shuff_ppa.r[i] <- mean(nndist(ppp.spatial.temp_shuff_ppa))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ppa.r > ann_ppa.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ppa.r, main= "PPA (all time points)", las=1, breaks=40, col="grey", xlim=range(ann_ppa.p, ann_shuff_ppa.r), xlab = "ANN value")
abline(v=ann_ppa.p, col= "#CC79A7", lwd = 4)



# ppa T1 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ppa_t1.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just T1 data for ppa
  temp_ppa_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ppas from this shuffled Dx column 
    filter(Dx.x == "PPA") %>%
    filter(Visit == 1)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ppa_t1 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ppas from this shuffled Dx column 
    filter(Dx_shuff == "PPA") %>%
    filter(Visit == 1)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ppa_t1 <- st_as_sf(temp_ppa_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ppa_t1 <- as.ppp(spatial.temp_ppa_t1)
  
  
  # for shuffled df
    spatial.temp_shuff_ppa_t1 <- st_as_sf(temp_shuff_ppa_t1, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ppa_t11 <- as_Spatial(spatial.temp_shuff_ppa_t1)
  ppp.spatial.temp_shuff_ppa_t1 <- as.ppp(spatial.temp_shuff_ppa_t1)
  
  # run the ANN on the standard DF
  ann_ppa_t1.p <- mean(nndist(ppp.spatial.temp_ppa_t1))
 
 # run the ANN on the shuffled df 
  ann_shuff_ppa_t1.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ppa_t1))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ppa_t1.r > ann_ppa_t1.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ppa_t1.r, main= "PPA (Visit 1)", las=1, breaks=40, col="grey", xlim=range(ann_ppa_t1.p, ann_shuff_ppa_t1.r), xlab = "ANN value")
abline(v=ann_ppa_t1.p, col= "#CC79A7", lwd = 4)


# ppa T2 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ppa_t2.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t2 data for ppa
  temp_ppa_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ppas from this shuffled Dx column 
    filter(Dx.x == "PPA") %>%
    filter(Visit == 2)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ppa_t2 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ppas from this shuffled Dx column 
    filter(Dx_shuff == "PPA") %>%
    filter(Visit == 2)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ppa_t2 <- st_as_sf(temp_ppa_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ppa_t2 <- as.ppp(spatial.temp_ppa_t2)
  
  
  # for shuffled df
    spatial.temp_shuff_ppa_t2 <- st_as_sf(temp_shuff_ppa_t2, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ppa_t21 <- as_Spatial(spatial.temp_shuff_ppa_t2)
  ppp.spatial.temp_shuff_ppa_t2 <- as.ppp(spatial.temp_shuff_ppa_t2)
  
  # run the ANN on the standard DF
  ann_ppa_t2.p <- mean(nndist(ppp.spatial.temp_ppa_t2))
 
 # run the ANN on the shuffled df 
  ann_shuff_ppa_t2.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ppa_t2))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ppa_t2.r > ann_ppa_t2.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ppa_t2.r, main= "PPA (Visit 2)", las=1, breaks=40, col="grey", xlim=range(ann_ppa_t2.p, ann_shuff_ppa_t2.r), xlab = "ANN value")
abline(v=ann_ppa_t2.p, col= "#CC79A7", lwd = 4)


# ppa t3 only

# set the simulation parameter
n <- 1000

# create an empty object space to store the ANN values
ann_shuff_ppa_t3.r <- vector(length = n)

# create a for loop to do the following functions
for (i in 1:n){
  # create a standard DF with just t3 data for ppa
  temp_ppa_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID) %>% 
    # subset the ppas from this shuffled Dx column 
    filter(Dx.x == "PPA") %>%
    filter(Visit == 3)
  
  # create a new DF where I shuffle the Dx while grouped for ID
  temp_shuff_ppa_t3 <- temp %>% 
    # group by ID and Dx
    group_by(ID, Dx.x) %>% 
    # create a new var where we shuffle the Dx up
    transform(Dx_shuff = sample(Dx.x)) %>%
    # subset the ppas from this shuffled Dx column 
    filter(Dx_shuff == "PPA") %>%
    filter(Visit == 3)
  
  # now turn these coords into a df
  # for standard df
  spatial.temp_ppa_t3 <- st_as_sf(temp_ppa_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
  ppp.spatial.temp_ppa_t3 <- as.ppp(spatial.temp_ppa_t3)
  
  
  # for shuffled df
    spatial.temp_shuff_ppa_t3 <- st_as_sf(temp_shuff_ppa_t3, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

  # now I convert this into a "ppp" object (there are two steps to do this- see above).
#  spatial.temp_shuff_ppa_t31 <- as_Spatial(spatial.temp_shuff_ppa_t3)
  ppp.spatial.temp_shuff_ppa_t3 <- as.ppp(spatial.temp_shuff_ppa_t3)
  
  # run the ANN on the standard DF
  ann_ppa_t3.p <- mean(nndist(ppp.spatial.temp_ppa_t3))
 
 # run the ANN on the shuffled df 
  ann_shuff_ppa_t3.r[i] <- mean(nndist( ppp.spatial.temp_shuff_ppa_t3))  # Tally the ANN values of the ppp object
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_shuff_ppa_t3.r > ann_ppa_t3.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_shuff_ppa_t3.r, main= "PPA (Visit 3)", las=1, breaks=40, col="grey", xlim=range(ann_ppa_t3.p, ann_shuff_ppa_t3.r), xlab = "ANN value")
abline(v=ann_ppa_t3.p, col= "#CC79A7", lwd = 4)


```

#### KDE tests of differences in kernel density distributions

While the analysis above tells whether the data are distributed randomly or not, we also want to know whether the locations of the points in each group are significantly different from the other groups. We can do this using KDE tests from the ks package. This is done for all time points combined

```{r}

# create a list of coordinates only for each group from the df above
temp_ad_coords <- temp_ad %>% ungroup() %>% dplyr::select(x,y)
temp_bvftd_coords <- temp_bvftd %>% ungroup() %>% dplyr::select(x,y)
temp_ftldm_coords <- temp_ftldm %>% ungroup() %>% dplyr::select(x,y)
temp_ftldnos_coords <- temp_ftldnos %>% ungroup() %>% dplyr::select(x,y)
temp_ppa_coords <- temp_ppa %>% ungroup() %>% dplyr::select(x,y)


# pairwise do the KDE tests and extract the z and p values
# AD vs others
kde.test(temp_ad_coords, temp_bvftd_coords)
kde.test(temp_ad_coords, temp_ftldm_coords)
kde.test(temp_ad_coords, temp_ftldnos_coords)
kde.test(temp_ad_coords, temp_ppa_coords)

# bvFTD vs others
kde.test(temp_bvftd_coords, temp_ftldm_coords)
kde.test(temp_bvftd_coords, temp_ftldnos_coords)
kde.test(temp_bvftd_coords, temp_ppa_coords)

# ftld m vs others
kde.test(temp_ftldm_coords, temp_ftldnos_coords)
kde.test(temp_ftldm_coords, temp_ppa_coords)

# ftld NOS vs PPA
kde.test(temp_ftldnos_coords, temp_ppa_coords)



# repeat for each time point
# create three different dfs for each time point for each group
# AD
temp_ad_coords_t1 <- temp_ad %>% group_by(ID, Dx.x) %>% filter(Visit == 1) %>% ungroup() %>% dplyr::select(x,y)
temp_ad_coords_t2 <- temp_ad %>% group_by(ID, Dx.x) %>% filter(Visit == 2) %>% ungroup() %>% dplyr::select(x,y)
temp_ad_coords_t3 <- temp_ad %>% group_by(ID, Dx.x) %>% filter(Visit == 3) %>% ungroup() %>% dplyr::select(x,y)

# bvFTD
temp_bvftd_coords_t1 <- temp_bvftd %>% group_by(ID, Dx.x) %>% filter(Visit == 1) %>% ungroup() %>% dplyr::select(x,y)
temp_bvftd_coords_t2 <- temp_bvftd %>% group_by(ID, Dx.x) %>% filter(Visit == 2) %>% ungroup() %>% dplyr::select(x,y)
temp_bvftd_coords_t3 <- temp_bvftd %>% group_by(ID, Dx.x) %>% filter(Visit == 3) %>% ungroup() %>% dplyr::select(x,y)

# FTLD m
temp_ftldm_coords_t1 <- temp_ftldm %>% group_by(ID, Dx.x) %>% filter(Visit == 1) %>% ungroup() %>% dplyr::select(x,y)
temp_ftldm_coords_t2 <- temp_ftldm %>% group_by(ID, Dx.x) %>% filter(Visit == 2) %>% ungroup() %>% dplyr::select(x,y)
temp_ftldm_coords_t3 <- temp_ftldm %>% group_by(ID, Dx.x) %>% filter(Visit == 3) %>% ungroup() %>% dplyr::select(x,y)

# FTLD NOS
temp_ftldnos_coords_t1 <- temp_ftldnos %>% group_by(ID, Dx.x) %>% filter(Visit == 1) %>% ungroup() %>% dplyr::select(x,y)
temp_ftldnos_coords_t2 <- temp_ftldnos %>% group_by(ID, Dx.x) %>% filter(Visit == 2) %>% ungroup() %>% dplyr::select(x,y)
temp_ftldnos_coords_t3 <- temp_ftldnos %>% group_by(ID, Dx.x) %>% filter(Visit == 3) %>% ungroup() %>% dplyr::select(x,y)

# PPA
temp_ppa_coords_t1 <- temp_ppa %>% group_by(ID, Dx.x) %>% filter(Visit == 1) %>% ungroup() %>% dplyr::select(x,y)
temp_ppa_coords_t2 <- temp_ppa %>% group_by(ID, Dx.x) %>% filter(Visit == 2) %>% ungroup() %>% dplyr::select(x,y)
temp_ppa_coords_t3 <- temp_ppa %>% group_by(ID, Dx.x) %>% filter(Visit == 3) %>% ungroup() %>% dplyr::select(x,y)


# pairwise do the KDE tests and extract the z and p values
# AD
kde.test(temp_ad_coords_t1, temp_ad_coords_t2)
kde.test(temp_ad_coords_t2, temp_ad_coords_t3)
kde.test(temp_ad_coords_t1, temp_ad_coords_t3)

# bvFTD
kde.test(temp_bvftd_coords_t1, temp_bvftd_coords_t2)
kde.test(temp_bvftd_coords_t2, temp_bvftd_coords_t3)
kde.test(temp_bvftd_coords_t1, temp_bvftd_coords_t3)

# FTLDm
kde.test(temp_ftldm_coords_t1, temp_ftldm_coords_t2)
kde.test(temp_ftldm_coords_t2, temp_ftldm_coords_t3)
kde.test(temp_ftldm_coords_t1, temp_ftldm_coords_t3)

# FTLD NOS
kde.test(temp_ftldnos_coords_t1, temp_ftldnos_coords_t2)
kde.test(temp_ftldnos_coords_t2, temp_ftldnos_coords_t3)
kde.test(temp_ftldnos_coords_t1, temp_ftldnos_coords_t3)

# PPA
kde.test(temp_ppa_coords_t1, temp_ppa_coords_t2)
kde.test(temp_ppa_coords_t2, temp_ppa_coords_t3)
kde.test(temp_ppa_coords_t1, temp_ppa_coords_t3)

```

#### Trajectory analysis within UMAP space and plot PPA subgroups

I'm trying out some analysis where we can see how and where groups of patients move. Inspired by ecological trajectory analysis to map movement of animals in spaces. See this link: <https://cran.r-project.org/web/packages/ecotraj/vignettes/IntroductionETA.html>.

The problem with this analysis is that the trajectories are all over the place so its very difficult to find a nice way to plot this. Instead, what I'll do is map the centroids of each Time visit for each group and then connect those centroids to show the average movement for the group.

```{r}

# first lets create a temporary data frame to play with

umap_df_rank_withMeta_1 %>%
   # group by Dx and VIist
  group_by(Visit, Dx.x) %>%
   # let us create some new variables
   # centroids for V1 axis and V2 axis by group
  summarise(Visit_V1_mean = mean(V1),
            Visit_V2_mean = mean(V2),
            # SD for these axes
            Visit_V1_sd = sd(V1),
            Visit_V2_sd = sd(V2),
            n = n(),
            # standard errors for these axes
            Visit_V1_se = Visit_V1_sd/sqrt(n),
            Visit_V2_se = Visit_V2_sd/sqrt(n),
            ) %>%
   # now plot this
  ggplot(aes(x = Visit_V1_mean,
             y = Visit_V2_mean,
             colour = Dx.x,
             label = Visit)) +
   geom_path() +
  geom_point(aes(shape = Visit), size = 4) +
  scale_colour_manual(values = group.colours) +
  # create error bars on either sides of the points
  geom_errorbar(aes(ymin = Visit_V2_mean - Visit_V2_se, 
                    ymax = Visit_V2_mean + Visit_V2_se), width = 0.1) +
  geom_errorbarh(aes(xmin = Visit_V1_mean - Visit_V1_se, 
                    xmax = Visit_V1_mean + Visit_V1_se), height = 0.1) +
  xlim(-6.117606,  5.510033) +
  ylim(-6.341354 , 4.440421) +
  xlab("V1") +
  ylab("V2") +
  labs(colour = "Group") +
  #move legend inside and change themes
  theme_bw() +
  theme(legend.direction = "vertical",
        legend.box = "horizontal",
        legend.position = c(0.25,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
  

# Plot only PPAs in the UMAP space as KDEs
umap_df_rank_withMeta_1 %>%
   # group by Dx and VIist
  group_by(NACCID) %>%
  # filter only PPAs
  filter(Dx.x == "PPA") %>%
  # rename PPADementia to Mixed-PPA
  mutate(PPA = case_when(Group == "PPADementia" ~ "Mixed PPA",
                         Group == "PPANOS" ~ "PPA-NOS",
                         TRUE ~ Group)) %>%
  # plot into UMAP
   ggplot(aes(x = V1, 
             y = V2)) +
  stat_density2d_filled(contour_var = "ndensity") + # contour_var = "ndensity" displays the density estimate scaled to a max of 1
  # say I chuck in an ellipse for each time point 
   facet_grid("Visit" + Visit ~ PPA) + # the "Visit" adds the text Visit before Visit number +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  # shift legend into the FTLD motor panel
  theme(#legend.position = c(0.93,0.77),
        legend.background = element_rect(colour = "white"),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9)) +
  guides(fill = guide_legend(title = "Scaled density")) +
  ggtitle("PPA stratified by visit") +
  theme(plot.title = element_text(hjust = 0.5))
# sved as 1200*800 

```

#### Exploring diagnosis change within multidimensional space

Now, in the next step, we can do two specific highlights on this UMAP plot. First, for the specific PPA and FTLD-m types and the second for those whose diagnosis changes. Lets make these additions to the UMAP_df and plot them too. First, I use the nacc_voi_dem_all_11 dataset to create a subset of people whose diagnosis changes across time points. I then left_join this with the UMAP dataset.

```{r}

# first, find all rows where patient has the same Dx for all time points
nacc_voi_dem_all_11_samedx <- nacc_voi_dem_all_11 %>%
  # select only ID, visit nad Dx
  dplyr::select(1:3) %>%
  group_by(NACCID, Group) %>%
  filter(n() == 3)

# anti_join this df from the nacc_voi_dem_all_11 df to find those IDs that are left out = these will be the ones whose dx changes with time
nacc_voi_dem_all_11_dxchange <- nacc_voi_dem_all_11 %>% anti_join(nacc_voi_dem_all_11_samedx) %>%
  dplyr::select(1:3) %>%
  rename("ID" = "NACCID", "Visit" = "NACCVNUM") %>%
  factor()
# perfect

# keep a log of only the unique IDs
nacc_voi_dem_all_11_dxchange_ID <- nacc_voi_dem_all_11_dxchange %>% 
  filter(Visit == 1) %>% 
  dplyr::select(ID)

# now join this info in with the UMAP dataset
umap_df_dxchange <- umap_df %>% 
  left_join(nacc_voi_dem_all_11_dxchange, by = c("ID", "Visit"))

# I now have to create some filtered columns to tell ggplot where to find people who have Dx of PPA at T1, or bvFTD at T1 etc.
# Lets create these conditions

umap_df_dxchange_subgroups <- umap_df_dxchange %>%
  # create new variables for each group
  mutate(t1_PPA = case_when(Visit == 1 & str_detect(Group, "PPA") ~ 1, 
                            TRUE ~ NA_real_),
         t1_AD = case_when(Visit == 1 & Group == "AD" ~ 1, 
                            TRUE ~ NA_real_),
         t1_bvFTD = case_when(Visit == 1 & Group == "bvFTD" ~ 1, 
                            TRUE ~ NA_real_),
         t1_FTLDm = case_when(Visit == 1 & Group == "CBD" ~ 1, 
                            TRUE ~ NA_real_),
         t1_FTLDNOS = case_when(Visit == 1 & Group == "FTLD-NOS" ~ 1, 
                               TRUE ~ NA_real_)
) %>%
  # fill the group membership to all instances of the ID
  group_by(ID) %>%
  fill(c(t1_PPA:t1_FTLDNOS), .direction = "down") %>%
  ungroup()

# convert visit to factor
umap_df_dxchange_subgroups$Visit <- as.factor(umap_df_dxchange_subgroups$Visit)


# now make these plots for each Dx subgroup with labels for each time point Dx. 

# For AD
umap_dxchange_p1 <- umap_df_dxchange_subgroups %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Group)) +
  # shape all points by visit
  geom_point(aes(shape = Visit), size = 2, alpha = 0.5) +
  geom_line(aes(group=ID), alpha = 0.5) + # connect points
  # highlight only the dx group of interest
  gghighlight(t1_AD == 1,
              use_direct_label = FALSE) +
  # shape these points and connect these shapes
  geom_point(aes(shape = Visit), size = 4) +
  geom_line(aes(group=ID), size = 0.5) +
  # highlight the Dx only at time point 3
  geom_label_repel(data = . %>%
                     mutate(label1 = ifelse(
                       Visit %in% c(3), Group, ""
                     )),
        aes(label = label1),
                  box.padding = 0.5,
                  fill = "white",
                  segment.color = 'black',
                  colour = "black",
                  show.legend = F) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("AD as first diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))+
  # remove one legend from double legend
  guides(color = "none")



# for bvFTD
umap_dxchange_p2 <- umap_df_dxchange_subgroups %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Group)) +
  # shape all points by visit
  geom_point(aes(shape = Visit), size = 2, alpha = 0.5) +
  geom_line(aes(group=ID), alpha = 0.5) + # connect points
  # highlight only the dx group of interest
  gghighlight(t1_bvFTD == 1,
              use_direct_label = FALSE) +
  # shape these points and connect these shapes
  geom_point(aes(shape = Visit), size = 4) +
  geom_line(aes(group=ID), size = 0.5) +
  # highlight the Dx only at time point 3
  geom_label_repel(data = . %>%
                     mutate(label1 = ifelse(
                       Visit %in% c(3), Group, ""
                     )),
        aes(label = label1),
                  box.padding = 0.5,
                  fill = "white",
                  segment.color = 'black',
                  colour = "black",
                  show.legend = F, 
                  max.overlaps = Inf) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("bvFTD as first diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))+
  # remove one legend from double legend
  guides(color = "none")


# for FTLDm
umap_dxchange_p3 <- umap_df_dxchange_subgroups %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Group)) +
  # shape all points by visit
  geom_point(aes(shape = Visit), size = 2, alpha = 0.5) +
  geom_line(aes(group=ID), alpha = 0.5) + # connect points
  # highlight only the dx group of interest
  gghighlight(t1_FTLDm == 1,
              use_direct_label = FALSE) +
  # shape these points and connect these shapes
  geom_point(aes(shape = Visit), size = 4) +
  geom_line(aes(group=ID), size = 0.5) +
  # highlight the Dx only at time point 3
  geom_label_repel(data = . %>%
                     mutate(label1 = ifelse(
                       Visit %in% c(3), Group, ""
                     )),
        aes(label = label1),
                  box.padding = 0.5,
                  fill = "white",
                  segment.color = 'black',
                  colour = "black",
                  show.legend = F, 
                  max.overlaps = Inf) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-motor as first diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))+
  # remove one legend from double legend
  guides(color = "none")


# For FTLD-NOS
umap_dxchange_p4 <- umap_df_dxchange_subgroups %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Group)) +
  # shape all points by visit
  geom_point(aes(shape = Visit), size = 2, alpha = 0.5) +
  geom_line(aes(group=ID), alpha = 0.5) + # connect points
  # highlight only the dx group of interest
  gghighlight(t1_FTLDNOS == 1,
              use_direct_label = FALSE) +
  # shape these points and connect these shapes
  geom_point(aes(shape = Visit), size = 4) +
  geom_line(aes(group=ID), size = 0.5) +
  # highlight the Dx only at time point 3
  geom_label_repel(data = . %>%
                     mutate(label1 = ifelse(
                       Visit %in% c(3), Group, ""
                     )),
    
    aes(label = label1),
                  box.padding = 0.5,
                  fill = "white",
                  segment.color = 'black',
                  colour = "black",
                  show.legend = F, 
                  max.overlaps = Inf) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("FTLD-NOS as first diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))+
  # remove one legend from double legend
  guides(color = "none")


# For PPA
umap_dxchange_p5 <- umap_df_dxchange_subgroups %>%
  ggplot(aes(x = V1, 
             y = V2,
             color = Group)) +
  # shape all points by visit
  geom_point(aes(shape = Visit), size = 2, alpha = 0.5) +
  geom_line(aes(group=ID), alpha = 0.5) + # connect points
  # highlight only the dx group of interest
  gghighlight(t1_PPA == 1,
              use_direct_label = FALSE) +
  # shape these points and connect these shapes
  geom_point(aes(shape = Visit), size = 4) +
  geom_line(aes(group=ID), size = 0.5) +
  # highlight the Dx only at time point 3
  geom_label_repel(data = . %>%
                     mutate(label1 = ifelse(
                       Visit %in% c(3), Group, ""
                     )),
    
    aes(label = label1),
                  box.padding = 0.5,
                  fill = "white",
                  segment.color = 'black',
                  colour = "black",
                  show.legend = F, 
                  max.overlaps = Inf) +
  scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PPA as first diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))+
  # remove one legend from double legend
  guides(color = "none")

# multiplot
cowplot::plot_grid(umap_dxchange_p1,
                   umap_dxchange_p2,
                   umap_dxchange_p3,
                   umap_dxchange_p4,
                   umap_dxchange_p5,
                   labels = "AUTO",
                   ncol = 3)
# saved as 2500*1200 dim

# each plot (per group) saved as 1k*1k
```

#### What PCs do the UMAP dimensions reflect?

Since the UMAP is a nonlinear dimension compression ML algorithm, one of the questions is what PCs do the UMAP dimensions 1 and 2 reflect? There are few ways of understanding this. First, I can do a 2component PCA on the 6 PCs and see what emerges. I don't think this will work as I don't know if the final 2PCs should be orthogonal (on UMAP I don't think orthogonal dimensions are derived). Another way to decode this is to do non-linear regression of the UMAP dimensions back into the PC space and see what dimensions that UMAP explains. The problem with this approach is that I don't know what exact nonlinear technique the UMAP uses so its hard to understand what type of regression to use to predict the PC values. Instead, Dan suggested an alternate. I simply rank the PC values for each PC and then plot the ranking on top of the patient scores. Lets try this out.

```{r}

# lets create a new datasetout of the fit_pca_noLL_scores_df_ID dataset 

rank_fit_pca_noLL_scores_df_ID <- fit_pca_noLL_scores_df_ID

# rank using the - sign as it then assigns the first rank to the larges positive value and the last rank to the smallest negative value

rank_fit_pca_noLL_scores_df_ID$RC1rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC1)
rank_fit_pca_noLL_scores_df_ID$RC2rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC2)
rank_fit_pca_noLL_scores_df_ID$RC3rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC3)
rank_fit_pca_noLL_scores_df_ID$RC4rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC4)
rank_fit_pca_noLL_scores_df_ID$RC5rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC5)
rank_fit_pca_noLL_scores_df_ID$RC6rank <- rank(-rank_fit_pca_noLL_scores_df_ID$RC6)

# left join these values to the UMAP df. I will create a new UMAP df so as to not contaminate the original dataset

umap_df_rank <- umap_df %>%
  # left join rank dataset
  left_join(rank_fit_pca_noLL_scores_df_ID, 
            by = c("ID", "Visit", "Dx"))

# plot the dimensions with circle size weighed for rank of PC1 - I will remove any colours for the dots

# PC1 rank
umap_rank_pc1 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC1rank,
                 colour = RC1rank), alpha = 0.3) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")+
  ggtitle("PC1: Func. stat.")

# PC2 rank
umap_rank_pc2 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC2rank,
                 colour = RC2rank), alpha = 0.3) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
 # labs(size='PC2 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none") +
   guides(size = "none") +
  ggtitle("PC2: Apat./impuls.")

# PC3 rank
umap_rank_pc3 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC3rank, 
                 colour = RC3rank), alpha = 0.3) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
 # labs(size='PC3 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")+
   guides(size = "none") +
  ggtitle("PC3: Motor")


# PC4 rank
umap_rank_pc4 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC4rank, 
                 colour = RC4rank), alpha = 0.3) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
 # labs(size='PC4 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")+
   guides(size = "none") +
  ggtitle("PC4: Psychosis")


# PC5 rank
umap_rank_pc5 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC5rank, 
                 colour = RC5rank), alpha = 0.3) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
 # labs(size='PC5 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")+
   guides(size = "none") +
  ggtitle("PC5: Affective.")


# PC6 rank
umap_rank_pc6 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC6rank, 
                 colour = RC6rank), alpha = 0.4) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
#  labs(size='PC6 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")+
   guides(size = "none") +
  ggtitle("PC6: Depression")

# plot all together
cowplot::plot_grid(umap_rank_pc1,
                   umap_rank_pc2,
                   umap_rank_pc3,
                   umap_rank_pc4,
                   umap_rank_pc5,
                   umap_rank_pc6,
                   labels = "AUTO",
                   ncol = 3)
# saved as 1500*2000 dim

# each plot (per group) saved as 550*550

```

Another method is we can compute % change in the PC score and see where people who change the most lie in the UMAP space. Lets try this out.

```{r}

# rank PC scores by percentage change
temp <- rank_fit_pca_noLL_scores_df_ID %>%
  group_by(ID) %>%
  arrange(Visit, .by_group = T) %>%
  mutate(RC1_pctchg = (RC1/first(RC1)-1)*100,
         RC2_pctchg = (RC2/first(RC2)-1)*100,
         RC3_pctchg = (RC3/first(RC3)-1)*100,
         RC4_pctchg = (RC4/first(RC4)-1)*100,
         RC5_pctchg = (RC5/first(RC5)-1)*100,
         RC6_pctchg = (RC6/first(RC6)-1)*100)
```

One of the questions of interest will be where each patient group lies in these spaces. For this, I can draw an ellipse either for each patient groups or for each time points. For this, I will simply replicate the code above and add a `stat_ellipse` function.

```{r}

# Stat ellipse by group

# PC1 

umap_rank_pc1_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC1rank,
                 colour = RC1rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC1") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC1 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
   # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) +
  scale_fill_manual(values = group.colours)


# PC2 
umap_rank_pc2_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC2rank,
                 colour = RC2rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC2") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC2 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
   # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) + 
  scale_fill_manual(values = group.colours)


# PC 3
umap_rank_pc3_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC3rank,
                 colour = RC3rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC3") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC3 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
   # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) + 
  scale_fill_manual(values = group.colours)


# PC4 
umap_rank_pc4_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC4rank,
                 colour = RC4rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC4") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC4 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) + 
  scale_fill_manual(values = group.colours)


# PC5
umap_rank_pc5_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC5rank,
                 colour = RC5rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC5") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC5 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) + 
  scale_fill_manual(values = group.colours)


# PC6
umap_rank_pc6_groupellipse <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC6rank,
                 colour = RC6rank), alpha = 0.5) +
  stat_ellipse(geom = "polygon",
               aes(fill = Dx),
               alpha = 0.25, 
               level = 0.9,
               show.legend = T,
               # multivariate t-distribution
               type = "t") +
  theme_bw() +
  theme(legend.position = c(0.12,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC6") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC6 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none",
         fill = guide_legend("Group")) + 
  scale_fill_manual(values = group.colours)

# plot all together
# since I want a common legend for them, I'm using ggpubr's ggarrange
ggpubr::ggarrange(umap_rank_pc1_groupellipse,
                   umap_rank_pc2_groupellipse,
                   umap_rank_pc3_groupellipse,
                   umap_rank_pc4_groupellipse,
                   umap_rank_pc5_groupellipse,
                   umap_rank_pc6_groupellipse,
                  labels = "AUTO",
                  ncol = 2,
                  nrow = 3, 
                  common.legend = T, 
                  legend = "bottom")

# saved as 1500*1500 dim

# each plot (per group) saved as 1070*750

```

#### Explore PC rank changes by Dx (still and animate)

One of the questions is how does the ranking of the PCs correspond to the Dx that the patient has. For this, I will have to repeat the PC UMAP rank plots and double code the points for colour (dx) and shape (rank). I will also try to animate these as well.

```{r}

# re use the umap_df_rank dataset to make a multiplot of all PCs

# PC1 rank
umap_rank_bydx_pc1 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC1rank,
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC1") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC1 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")

# PC2 rank
umap_rank_bydx_pc2 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC2rank,
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC2") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC2 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")

# PC3 rank
umap_rank_bydx_pc3 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC3rank, 
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC3") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC3 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")


# PC4 rank
umap_rank_bydx_pc4 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC4rank, 
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC4") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC4 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")


# PC5 rank
umap_rank_bydx_pc5 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC5rank, 
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC5") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC5 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")


# PC6 rank
umap_rank_bydx_pc6 <- umap_df_rank %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = RC6rank, 
                 colour = Dx), alpha = 0.5) +
  theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC6") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='PC6 Rank') +
  # change legend limits
  scale_size_continuous(limits = c(0, 1170), 
                        breaks = c(1, 100, 400, 700, 1170)) +
  scale_color_manual(values = group.colours) +
  labs(colour = "Group")

# plot all together
cowplot::plot_grid(umap_rank_bydx_pc1,
                   umap_rank_bydx_pc2,
                   umap_rank_bydx_pc3,
                   umap_rank_bydx_pc4,
                   umap_rank_bydx_pc5,
                   umap_rank_bydx_pc6,
                   labels = "AUTO",
                   ncol = 2)
# saved as 1500*1500 dim

# each plot (per group) saved as 700*550

```

#### Contour PC ranks by Dx and tertiles

Similar to the rank plots, MALR suggested it would be useful to see what the contour of the every group's PC ranks are. I think this means he wants me to explore a combination of PC rank plots nad density plots. This can be a bit challenging as there are two continuous variables that we are trying to plot. One way is to divide the PC ranks into tertiles and see where the low performers vs high performers are.

```{r}

# lets create tertiles for each rank

umap_df_rank_ntile <- umap_df_rank

umap_df_rank_ntile$RC1rank_ttile <- ntile(umap_df_rank_ntile$RC1rank, 3)
umap_df_rank_ntile$RC2rank_ttile <- ntile(umap_df_rank_ntile$RC2rank, 3)
umap_df_rank_ntile$RC3rank_ttile <- ntile(umap_df_rank_ntile$RC3rank, 3)
umap_df_rank_ntile$RC4rank_ttile <- ntile(umap_df_rank_ntile$RC4rank, 3)
umap_df_rank_ntile$RC5rank_ttile <- ntile(umap_df_rank_ntile$RC5rank, 3)
umap_df_rank_ntile$RC6rank_ttile <- ntile(umap_df_rank_ntile$RC6rank, 3)

# factorise these columns and rename them into Top, mid, bottom
umap_df_rank_ntile_2 <- umap_df_rank_ntile %>% 
  mutate(across(13:18, ~  case_when(. == 1 ~ "Top",
                          . == 2 ~ "Mid",
                          . == 3 ~ "Bottom"))) %>%
  mutate(across(ends_with("tile"), as.factor))

# reorder factor levels for Top to be highest and bottom to be lowest
umap_df_rank_ntile_2$RC1rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC1rank_ttile))
umap_df_rank_ntile_2$RC2rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC2rank_ttile))
umap_df_rank_ntile_2$RC3rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC3rank_ttile))
umap_df_rank_ntile_2$RC4rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC4rank_ttile))
umap_df_rank_ntile_2$RC5rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC5rank_ttile))
umap_df_rank_ntile_2$RC6rank_ttile <- fct_rev(factor(umap_df_rank_ntile_2$RC6rank_ttile))
                          

# factorise stable vs. stepwise vs. rapid progressors
umap_df_rank_ntile_2 %>%
  group_by(ID) %>%
  mutate(RC1rank_ttile_group = ifelse(
    (Visit = 1 & RC1rank_ttile == "Top") & 
      (Visit = 2 & RC1rank_ttile == "Top") &
      (Visit = 2 & RC1rank_ttile == "Top"), "Stable", "Unstable" ))
           
# plot the number of people shifting between these tertiles in alluvial plots
# umap_df_rank_ntile_2 %>%
#   ggplot(aes(x = Visit,
#              stratum = RC1rank_ttile,
#              alluvium = Dx,
#              fill = ID,
#              label = Dx)) +
#   geom_flow(aes(fill = Dx)) +  # <-- specify alternate stat
#   geom_stratum(color = "grey", na.rm = TRUE) +
#   geom_text(stat = "stratum", size = 4)


# plot these tertiles 

# PC1
umap_rank_ttile_pc1 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC1rank_ttile),
                  alpha = 0.5, bins = 3) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC1") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC1 tertile")


# PC2 
umap_rank_ttile_pc2 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC2rank_ttile),
                  alpha = 0.5) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC2") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC2 tertile")


# PC3
umap_rank_ttile_pc3 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC3rank_ttile),
                  alpha = 0.5, bins = 3) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC3") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC3 tertile")


# PC4
umap_rank_ttile_pc4 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC4rank_ttile),
                  alpha = 0.5, bins = 3) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC4") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC4 tertile")


# PC5
umap_rank_ttile_pc5 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC5rank_ttile),
                  alpha = 0.5, bins = 3) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC5") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC5 tertile")


# PC6
umap_rank_ttile_pc6 <- umap_df_rank_ntile_2 %>% 
    ggplot(aes(x = V1, 
               y = V2)) +
  stat_density_2d(geom = "polygon",
                  contour_var = "count", # scale polygon size by number of people in each bin
                  aes(fill = RC6rank_ttile),
                  alpha = 0.5, bins = 3) +
  # change colour scheme of each of the bins
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  theme(legend.position = c(0.15, 0.2),
        legend.direction = "vertical",
        legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("PC6") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(fill = "PC6 tertile")


# plot all together
cowplot::plot_grid(umap_rank_ttile_pc1,
                   umap_rank_ttile_pc2,
                   umap_rank_ttile_pc3,
                   umap_rank_ttile_pc4,
                   umap_rank_ttile_pc5,
                   umap_rank_ttile_pc6,
                   labels = "AUTO",
                   ncol = 2)
# saved as 1500*1500 dim

# each plot (per group) saved as 550*550


```

#### Associations with clinical and demographic vars

For these steps, lets merge in the meta-data into the UMAP. I will use the UMAP_df_rank data so we can look at some PC scores in detail as well.

```{r}

# first create a copy of the umap_df_rank dataset

umap_df_rank_withMeta <- umap_df_rank

# now let us create some copies of the ID (call id NACCID), Visit (call it NACCVNUM) cols so we can left_join the meta-data in.

umap_df_rank_withMeta$NACCVNUM <- as.numeric(umap_df_rank_withMeta$Visit)
umap_df_rank_withMeta$NACCID <- umap_df_rank_withMeta$ID

# left join nacc_voi_dem_meta 
umap_df_rank_withMeta_1 <- umap_df_rank_withMeta %>%
  # first left join in the original PC rotated scores
  left_join(fit_pca_noLL_scores_df_ID, by = c("ID", "Visit")) %>%
  # now join in the demographic and clinical vars of interest
  left_join(nacc_voi_dem_meta, by = c("NACCID", "NACCVNUM")) %>%
  # also left join in VISIT YEAR from nacc_9 so that we can have year of assessment
  left_join(nacc_9_meta, by = c("NACCID", "NACCVNUM")) %>%
  # finally, left join in the year of admiting to nursing home  data
  left_join(nacc_nursing_meta, by = c("NACCID", "NACCVNUM")) %>%
  # compute age at visit
  mutate(AgeVisit = VISITYR - BIRTHYR) %>%
  # replace all 8888 with NA for nursing home visit (see UDS-DD)
  replace_with_na(replace = list(NACCNRYR = c(8888))) %>%
  # compute age at which someone was admitted to nursing home
  mutate(AgeNursing = NACCNRYR - BIRTHYR) %>%
  # create binary var for whether admitted to nursing home or now
  mutate(AdmittedNursingHome = case_when(AgeNursing > 1 ~ "Yes",
                                     TRUE ~ "No")) %>%
   # compute difference between each visit time and year to death 
  mutate(AgeToDeath = AgeDeath - AgeVisit) %>%
   # create a new column for mutation status
  # note that this bit of code is redundant as no one has mutation data (all sporadic)
  mutate(MutationStatus = case_when(NACCAM == 1 ~ "AD-APP",
                                    NACCAM == 2 ~ "AD-PSEN1",
                                    NACCAM == 3 ~ "AD-PSEN2",
                                    NACCAM == 8 ~ "AD-Other",
                                    NACCFM == 1 ~ "FTLD-MAPT",
                                    NACCFM == 2 ~ "FTLD-PGRN",
                                    NACCFM == 3 ~ "FTLD-C9",
                                    NACCFM == 4 ~ "FTLD-FUS",
                                    NACCFM == 8 ~ "FTLD-Other",
                                    NACCOM == 1 ~ "Non-AD/FTLD",
                                    TRUE ~ NA_character_)) %>% # looks like no patients have mutation data so not worth exploring 
# create a new survival variable that is a factor
  mutate(SurvivalStatus = case_when(NACCDIED == 1 ~ "Deceased",
                                NACCDIED == 0 ~ "Not Deceased"))

```

##### Associations with disease severity

How does CDR SoB relate to UMAP dimension scores?

```{r}

umap_demvars_CDRSOB <- umap_df_rank_withMeta_1 %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(size = CDRSUM, 
                 colour = CDRSUM), alpha = 0.4) +
  theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  ggtitle("CDR SoB") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(size='CDR SoB') +
  # change legend limits
  scale_color_gradient2(low = "white", mid = "grey", high = "black") +
  # remove second legend
  guides(colour = "none")
#  saved as 550*550

```

#### Association with survival and year of death

For all those who have date of death (DoD) data, I can use their PCA/UMAP scores at Visit 1 and see if they are predicting DoD using a staggered model where I keep adding each PC/UMAP dimension as a predictor. Some considerations with this are:

1.  Here, I've only used 3 time points data as its easy to wrangle. Some patients may have died many years later so to use their first/tfirst 3 time points to predict DoD is not a good idea.

2.  There may be other variables unaccounted for that could predict DoD but I haven't factored them in or don't have the data for it.

```{r}

# first let us plot patients by their survival status
umap_df_rank_withMeta_1 %>%
  group_by(ID) %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(colour = SurvivalStatus),
             size = 4,
             alpha = 0.4) +
  scale_colour_manual(values = c("black", "lightgrey")) +
    theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))


# lets repeat the same but this time only with KDE
umap_df_rank_withMeta_1 %>%
  group_by(ID) %>%
  filter(SurvivalStatus == "Deceased") %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  stat_density2d_filled(contour_var = "ndensity")  +# contour_var = "ndensity" displays the density estimate scaled to a max of 1
  facet_wrap(~Visit) +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
  # shift legend into panel
  theme(legend.background = element_rect(colour = "white"),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 9)) +
  guides(fill = guide_legend(title = "Scaled density"))

# 1220*500 for faceted plot, otherwise 550*550 for just fuill group KDE


# Now lets plot patients by time to nursing home admission
umap_df_rank_withMeta_1 %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(colour = AdmittedNursingHome),
             size = 4,
             alpha = 0.4) +
  scale_colour_manual(values = c("grey", "black")) +
    theme_bw() +
  theme(legend.position = c(0.2,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))

```

In discussions with Dan, we thought of a few ideas. First, we need to account for where each person started (so thier Time 1 PC score) and we need to account for how much they've moved across the three time points on that PC (avg. Euclidean distance score across 3 time points for each PC). Then we plug these two values into a logisitc regression to predict which PC can predict survival.

Update (14/2/23): In JBR lab meeting, he suggested that just using survival as an endpoint doesn't make sense because survival can be dependent on a number of factors. Instead, we should aim to use a cut-off (like survival in 3 years after first assessment) and then see whether T1 and other indices correlate with it. This is more meaningful to use in log regressions. I will also calculate this from the

```{r}

# For this, I first need to summarise the Euclidean distance within every PC, across all visits, per person and create a new DF. Then I will left-join in their TIme 1 PC values as the starting coordinate.

nacc_survival <- umap_df_rank_withMeta_1 %>% 
  group_by(ID) %>% 
  summarise(distRC1 = mean(dist(RC1)),
            distRC2 = mean(dist(RC2)),
            distRC3 = mean(dist(RC3)),
            distRC4 = mean(dist(RC4)),
            distRC5 = mean(dist(RC5)),
            distRC6 = mean(dist(RC6))) %>%
  as.data.frame()

# now left join time1 of the UMAP dataset (mainly PC values, survival etc.)
temp_join <- umap_df_rank_withMeta_1 %>%
  group_by(ID) %>%
  filter(Visit == 1) %>%
  dplyr::select(ID, Dx.x, RC1:RC6, NACCAGE, SEX, EDUC, AgeDeath, AgeVisit, AgeNursing, AgeToDeath, CDRSUM, CDRGLOB, SurvivalStatus, AdmittedNursingHome) %>%
  as.data.frame()

# left join this into the naccsurvival dataset
nacc_survival_join <- nacc_survival %>%
  left_join(temp_join, by = "ID")

#  histogram of mean time to death for all those who have died
nacc_survival_join <- nacc_survival_join %>%
    # create a new var for whether pt has died within 3 years of first time visit
  mutate(SurvWithin3Yr1stVisit = case_when(AgeToDeath <= 3 ~ "Deceased",
                                           AgeToDeath > 3 ~ "Not Deceased",
                                           TRUE ~ NA_character_))

# now you have each PC score and the average distance moved across the PCs
# do stepwise logistic regression for each Group

# binarise survival daata for both regular and SurvWithin3Yr1stVisit
nacc_survival_join$DeceasedStatus[nacc_survival_join$SurvivalStatus == "Deceased"] <- 1
nacc_survival_join$SurvWithin3Yr1stVisitBin[nacc_survival_join$SurvWithin3Yr1stVisit == "Deceased"] <- 1

nacc_survival_join$DeceasedStatus[nacc_survival_join$SurvivalStatus == "Not Deceased"] <- 0
nacc_survival_join$SurvWithin3Yr1stVisitBin[nacc_survival_join$SurvWithin3Yr1stVisit == "Not Deceased"] <- 0


# now do a histogram to see how what is the frequency of the number of years someone survives (by group)
nacc_survival_join %>%
  filter(DeceasedStatus == 1) %>%
  group_by(Dx.x) %>%
  count(AgeToDeath) %>%
  ggplot(aes(x = as.factor(AgeToDeath),
             y = n,
             fill = Dx.x,
             label = n)) +
  geom_bar(position = "stack",
           stat = "identity") +
  geom_text(size=4, 
            position = position_stack(v = 0.5)) +
  scale_fill_manual(values = group.colours) +
  labs(fill = "Group") +
  theme_bw() +
  theme(legend.box = "vertical",
        legend.position = c(0.8, 0.8),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  ylab("Count") +
  xlab("Years to death from first NACC visit")
  # saved as 650*650
  



#### Survival analysis for absolute survival status (i.e., surviving or not)

# now keep a subset of only the distance and raw PC scores that can be entered into a stepwise model
nacc_survival_1 <- nacc_survival_join %>%
  dplyr::select(starts_with(c("dist", "RC")), DeceasedStatus, Dx.x)

# now create a subset of the data for each group
nacc_survival_AD <- filter(nacc_survival_1, Dx.x == "AD")
nacc_survival_bvFTD <- filter(nacc_survival_1, Dx.x == "bvFTD")
nacc_survival_FTLDm <- filter(nacc_survival_1, Dx.x == "FTLD-motor")
nacc_survival_FTLDnos <- filter(nacc_survival_1, Dx.x == "FTLD-NOS")
nacc_survival_PPA <- filter(nacc_survival_1, Dx.x == "PPA")

# Remove the Dx variable from each dataset so we can feed the entire dataset into the stepwise logistic regression
nacc_survival_AD_1 <-nacc_survival_AD %>%
  dplyr::select(-Dx.x)
nacc_survival_bv_1 <-nacc_survival_bvFTD %>%
  dplyr::select(-Dx.x)
nacc_survival_FTLDm_1 <-nacc_survival_FTLDm %>%
  dplyr::select(-Dx.x)
nacc_survival_FTLDnos_1 <-nacc_survival_FTLDnos %>%
  dplyr::select(-Dx.x)
nacc_survival_PPA_1 <-nacc_survival_PPA %>%
  dplyr::select(-Dx.x)

# compute stepwise logistic regresssion predicting Survival status using raw PC scores and distance travelled on each PC

# AD
# build base model
AD_surv_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_AD_1)
summary(AD_surv_m1)

# perform stepwise variable selection using MASS package
AD_surv_stepwise_m1 <- AD_surv_m1 %>% stepAIC(trace = F)
summary(AD_surv_stepwise_m1)


# bvFTD
# build base model
bvFTD_surv_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_bv_1)
summary(bvFTD_surv_m1)

# perform stepwise variable selection using MASS package
bvFTD_surv_stepwise_m1 <- bvFTD_surv_m1 %>% stepAIC(trace = F)
summary(bvFTD_surv_stepwise_m1)


# FTLD m
ftldm_surv_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_FTLDm_1)
summary(ftldm_surv_m1)

# perform stepwise variable selection using MASS package
ftldm_surv_stepwise_m1 <- ftldm_surv_m1 %>% stepAIC(trace = F)
summary(ftldm_surv_stepwise_m1)


# FTLD nos
ftldnos_surv_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_FTLDnos_1)
summary(ftldnos_surv_m1)

# perform stepwise variable selection using MASS package
ftldnos_surv_stepwise_m1 <- ftldnos_surv_m1 %>% stepAIC(trace = F)
summary(ftldnos_surv_stepwise_m1)


# PPA
ppa_surv_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_PPA_1)
summary(ppa_surv_m1)

# perform stepwise variable selection using MASS package
ppa_surv_stepwise_m1 <- ppa_surv_m1 %>% stepAIC(trace = F)
summary(ppa_surv_stepwise_m1)


# REPEAT WITH DEMOGRAPHICS included = AGE, EDUCATION, AGE oF VISIT, SEX

nacc_survival_dem_1 <- nacc_survival %>%
  dplyr::select(starts_with(c("dist", "RC")), SEX, EDUC, AgeVisit, DeceasedStatus, Dx.x)

# now create a subset of the data for each group
nacc_survival_dem_AD <- filter(nacc_survival_dem_1, Dx.x == "AD")
nacc_survival_dem_bvFTD <- filter(nacc_survival_dem_1, Dx.x == "bvFTD")
nacc_survival_dem_FTLDm <- filter(nacc_survival_dem_1, Dx.x == "FTLD-motor")
nacc_survival_dem_FTLDnos <- filter(nacc_survival_dem_1, Dx.x == "FTLD-NOS")
nacc_survival_dem_PPA <- filter(nacc_survival_dem_1, Dx.x == "PPA")

# Remove the Dx variable from each dataset so we can feed the entire dataset into the stepwise logistic regression
nacc_survival_dem_AD_1 <-nacc_survival_dem_AD %>%
  dplyr::select(-Dx.x)
nacc_survival_dem_bv_1 <-nacc_survival_dem_bvFTD %>%
  dplyr::select(-Dx.x)
nacc_survival_dem_FTLDm_1 <-nacc_survival_dem_FTLDm %>%
  dplyr::select(-Dx.x)
nacc_survival_dem_FTLDnos_1 <-nacc_survival_dem_FTLDnos %>%
  dplyr::select(-Dx.x)
nacc_survival_dem_PPA_1 <-nacc_survival_dem_PPA %>%
  dplyr::select(-Dx.x)

# compute stepwise logistic regresssion predicting Survival status using raw PC scores and distance travelled on each PC

# AD
# build base model
AD_surv_dem_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_dem_AD_1)
summary(AD_surv_dem_m1)

# perform stepwise variable selection using MASS package
AD_surv_dem_stepwise_m1 <- AD_surv_dem_m1 %>% stepAIC(trace = F)
summary(AD_surv_dem_stepwise_m1)


# bvFTD
# build base model
bvFTD_surv_dem_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_dem_bv_1)
summary(bvFTD_surv_dem_m1)

# perform stepwise variable selection using MASS package
bvFTD_surv_dem_stepwise_m1 <- bvFTD_surv_dem_m1 %>% stepAIC(trace = F)
summary(bvFTD_surv_dem_stepwise_m1)


# FTLD m
ftldm_surv_dem_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_dem_FTLDm_1)
summary(ftldm_surv_dem_m1)

# perform stepwise variable selection using MASS package
ftldm_surv_dem_stepwise_m1 <- ftldm_surv_dem_m1 %>% stepAIC(trace = F)
summary(ftldm_surv_dem_stepwise_m1)


# FTLD nos
ftldnos_surv_dem_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_dem_FTLDnos_1)
summary(ftldnos_surv_dem_m1)

# perform stepwise variable selection using MASS package
ftldnos_surv_dem_stepwise_m1 <- ftldnos_surv_dem_m1 %>% stepAIC(trace = F)
summary(ftldnos_surv_dem_stepwise_m1)


# PPA
ppa_surv_dem_m1 <- glm(DeceasedStatus ~ ., data = nacc_survival_dem_PPA_1)
summary(ppa_surv_dem_m1)

# perfor8m stepwise variable selection using MASS package
ppa_surv_dem_stepwise_m1 <- ppa_surv_dem_m1 %>% stepAIC(trace = F)
summary(ppa_surv_dem_stepwise_m1)


```

Now let us try using SurvWithin3Years1stVisit var.

```{r}


# now keep a subset of only the distance and raw PC scores that can be entered into a stepwise model
nacc_survival_3y1stvis <- nacc_survival_join %>%
  dplyr::select(starts_with(c("dist", "RC")), SurvWithin3Yr1stVisitBin, Dx.x) %>%
  # remove all NA rows from SurvWithin3Yr1stVisitBin as these are not dead
  drop_na(SurvWithin3Yr1stVisitBin)


# first run log regression in the full group
nacc_survival_3y1stvis_log <- nacc_survival_3y1stvis %>%
  dplyr::select(-Dx.x)

# build base model
all_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_log)
summary(all_surv_3y_m1)

# perform stepwise variable selection using MASS package
all_surv_3y_stepwise_m1 <- all_surv_3y_m1 %>% stepAIC(trace = F)
summary(all_surv_3y_stepwise_m1)


# now create a subset of the data for each group
nacc_survival_3y1stvis_AD <- filter(nacc_survival_3y1stvis, Dx.x == "AD")
nacc_survival_3y1stvis_bvFTD <- filter(nacc_survival_3y1stvis, Dx.x == "bvFTD")
nacc_survival_3y1stvis_FTLDm <- filter(nacc_survival_3y1stvis, Dx.x == "FTLD-motor")
nacc_survival_3y1stvis_FTLDnos <- filter(nacc_survival_3y1stvis, Dx.x == "FTLD-NOS")
nacc_survival_3y1stvis_PPA <- filter(nacc_survival_3y1stvis, Dx.x == "PPA")

# Remove the Dx variable from each dataset so we can feed the entire dataset into the stepwise logistic regression
nacc_survival_3y1stvis_AD_1 <-nacc_survival_3y1stvis_AD %>%
  dplyr::select(-Dx.x)
nacc_survival_3y1stvis_bv_1 <-nacc_survival_3y1stvis_bvFTD %>%
  dplyr::select(-Dx.x)
nacc_survival_3y1stvis_FTLDm_1 <-nacc_survival_3y1stvis_FTLDm %>%
  dplyr::select(-Dx.x)
nacc_survival_3y1stvis_FTLDnos_1 <-nacc_survival_3y1stvis_FTLDnos %>%
  dplyr::select(-Dx.x)
nacc_survival_3y1stvis_PPA_1 <-nacc_survival_3y1stvis_PPA %>%
  dplyr::select(-Dx.x)


# compute stepwise logistic regresssion predicting Survival_3y1stvis status using raw PC scores and distance travelled on each PC

# AD
# build base model
AD_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_AD_1)
summary(AD_surv_3y_m1)

# perform stepwise variable selection using MASS package
AD_surv_3y_stepwise_m1 <- AD_surv_3y_m1 %>% stepAIC(trace = F)
summary(AD_surv_3y_stepwise_m1)


# bvFTD
# build base model
bvFTD_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_bv_1)
summary(bvFTD_surv_3y_m1)

# perform stepwise variable selection using MASS package
bvFTD_surv_3y_stepwise_m1 <- bvFTD_surv_3y_m1 %>% stepAIC(trace = F)
summary(bvFTD_surv_3y_stepwise_m1)


# FTLD m
ftldm_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_FTLDm_1)
summary(ftldm_surv_3y_m1)

# perform stepwise variable selection using MASS package
ftldm_surv_3y_stepwise_m1 <- ftldm_surv_3y_m1 %>% stepAIC(trace = F)
summary(ftldm_surv_3y_stepwise_m1)


# FTLD nos
ftldnos_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_FTLDnos_1)
summary(ftldnos_surv_3y_m1)

# perform stepwise variable selection using MASS package
ftldnos_surv_3y_stepwise_m1 <- ftldnos_surv_3y_m1 %>% stepAIC(trace = F)
summary(ftldnos_surv_3y_stepwise_m1)


# PPA
ppa_surv_3y_m1 <- glm(SurvWithin3Yr1stVisitBin ~ ., data = nacc_survival_3y1stvis_PPA_1)
summary(ppa_surv_3y_m1)

# perform stepwise variable selection using MASS package
ppa_surv_3y_stepwise_m1 <- ppa_surv_3y_m1 %>% stepAIC(trace = F)
summary(ppa_surv_3y_stepwise_m1)

```

UPDATE: in a follow up discussion with MALR, he suggests using the centroid based approach like I've done in the above code. Using every one's starting score says how far each person is moving from themselves, but instead what we need to know is how far they are moving in the whole space (which can be measured by using the overall group centroid).

```{r}


###### REPEAT USING CENTROID based data For all PCs and for each PC

# lets work from the umap_df_rank_withMeta_1 dataset above
nacc_survival_centroid <- umap_df_rank_withMeta_1 %>%
  dplyr::select(ID, Dx.x, Visit, RC1:RC6, NACCDIED, SurvivalStatus, AgeToDeath) %>%
  # compute colwise mean and grand mean of each row
  mutate(meanRC1 = mean(RC1),
            meanRC2 = mean(RC2),
            meanRC3 = mean(RC3),
            meanRC4 = mean(RC4),
            meanRC5 = mean(RC5),
            meanRC6 = mean(RC6), 
           grandmean = mean(c(meanRC1, meanRC2, meanRC3, meanRC4, meanRC5, meanRC6))) %>%
           # now calculate a new score with the rowwise PC means
  group_by(ID) %>%
  rowwise() %>%
  mutate(PCmeans = mean(c(RC1, RC2, RC3, RC4, RC5, RC6))) %>%
  # positive values means the person is performing bettter than the group
  mutate(Dispersion = PCmeans-grandmean) 

nacc_survival_centroid_2 <- nacc_survival_centroid %>%
  group_by(ID) %>%
  # calculate the average distance travelled from Centroid (i.e, mean dispersion per person over time)
  mutate(meanDispersion = mean(Dispersion)) %>% # this will have put in the same value for all IDs
      # create a new var for whether pt has died within 3 years of first time visit
  mutate(SurvWithin3Yr1stVisit = case_when(AgeToDeath <= 3 ~ "1",
                                           AgeToDeath > 3 ~ "0",
                                           TRUE ~ NA_character_)) %>%
  # so now lets filter T1 (as T1 data will have the starting dispersion value (called Dispersion) and the mean value (meanDispersion))
  filter(Visit == 1) %>%
  ungroup(ID) %>%
  drop_na(SurvWithin3Yr1stVisit) %>%
  # keep only the ID, Dx, and Dispersion values
  dplyr::select(Dx.x, Dispersion, meanDispersion, SurvWithin3Yr1stVisit)

# change Surv var to numeric
nacc_survival_centroid_2$SurvWithin3Yr1stVisit <- as.numeric(nacc_survival_centroid_2$SurvWithin3Yr1stVisit)


# now create a subset of the data for each group
nacc_survival_centroid_AD <- filter(nacc_survival_centroid_2, Dx.x == "AD")
nacc_survival_centroid_bvFTD <- filter(nacc_survival_centroid_2, Dx.x == "bvFTD")
nacc_survival_centroid_FTLDm <- filter(nacc_survival_centroid_2, Dx.x == "FTLD-motor")
nacc_survival_centroid_FTLDnos <- filter(nacc_survival_centroid_2, Dx.x == "FTLD-NOS")
nacc_survival_centroid_PPA <- filter(nacc_survival_centroid_2, Dx.x == "PPA")


# Remove the Dx variable from each dataset so we can feed the entire dataset into the stepwise logistic regression
nacc_survival_centroid_AD_1 <- nacc_survival_centroid_AD %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_bv_1 <-nacc_survival_centroid_bvFTD %>%
 dplyr::select(-Dx.x) 
nacc_survival_centroid_FTLDm_1 <-nacc_survival_centroid_FTLDm %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_FTLDnos_1 <-nacc_survival_centroid_FTLDnos %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_PPA_1 <-nacc_survival_centroid_PPA %>%
  dplyr::select(-Dx.x) 


# compute stepwise logistic regresssion predicting Survival status using raw PC scores and distance travelled on each PC

# AD
# build base model
AD_surv_centroid_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_AD_1, family = "binomial")
summary(AD_surv_centroid_m1)

# perform stepwise variable selection using MASS package
AD_surv_centroid_stepwise_m1 <- AD_surv_centroid_m1 %>% stepAIC(trace = F)
summary(AD_surv_centroid_stepwise_m1)


# bvFTD
# build base model
bvFTD_surv_centroid_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_bv_1, family = "binomial")
summary(bvFTD_surv_centroid_m1)

# perform stepwise variable selection using MASS package
bvFTD_surv_centroid_stepwise_m1 <- bvFTD_surv_centroid_m1 %>% stepAIC(trace = F)
summary(bvFTD_surv_centroid_stepwise_m1)


# FTLD m
ftldm_surv_centroid_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_FTLDm_1, family = "binomial")
summary(ftldm_surv_centroid_m1)

# perform stepwise variable selection using MASS package
ftldm_surv_centroid_stepwise_m1 <- ftldm_surv_centroid_m1 %>% stepAIC(trace = F)
summary(ftldm_surv_centroid_stepwise_m1)


# FTLD nos
ftldnos_surv_centroid_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_FTLDnos_1, family = "binomial")
summary(ftldnos_surv_centroid_m1)

# perform stepwise variable selection using MASS package
ftldnos_surv_centroid_stepwise_m1 <- ftldnos_surv_centroid_m1 %>% stepAIC(trace = F)
summary(ftldnos_surv_centroid_stepwise_m1)


# PPA
ppa_surv_centroid_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PPA_1, family = "binomial")
summary(ppa_surv_centroid_m1)

# perform stepwise variable selection using MASS package
ppa_surv_centroid_stepwise_m1 <- ppa_surv_centroid_m1 %>% stepAIC(trace = F)
summary(ppa_surv_centroid_stepwise_m1)



# repeat with PC specific means - i.e., centroid on each PC

nacc_survival_centroid_PCspecific <- umap_df_rank_withMeta_1 %>%
  dplyr::select(ID, Dx.x, Visit, RC1:RC6, NACCDIED, SurvivalStatus, AgeToDeath) %>%
  # calculate mean of each of the RC columns
  mutate(meanRC1 = mean(RC1),
         meanRC2 = mean(RC2),
         meanRC3 = mean(RC3),
         meanRC4 = mean(RC4),
         meanRC5 = mean(RC5),
         meanRC6 = mean(RC6)) %>%
  # compute difference between everyone's PC score with the mean of the overall PC
  mutate(RC1Dist = RC1-meanRC1,
         RC2Dist = RC2-meanRC2,
         RC3Dist = RC3-meanRC3,
         RC4Dist = RC4-meanRC4,
         RC5Dist = RC5-meanRC5,
         RC6Dist = RC6-meanRC6) %>%
  group_by(ID) %>%
  # calculate the average distance travelled from Centroid (i.e, mean dispersion per person over time)
  mutate(MeanRC1Dist = mean(RC1Dist),
            MeanRC2Dist = mean(RC2Dist),
            MeanRC3Dist = mean(RC3Dist),
            MeanRC4Dist = mean(RC4Dist),
            MeanRC5Dist = mean(RC5Dist),
            MeanRC6Dist = mean(RC6Dist)) %>% # this will have put in the same value for all IDs
        # create a new var for whether pt has died within 3 years of first time visit
  mutate(SurvWithin3Yr1stVisit = case_when(AgeToDeath <= 3 ~ "1",
                                           AgeToDeath > 3 ~ "0",
                                           TRUE ~ NA_character_)) %>%
  # so now lets filter T1 (as T1 data will have the starting dispersion value (called Dispersion) and the mean value (meanDispersion))
  filter(Visit == 1) %>%
  ungroup(ID) %>%
  drop_na(SurvWithin3Yr1stVisit) %>%
  # keep only the ID, Dx, and Dispersion values
  dplyr::select(Dx.x, ends_with("Dist"), SurvWithin3Yr1stVisit)


# change Surv var to numeric
nacc_survival_centroid_PCspecific$SurvWithin3Yr1stVisit <- as.numeric(nacc_survival_centroid_PCspecific$SurvWithin3Yr1stVisit)


# now create a subset of the data for each group
nacc_survival_centroid_PCspecific_AD <- filter(nacc_survival_centroid_PCspecific, Dx.x == "AD")
nacc_survival_centroid_PCspecific_bvFTD <- filter(nacc_survival_centroid_PCspecific, Dx.x == "bvFTD")
nacc_survival_centroid_PCspecific_FTLDm <- filter(nacc_survival_centroid_PCspecific, Dx.x == "FTLD-motor")
nacc_survival_centroid_PCspecific_FTLDnos <- filter(nacc_survival_centroid_PCspecific, Dx.x == "FTLD-NOS")
nacc_survival_centroid_PCspecific_PPA <- filter(nacc_survival_centroid_PCspecific, Dx.x == "PPA")


# Remove the Dx variable from each dataset so we can feed the entire dataset into the stepwise logistic regression
nacc_survival_centroid_PCspecific_AD_1 <- nacc_survival_centroid_PCspecific_AD %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_PCspecific_bv_1 <-nacc_survival_centroid_PCspecific_bvFTD %>%
 dplyr::select(-Dx.x) 
nacc_survival_centroid_PCspecific_FTLDm_1 <-nacc_survival_centroid_PCspecific_FTLDm %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_PCspecific_FTLDnos_1 <-nacc_survival_centroid_PCspecific_FTLDnos %>%
  dplyr::select(-Dx.x) 
nacc_survival_centroid_PCspecific_PPA_1 <-nacc_survival_centroid_PCspecific_PPA %>%
  dplyr::select(-Dx.x) 


# compute stepwise logistic regresssion predicting Survival status using raw PC scores and distance travelled on each PC

# AD
# build base model
AD_surv_centroid_PCspecific_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PCspecific_AD_1, family = "binomial")
summary(AD_surv_centroid_PCspecific_m1)

# perform stepwise variable selection using MASS package
AD_surv_centroid_PCspecific_stepwise_m1 <- AD_surv_centroid_PCspecific_m1 %>% stepAIC(trace = F)
summary(AD_surv_centroid_PCspecific_stepwise_m1)


# bvFTD
# build base model
bvFTD_surv_centroid_PCspecific_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PCspecific_bv_1, family = "binomial")
summary(bvFTD_surv_centroid_PCspecific_m1)

# perform stepwise variable selection using MASS package
bvFTD_surv_centroid_PCspecific_stepwise_m1 <- bvFTD_surv_centroid_PCspecific_m1 %>% stepAIC(trace = F)
summary(bvFTD_surv_centroid_PCspecific_stepwise_m1)


# FTLD m
ftldm_surv_centroid_PCspecific_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PCspecific_FTLDm_1, family = "binomial")
summary(ftldm_surv_centroid_PCspecific_m1)

# perform stepwise variable selection using MASS package
ftldm_surv_centroid_PCspecific_stepwise_m1 <- ftldm_surv_centroid_PCspecific_m1 %>% stepAIC(trace = F)
summary(ftldm_surv_centroid_PCspecific_stepwise_m1)


# FTLD nos
ftldnos_surv_centroid_PCspecific_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PCspecific_FTLDnos_1, family = "binomial")
summary(ftldnos_surv_centroid_PCspecific_m1)

# perform stepwise variable selection using MASS package
ftldnos_surv_centroid_PCspecific_stepwise_m1 <- ftldnos_surv_centroid_PCspecific_m1 %>% stepAIC(trace = F)
summary(ftldnos_surv_centroid_PCspecific_stepwise_m1)


# PPA
ppa_surv_centroid_PCspecific_m1 <- glm(SurvWithin3Yr1stVisit ~ ., data = nacc_survival_centroid_PCspecific_PPA_1, family = "binomial")
summary(ppa_surv_centroid_PCspecific_m1)

# perform stepwise variable selection using MASS package
ppa_surv_centroid_PCspecific_stepwise_m1 <- ppa_surv_centroid_PCspecific_m1 %>% stepAIC(trace = F)
summary(ppa_surv_centroid_PCspecific_stepwise_m1)

```

#### Modelling spatial location and association with survival

I was wondering if there is a better method to map the relationship between the location of each person on the UMAP space and associations with survival. For this, I have a few options:

1.  If the relationship is between survival and each person's X and Y UMAP position is linear, then I do a straightforward GLM with the interaction of x&y terms (mean centered).
2.  If it is non linear, maybe I can use a spatial regression model?
3.  I also trie dout a Generalised Additive Model (see <https://m-clark.github.io/generalized-additive-models/appendix.html#space> and <https://stats.stackexchange.com/questions/380426/when-to-use-a-gam-vs-glm>). Bascially, a GAM is a better version of GLM when the underlying pattern of data are non linear. Whereas in GLM, the linear predictor is a weighted sum of the coviarates, in the GAM, this term is replaced with a sum of a smooth function which allows it to uncover non linear effects of numerical covariates. See more Here; <https://stats.stackexchange.com/questions/318278/generalized-additive-models-gams-interactions-and-covariates>

A nice explanation here: <https://www.r-bloggers.com/2019/09/spatial-regression-in-r-part-1-spamm-vs-glmmtmb/> and <https://stats.stackexchange.com/questions/12223/how-to-tune-smoothing-in-mgcv-gam-model>. Interprtation of Moran's I here: <https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm#:~:text=If%20the%20values%20in%20the,the%20Index%20will%20be%20negative.>

I will use the Survival within 3 years from 1st NACC Visit var as the outcome variable

```{r}

# first create a new df with UMAP coordinates
nacc_survival_umap <- umap_df_rank_withMeta_1 %>% 
  group_by(ID) %>%
  filter(Visit == 1) %>%
  dplyr::select(ID, V1, V2, Visit, Dx.x, RC1:RC6, NACCAGE, SEX, EDUC, AgeDeath, AgeVisit, AgeNursing, AgeToDeath, CDRSUM, CDRGLOB, SurvivalStatus, AdmittedNursingHome) %>%
  # create a new var for whether pt has died within 3 years of first time visit
  mutate(SurvWithin3Yr1stVisit = case_when(AgeToDeath <= 3 ~ "1",
                                           AgeToDeath > 3 ~ "0",
                                           TRUE ~ NA_character_)) %>%
  # remove NA from SurvWithin3Yr1stVisit
  drop_na(SurvWithin3Yr1stVisit) %>%
  as.data.frame()

# convert SurvWithin3Yr1stVisit to numeric
nacc_survival_umap$SurvWithin3Yr1stVisit <- as.numeric(nacc_survival_umap$SurvWithin3Yr1stVisit)


# create a sequence of integers to left join residuals later
nacc_survival_umap$int <- seq.int(nrow(nacc_survival_umap))

# binarise survival daata
nacc_survival_umap$DeceasedStatus[nacc_survival_umap$SurvivalStatus == "Deceased"] <- 1
nacc_survival_umap$DeceasedStatus[nacc_survival_umap$SurvivalStatus == "Not Deceased"] <- 0


# Now, we have to run two things. 
# First is a GLM. Here, we will predict deceased stattus using the scaled iinteraction of the UMAP coordinates. 

# Now I run a GLM using the UMAP coordinates (mean centered)

g <- glm(SurvWithin3Yr1stVisit ~ scale(V1)*scale(V2), data = nacc_survival_umap, family = binomial(link = "logit"))

# take the residuals and fitted values and left-join it back into the nacc_survival_umap df
# predict using probability of the event
nacc_survival_glm <- as.data.frame(predict(g, type = "response"))
nacc_survival_glm$resid <- as.vector(resid(g))

# change colnames
colnames(nacc_survival_glm) <- c("glm.fitted", "glm.resid")
nacc_survival_glm$int <- seq.int(nrow(nacc_survival_glm))

nacc_survival_umap_1 <- nacc_survival_umap %>% 
  left_join(nacc_survival_glm, by = "int")


# Since this is spatial data, we need to test for spatial autocorrelation. 
# If autocorrelated, then we need to use a spatial regression model. If not, you can get away with the GLM

# using DHARMA
sims <- simulateResiduals(g)
testSpatialAutocorrelation(sims, x = nacc_survival_umap$V1, y = nacc_survival_umap$V2, plot = FALSE)

# there doesn't seem to be autocrrelation so we can probably go ahead with GLM?
# this lack of autocorrelation finding also maps on to the KDE statistics that we did where the data are not randomly distributed.


# Lets try a generalised additive model in any case. I will left join its fitted values and residuals into the df and then examine if GLM and GAM converge

# use mgcv to construct GAM

f <- mgcv::gam(SurvWithin3Yr1stVisit ~ s(V1, V2, bs = "gp"), family = binomial(logit), data = nacc_survival_umap_1, scale = -1, method = "REML")

# take residuals and fitted values
nacc_survival_gam <- as.data.frame(predict(f, type = "terms"))
nacc_survival_gam$resid <- as.vector(residuals(f))

# change colnames
colnames(nacc_survival_gam) <- c("gam.fitted", "gam.resid")
nacc_survival_gam$int <- seq.int(nrow(nacc_survival_gam))

nacc_survival_umap_2 <- nacc_survival_umap_1 %>% 
  left_join(nacc_survival_gam, by = "int")

# check correlations between GLM and GAM fitted values
cor(nacc_survival_umap_2$glm.fitted, nacc_survival_umap_2$gam.fitted)
# between residuals
cor(nacc_survival_umap_2$glm.resid, nacc_survival_umap_2$gam.resid)
# excellent

# check model fits for both GLM and GAM
  
# GLM
roc(nacc_survival_umap_2$SurvWithin3Yr1stVisit, as.vector(nacc_survival_umap_2$glm.fitted), percent = T)
  
# GAM
roc(nacc_survival_umap_2$SurvWithin3Yr1stVisit, as.vector(nacc_survival_umap_2$gam.fitted), percent = T)


# make the ggplot but coding in colours for predicted values

# map residuals into UMAP space 
nacc_survival_umap_2 %>%
  group_by(ID) %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(colour = gam.fitted),
             alpha = 0.4) 


  scale_colour_manual(values = c("black", "lightgrey")) +
    theme_bw() +
  theme(legend.position = c(0.1,0.2),
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))


# now lets factorise the decision boundary of both GLM and GAM fitted values at 0.5. If fitted > .5 and deceased = 1, then correct classification, nad vice versa for <.5 and 0. Else, incorrect. Then we cna plot how many correct and incorrect and in which space these correct decisions fall.

nacc_survival_umap_2 %>%
  # GLM pred accuracy
  mutate(GLM.Prediction.Accuracy = case_when(glm.fitted > .5 & SurvWithin3Yr1stVisit == 1 ~ "Correct-Deceased",
                                    glm.fitted < .5 & SurvWithin3Yr1stVisit == 0 ~ "Correct-NotDeceased",
                                    glm.fitted > .5 & SurvWithin3Yr1stVisit == 0 ~ "Incorrect-NotDeceased",
                                    TRUE ~ "Incorrect-Deceased")) %>%
  # GAM pred accuracy
  mutate(GAM.Prediction.Accuracy = case_when(gam.fitted > .5 & SurvWithin3Yr1stVisit == 1 ~ "Correct-Deceased",
                                    gam.fitted < .5 & SurvWithin3Yr1stVisit == 0 ~ "Correct-NotDeceased",
                                    gam.fitted > .5 & SurvWithin3Yr1stVisit == 0 ~ "Incorrect-NotDeceased",
                                    TRUE ~ "Incorrect-Deceased")) %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  facet_wrap(~GAM.Prediction.Accuracy) +
  geom_point(aes(colour = Dx.x),
    size = 4,
    alpha = .7) +
  scale_color_manual(values = group.colours) +
  theme_bw() +
  theme(legend.box = "horizontal",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(colour = "Group")
  # saved as 1k*700
```

#### Associations with pathology data

For this, I will import the pathology dataset and then merge it with the UMAP data. Shaz and Matt R are planning on working on this so for now, I will just import the data and write it as an excel sheet and send it to them.

Update as of 12/4/23: Shaz has worked on this and cleaned the data up. I'm copying her code here so all the code is in one place but also saving in her code and output as a different file in the NACC project folder.

Update as of 11/5/23: I met with Kieren the pathologist to discuss how to work with this data. His suggestion was that I code AD based on the ADNC score, split vascular into CAA and other non-amyloid vasculopathy (vascular changes/CVD), preserve LBD as is, leave our hippocampal sclerosis, cut PSP nad CBD into thier own entities, nad keep the FTLD pathologies as they are. If ADNC == -4, then I look at VASC to see whether the primary pathology is Vacsular.

```{r}

nacc_path_cols_keep <- nacc %>%
  dplyr::select("NACCID", "NACCVNUM", "NACCCOGF", "NPFORMVER", "NPSEX", "NPPMIH", 
"NPFIX", "NPFIXX", "NPWBRWT", "NPWBRF", "NACCBRNN", "NPGRCCA", 
"NPGRLA", "NPGRHA", "NPGRSNH", "NPGRLCH", "NACCAVAS", "NPTAN", 
"NPTANX", "NPABAN", "NPABANX", "NPASAN", "NPASANX", "NPTDPAN", 
"NPTDPANX", "NPHISMB", "NPHISG", "NPHISSS", "NPHIST", "NPHISO", 
"NPHISOX", "NPTHAL", "NACCBRAA", "NACCNEUR", "NPADNC", "NACCDIFF", 
"NACCVASC", "NACCAMY", "NPLINF", "NPLAC", "NPINF", "NPINF1A", 
"NPINF1B", "NPINF1D", "NPINF1F", "NPINF2A", "NPINF2B", "NPINF2D", 
"NPINF2F", "NPINF3A", "NPINF3B", "NPINF3D", "NPINF3F", "NPINF4A", 
"NPINF4B", "NPINF4D", "NPINF4F", "NACCINF", "NPHEM", "NPHEMO", 
"NPHEMO1", "NPHEMO2", "NPHEMO3", "NPMICRO", "NPOLD", "NPOLD1", 
"NPOLD2", "NPOLD3", "NPOLD4", "NACCMICR", "NPOLDD", "NPOLDD1", 
"NPOLDD2", "NPOLDD3", "NPOLDD4", "NACCHEM", "NACCARTE", "NPWMR", 
"NPPATH", "NACCNEC", "NPPATH2", "NPPATH3", "NPPATH4", "NPPATH5", 
"NPPATH6", "NPPATH7", "NPPATH8", "NPPATH9", "NPPATH10", "NPPATH11", 
"NPPATHO", "NPPATHOX", "NPART", "NPOANG", "NACCLEWY", "NPLBOD", 
"NPNLOSS", "NPHIPSCL", "NPSCL", "NPFTDTAU", "NACCPICK", "NPFTDT2", 
"NACCCBD", "NACCPROG", "NPFTDT5", "NPFTDT6", "NPFTDT7", "NPFTDT8", 
"NPFTDT9", "NPFTDT10", "NPFRONT", "NPTAU", "NPFTD", "NPFTDTDP", 
"NPALSMND", "NPOFTD", "NPOFTD1", "NPOFTD2", "NPOFTD3", "NPOFTD4", 
"NPOFTD5", "NPFTDNO", "NPFTDSPC", "NPTDPA", "NPTDPB", "NPTDPC", 
"NPTDPD", "NPTDPE", "NPPDXA", "NPPDXB", "NACCPRIO", "NPPDXD", 
"NPPDXE", "NPPDXF", "NPPDXG", "NPPDXH", "NPPDXI", "NPPDXJ", "NPPDXK", 
"NPPDXL", "NPPDXM", "NPPDXN", "NACCDOWN", "NPPDXP", "NPPDXQ", 
"NACCOTHP", "NACCWRI1", "NACCWRI2", "NACCWRI3", "NACCBNKF", "NPBNKB", 
"NACCFORM", "NACCPARA", "NACCCSFP", "NPBNKF", "NPFAUT", "NPFAUT1", 
"NPFAUT2", "NPFAUT3", "NPFAUT4", "NACCDAGE", "NACCINT", "NPNIT", 
"NPCERAD", "NPADRDA", "NPOCRIT", "NPVOTH", "NPLEWYCS", "NPGENE", 
"NPFHSPEC", "NPTAUHAP", "NPPRNP", "NPCHROM", "NPPNORM", "NPCNORM", 
"NPPADP", "NPCADP", "NPPAD", "NPCAD", "NPPLEWY", "NPCLEWY", "NPPVASC", 
"NPCVASC", "NPPFTLD", "NPCFTLD", "NPPHIPP", "NPCHIPP", "NPPPRION", 
"NPCPRION", "NPPOTH1", "NPCOTH1", "NPOTH1X", "NPPOTH2", "NPCOTH2", 
"NPOTH2X", "NPPOTH3", "NPCOTH3", "NPOTH3X", "NPARTAG", "NPATGSEV", 
"NPATGAMY", "NPATGAM1", "NPATGAM2", "NPATGAM3", "NPATGAM4", "NPATGAM5", 
"NPATGFRN", "NPATGFR1", "NPATGFR2", "NPATGFR3", "NPATGFR4")


umap_df_rank_withMeta_1_path <- umap_df_rank_withMeta_1 %>% 
  left_join(nacc_path_cols_keep, by = c("NACCID", "NACCVNUM"))

# write this as a .csv file for Shaz to work on
# write.csv(umap_df_rank_withMeta_1_path, "C:/Users/sr06/Documents/NACCProject/Umap_with_meta_Path.csv")

# Subset those with Path data
umap_df_rank_withMeta_1_pathsub <- umap_df_rank_withMeta_1_path %>%
  group_by(ID) %>%
  filter(SurvivalStatus == "Deceased")

# lets categorise these pathologies

# Please note, here and below, when the value for a specific pathology is 0, I had previously coded it as "not that pathology" (e.g., NPADNC == 0 ~ "Not AD") as per the data dictionary. While this made the story clear, I had to then create a new DF where I had to then rewrite these not strings as NAs. Instead, I'm clearly specifying them as NA here itself to save coding time and computational space

umap_df_rank_withMeta_1_pathsub_2 <- umap_df_rank_withMeta_1_pathsub %>%
  group_by(ID) %>%
  # first, there are some vars from the old NACC forms which clearly outline who has what pathology as primary or contributing. I will start with this as this is the best indicator of the main disease in each person.
  mutate(PrimPath = case_when(NPPAD == "1" ~ "AD",
                                 NPPLEWY == "1" ~ "LBD",
                                 NPPVASC == "1" ~ "CVD",
                                 NPPFTLD == "1" ~ "FTLD",
                                 NPPHIPP == "1" ~ "Hipp. Scl.",
                                 NPPPRION == "1" ~ "Prion-assoc.",
                                 NPPADP == "1" ~ "AD, insuff for Dx.",
                                 TRUE ~ NA_character_)) %>%
  mutate(ContribPath = case_when(NPCAD == "1" ~ "AD",
                                 NPCLEWY == "1" ~ "LBD",
                                 NPCVASC == "1" ~ "CVD",
                                 NPCFTLD == "1" ~ "FTLD",
                                 NPCHIPP == "1" ~ "Hipp. Scl.",
                                 NPCPRION == "1" ~ "Prion-assoc.",
                                 NPCADP == "1" ~ "AD, insuff for Dx.",
                                 TRUE ~ NA_character_)) %>%
  rowwise() %>%
  # now, for ADNC, indicate whether they have low, medium or high ADNC
  mutate(Path_ADNC = case_when(NPADNC == 1 ~ "Low ADNC",
                               NPADNC == 2 ~ "Intermediate ADNC",
                               NPADNC == 3 ~ "High ADNC",
                               NPADNC == 0 ~ NA_character_,
                               TRUE ~ NA_character_
                               )) %>%
  # for CAA - amyloid angiopathy
  mutate(Path_CAA = case_when(NACCAMY == 0 ~ NA_character_,
                              (NACCAMY > 0 & NACCAMY < 8) ~ "CAA",
                              TRUE ~ NA_character_)) %>%
  # for non-amyloid angiopathies
  mutate(Path_NonAmyAngiop = case_when(NPOANG == 1 ~ "Non-Amyloid Angiopathy",
                                       TRUE ~ NA_character_)) %>%
    # for vascular changes
  mutate(Path_VascularChanges = case_when(
                          (NPLINF == "1" |
                           NPLAC == "1" | 
                           NPINF == "1" |  
                           NACCINF == "1" | 
                           NPMICRO == "1" |
                           NACCMICR == "1" |
                           NPOLD == "1" | 
                          (NACCARTE > "0" & NACCARTE < "8") |
                          (NPWMR > "0" & NPWMR < "8") | 
                           NPPATH == "1" | 
                           NACCNEC == "1" |
                           NPPATH11 == "1" |
                           NPPATHO == "1" |
                           NPART == "1") ~ "Vascular changes",
                          TRUE ~ NA_character_)) %>%
  # for acute injury
  mutate(Path_AcuteInjury = case_when(
                           (NPHEM == "1" |
                           NPOLDD == "1" |
                           NACCHEM == "1" | 
                           NPPATH2 == "1" |
                           NPPATH3 == "1" |
                           NPPATH4 == "1" | 
                           NPPATH5 == "1" | 
                           NPPATH6 == "1") ~ "Acute Injury",
                         TRUE ~ NA_character_)) %>%
  # For lewy body disease
  mutate(Path_LewyBody = case_when((NACCLEWY > "0" & NACCLEWY < "8") | 
                                (NPLBOD > "0" & NPLBOD < "8")  ~ "LBD",
                              NACCLEWY == 0 | NPLBOD == 0 ~ NA_character_,
                              TRUE ~ NA_character_)) %>%
  # for PSP
  mutate(Path_PSP = case_when(NACCPROG == "1"  ~  "PSP",
                         NACCPROG == "0"  ~ NA_character_,
                         TRUE ~ NA_character_)) %>%
  # for CBD
  mutate(Path_CBD = case_when(NACCCBD == "1"  ~  "CBD",
                         NACCCBD == "0"  ~ NA_character_,
                         TRUE ~ NA_character_)) %>%
  # for all other FTLDs
  #Starting with if identified as a tauopathy combining variables NPFTDTAU, NPFTDT2 (3R tau), NPFTDT6 (4R tau), #"Frontemporal dementia and parkinsonism with tau-positive or argyrophilic inclusions", #argyrophilic grains, tangle dominant disease, "other 3R + 4R tauopathy", and tau-other:
  mutate(Path_FTD = case_when((NPFTDTAU == "1" | 
                                 NPFTDT2 == "1" | 
                                 NPFTDT6 == "1" |
                                 NPFRONT == "1"|
                                 NPFTDT5 == "1" | 
                                 NPFTDT9 == "1" |
                                 NPFTDT10 == "1" |
                                 NPTAU == "1")  ~ "FTLD-Tau",
                              # Pick's disease
                              NACCPICK == "1" ~ "Pick",
                              # FTD with ubiquitin-positive (tau-negative) inclusions: this is combining answers "1" (FTD with motor neuron disease)
#and "2" (FTD without motor neuron disease)
                              (NPFTD > "0" & NPFTD < "3")  ~ "FTLD-Ubiq",
(NPFTDTDP == "1" | NPTDPA == "1" |NPTDPB == "1" |NPTDPC == "1" |
                              NPTDPD == "1" |NPTDPE == "1")  ~ "FTLD-TDP-43")) %>%
  mutate(Path_FTLD_Other = case_when((NPOFTD == "1" | NPOFTD5 == "1")  ~ "FTLD-Other",
                           TRUE ~ NA_character_)) %>%
#6 Other Path: Lumping all under one variable "otherpath" including Pigment-spheroid degeneration/NBIA, 
# white matter disease, and neoplasm
  mutate(Path_Other = case_when((NPPDXA == "1" | NPPDXG == "1" | NPPDXL == "1"| NACCOTHP == "1")  ~ "Other",
                               TRUE ~ NA_character_))
  
# Great, now a lot of people don't seem to have any values stored for these pathologies. So what I will do is see if they have a value across any of the path columns we have computed. If they have a value of 0 across these, it means they have no reported values for AD, FTLD, Vasc, DLB etc so we can safely remove them.
umap_df_rank_withMeta_1_pathsub_3 <- umap_df_rank_withMeta_1_pathsub_2 %>%
  # bring these path columns to the front
  group_by(ID) %>%
  rowwise() %>%
  # find the rows that have NAs for all of these values
  mutate(NoPath = sum(is.na(c_across(all_of(c("PrimPath", 
                                              "ContribPath", 
                                              "Path_ADNC", 
                                              "Path_CAA", 
                                              "Path_NonAmyAngiop",
                                              "Path_LewyBody", 
                                              "Path_PSP", 
                                              "Path_CBD", 
                                              "Path_FTD", 
                                              "Path_FTLD_Other",
                                              "Path_Other")))))) %>%
  # remove all rows where they have NA == 11 for NoPath because these have no pathology info for all 11 path columns. Note that I have excluded Vascular changes and acute injury from this list.
  filter(NoPath < 11)

# check how many cases have we dropped between these?
(nrow(umap_df_rank_withMeta_1_pathsub_2) - nrow(umap_df_rank_withMeta_1_pathsub_3))/3
n_distinct(umap_df_rank_withMeta_1_pathsub_3$ID)

# we have dropped 70 cases. This leaves 139 cases in the new df. 139 + 70 from above is 209 which means the df has not been split in a weird way as 209 people have deceased status overall. This mathces with the number of people wiht path info as reported in the manuscript.

# tally the number of people and their Dx with pathological data
umap_df_rank_withMeta_1_pathsub_3 %>%
  group_by(ID) %>%
  filter(Visit == 1) %>%
  ungroup() %>%
  group_by(Dx.x) %>%
  count()

# now we reorganise the path columns again to merge the info from the different columns all into one
test <- umap_df_rank_withMeta_1_pathsub_3 %>%
  dplyr::select(275:287) %>%
  group_by(ID) 
# this is going to be a bit tasking to code out because of the so many if and else statements so I'm going to write this into a CSV file and do it manually and then bring it back into R
write.csv(test, "test.csv")

# I've now copied and pasted these in the new data frame and now I will read and left-join it with the old DF

test <- read.csv("NACC_Path_postPathologistMeeting.csv", header = T, sep = ",")

# keep only ID and the prim path and cooccurring path vars
test1 <- test %>% 
  dplyr::select(ID, Visit, Primary, Cooccurring, CAABin, NonAmyAngiopBin, VascularChangesBin, AcuteInjuryBin)

# turn Visit into character

test1$Visit <- as.character(test1$Visit)

# in the original dataset, subset it to keep only the IDs that are in test1 (as a number of IDs have been removed because they don't have path data) and left join test1
umap_df_rank_withMeta_1_pathsub_4 <- umap_df_rank_withMeta_1_pathsub_3 %>%  group_by(ID) %>% 
  filter(ID %in% test1$ID) 

umap_df_rank_withMeta_1_pathsub_5 <- umap_df_rank_withMeta_1_pathsub_4 %>%
  left_join(test1, by = c("ID", "Visit"))
  

# Plot the distribution of these pathologies by clinical Dx.

# create a new df
umap_df_rank_withMeta_1_pathsub_6 <- umap_df_rank_withMeta_1_pathsub_5 %>% 
  # remove the original NACC path cols and keep only the new cols with the UMAP and PCA stuff..
  dplyr::select(1:22, 26:66, 277:294)

# now lets fit a linear model to the PC specific changes per pathology
g <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(NACCID) %>%
  dplyr::select(NACCID, Visit, Primary, RC1:RC6) %>%
  pivot_longer(!c(NACCID, Visit, Primary), 
               names_to = "Component",
               values_to = "Score") %>%
  filter(Component == "RC2")

mod2 <- lm(Score ~ Visit*Primary, data = g)
summary(mod2)


mod3 <- lmer(Score ~  Visit*Primary + (1 + Visit|NACCID), data = g)

# extract coefficients
coefs <- data.frame(coef(summary(mod1)))

# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs

# tally the number of path Dx occurrences by clin Dx and create a stacked bar chart
umap_df_rank_withMeta_1_pathsub_6 %>%
  filter(Visit == 1) %>%
  group_by(Dx.x, Primary) %>%
  count(Primary) %>%
  ggplot(aes(x = Dx.x, y = n, fill = Primary, label = n)) + 
  geom_bar(stat = "identity", position = "stack") +
  geom_text(size = 5, position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(type = "seq", palette = "Set3") +#(values = path.colours) +
  xlab("Group") +
  ylab("Count") + 
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12))

# tally the number of path Dx occurrences by clin Dx and create a faceted bar chart
fig7pa <- umap_df_rank_withMeta_1_pathsub_6 %>%
  filter(Visit == 1) %>%
  group_by(Dx.x, Primary) %>%
  summarise(n = n()) %>%
  ggplot(aes(x = Primary, y = n)) + 
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = n), position = position_dodge(width = 0.9), vjust = -0.25) +
  facet_wrap(~Dx.x, 
             scales = "free",
             ncol=5) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  scale_y_continuous(limits = c(0, 17)) +
  xlab("Primary pathology") +
  ylab("Count") +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
        strip.text = element_text(size = 12))
# saved as 1750 * 500


# tally the count with CAA, Non-amyloid angiopathy, vascular and acute injury changes by Dx
freqcount_vasc <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(NACCID) %>%
  filter(Visit == 1) %>%
  dplyr::select(NACCID, Dx.x, Primary, CAABin, NonAmyAngiopBin, VascularChangesBin, AcuteInjuryBin) %>%
 # mutate_all(na_if, "") %>%
  mutate(CAABin = case_when(
    CAABin == "Yes" ~ "CAA",
    TRUE ~ NA_character_)) %>%
  mutate(NonAmyAngiopBin = case_when(
    NonAmyAngiopBin == "Yes" ~ "NonAmylAngiopathy",
    TRUE ~ NA_character_)) %>%
  mutate(VascularChangesBin = case_when(
    VascularChangesBin == "Yes" ~ "VascularChanges",
    TRUE ~ NA_character_)) %>%
  mutate(AcuteInjuryBin = case_when(
    AcuteInjuryBin == "Yes" ~ "AcuteInjury",
    TRUE ~ NA_character_)) 
  
freqcount_vasc$Sum =  rowSums(is.na(freqcount_vasc))

freqcount_vasc1 <- freqcount_vasc %>%
  filter(Sum < 4) %>%
  unite(Vascular, CAABin, NonAmyAngiopBin, VascularChangesBin, AcuteInjuryBin, sep = "+", na.rm=T) %>%
  group_by(Primary, Vascular) %>%
  summarise(n=n())

# plot this as a stacked bar graph
freqcount_vasc1 %>%
  ggplot(aes(x = Primary, y = n, fill = Vascular, label = n)) + 
  geom_bar(stat = "identity", position = "stack") +
  geom_text(size = 5, position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(type = "seq", palette = "Set2") +#(values = path.colours) +
  xlab("Group") +
  ylab("Count") + 
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12)) +
  theme(legend.position = c(0.8, 0.8),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black"))
# save as 1200*550


# Plot the path data in the UMAP space

fig7pb <- umap_df_rank_withMeta_1_pathsub_6 %>% 
  ggplot(aes(x = V1, y = V2, colour = Dx.x)) + 
  geom_point(size = 3) + 
  facet_wrap(~Primary) +
   scale_colour_manual(values = group.colours) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.background = element_rect(colour = "black"),
        strip.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  #ggtitle("Clinico-pathological mapping in UMAP space") +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  labs(color='Group') 
# saved as 1200 * 850

# plot path bar chart and umap in one figure
cowplot::plot_grid(fig7pa,
                   fig7pb,
                   labels="AUTO",
                   ncol = 1)
# saved as 1200*1200


# create a subset of this data with stable vs. unstable Dx
# Dx change
umap_df_rank_withMeta_1_pathsub_new_dxchange <- umap_df_rank_withMeta_1_pathsub_new_4  %>%
# group by ID
  group_by(ID) %>%
  # keep only those IDs from nacc_dxchange_ID df
  filter(ID %in% unique(nacc_dxchange_ID$ID)) %>%
  # make a new grouping to delineate what their first Dx was
  mutate(FirstDx = case_when(Visit == 1 & Dx.x == "AD" ~ "FirstDx-AD",
                             Visit == 1 & Dx.x == "bvFTD" ~ "FirstDx-bvFTD",
                             Visit == 1 & Dx.x == "FTLD-motor" ~ "FirstDx-FTLD-motor",
                             Visit == 1 & Dx.x == "FTLD-NOS" ~ "FirstDx-FTLD-NOS",
                             Visit == 1 & Dx.x == "PPA" ~ "FirstDx-PPA")) %>%
  # fill this down into Group membership
  fill(FirstDx, .direction = "down")  %>%
  # keep on Dx, UMAP, PCA and newly coded path vars
  dplyr::select(1:2, 4:6, 15:20, 75, 64:74, 77) %>%
  rename(Group = Dx.x) %>%
  pivot_longer(!c(1:12, 24),
               names_to = "Pathology")

# plot path UMAP space for Dx change

umap_df_rank_withMeta_1_pathsub_new_dxchange %>%
  # keep only path where value is 1
  filter(value == 1) %>%
  ggplot(aes(x = V1, y = V2, colour = FirstDx)) +
  geom_point(size = 3) +
  facet_wrap(~Pathology, ncol = 5) +
  scale_colour_manual(values = dx_change.group.colours) +
    theme_bw() +
  theme(legend.position = c(0.9,0.15),
        legend.background = element_rect(colour = "black"),
        strip.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Clinico-pathological mapping in UMAP space for patients with Dx change") +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
# saved as 1k*700


# now also build a df with those who have stable dx
umap_df_rank_withMeta_1_pathsub_new_4 %>%
# group by ID
  group_by(ID) %>%
  # remove those IDs from nacc_dxchange_ID df
  anti_join(nacc_dxchange_ID, by = "ID") %>%
  dplyr::select(1:2, 4:6, 15:20, 75, 64:74) %>%
  rename(Group = Dx.x) %>%
  ggplot(aes(x = V1, y = V2, colour = Group)) +
  geom_point(size = 3) +
  facet_wrap(~PrimPath) +
  scale_colour_manual(values = group.colours) +
    theme_bw() +
  theme(legend.position = c(0.75,0.07),
        legend.background = element_rect(colour = "black"),
        strip.text = element_text(size = 12)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Clinico-pathological mapping in UMAP space for patients with stable Dx") +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
# saved as 1k*750


# plot box plots of PC scores by pathology performance
umap_df_rank_withMeta_1_pathsub_new_4 %>%
  group_by(ID) %>%
  dplyr::select(1:2, 4:6, 15:20, 75) %>%
  rename(Group = Dx.x,
         "Functional status (PC1)" = RC1,
         "Apathy/Impulsivity (PC2)" = RC2,
         "Motor function (PC3)" = RC3,
         "Psychosis (PC4)" = RC4,
         "Affective (PC5)" = RC5,
         "Depression (PC6)" = RC6) %>%
  pivot_longer(!c(1:5, 12),
               names_to = "PC") %>%
  mutate(PC = fct_relevel(PC, c("Functional status (PC1)",
         "Apathy/Impulsivity (PC2)",
         "Motor function (PC3)",
         "Psychosis (PC4)",
         "Affective (PC5)",
         "Depression (PC6)"))) %>%
  ggplot(aes(x = PrimPath, y = value)) +
  geom_boxplot(alpha = 0.9) +
  geom_point(size=3, 
             position=position_jitter(width=0.1), 
             alpha=0.1) +
  facet_wrap(~PC, scales = "free") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 12),
         axis.text.x = element_text(size = 12),
         axis.title.x = element_text(size = 12),
         axis.title.y = element_text(size = 12),
         strip.text = element_text(size = 12)) +
    theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  ylim(-6, 3) +
  xlab("Primary Pathology") +
  ylab ("Score")
# saved as 1200*800

```

#### Average nearest neighbour analysis on pathology data

```{r}

# create dataset stratified by pathologies and run ANN

### AD pathology
temp_path_ad <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "AD") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_AD <- st_as_sf(temp_path_ad, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_AD.sp <- as(spatial.temp_path_AD, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_AD.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_AD1 <- as_Spatial(spatial.temp_path_AD)
ppp.spatial.temp_path_AD <- as.ppp(spatial.temp_path_AD1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_AD.p <- mean(nndist(ppp.spatial.temp_path_AD))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_AD.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_AD$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_AD.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_AD.r > ann_path_AD.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_AD.r, main= "AD pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_AD.p, ann_path_AD.r), xlab = "ANN value")
abline(v=ann_path_AD.p, col="black", lwd = 4)
# saved as 500*300


### CBD pathology
temp_path_cbd <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "CBD") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_CBD <- st_as_sf(temp_path_cbd, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_CBD.sp <- as(spatial.temp_path_CBD, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_CBD.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_CBD1 <- as_Spatial(spatial.temp_path_CBD)
ppp.spatial.temp_path_CBD <- as.ppp(spatial.temp_path_CBD1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_CBD.p <- mean(nndist(ppp.spatial.temp_path_CBD))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_CBD.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_CBD$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_CBD.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_CBD.r > ann_path_CBD.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_CBD.r, main= "CBD pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_CBD.p, ann_path_CBD.r), xlab = "ANN value")
abline(v=ann_path_CBD.p, col="black", lwd = 4)
# saved as 500*300


### CVD pathology
temp_path_cvd <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "CVD") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_cvd <- st_as_sf(temp_path_cvd, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_cvd.sp <- as(spatial.temp_path_cvd, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_cvd.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_cvd1 <- as_Spatial(spatial.temp_path_cvd)
ppp.spatial.temp_path_cvd <- as.ppp(spatial.temp_path_cvd1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_cvd.p <- mean(nndist(ppp.spatial.temp_path_cvd))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_cvd.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_cvd$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_cvd.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_cvd.r > ann_path_cvd.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_cvd.r, main= "CVD pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_cvd.p, ann_path_cvd.r), xlab = "ANN value")
abline(v=ann_path_cvd.p, col="black", lwd = 4)
# saved as 500*300


### FTLD-Other
temp_path_ftldoth <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "FTLD-Other") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_ftldoth <- st_as_sf(temp_path_ftldoth, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_ftldoth.sp <- as(spatial.temp_path_ftldoth, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_ftldoth.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_ftldoth1 <- as_Spatial(spatial.temp_path_ftldoth)
ppp.spatial.temp_path_ftldoth <- as.ppp(spatial.temp_path_ftldoth1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_ftldoth.p <- mean(nndist(ppp.spatial.temp_path_ftldoth))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_ftldoth.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_ftldoth$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_ftldoth.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_ftldoth.r > ann_path_ftldoth.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_ftldoth.r, main= "FTLD-Other pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_ftldoth.p, ann_path_ftldoth.r), xlab = "ANN value")
abline(v=ann_path_ftldoth.p, col="black", lwd = 4)
# saved as 500*300


### FTLD-Pick
temp_path_ftldpick <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "FTLD-Pick") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_ftldpick <- st_as_sf(temp_path_ftldpick, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_ftldpick.sp <- as(spatial.temp_path_ftldpick, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_ftldpick.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_ftldpick1 <- as_Spatial(spatial.temp_path_ftldpick)
ppp.spatial.temp_path_ftldpick <- as.ppp(spatial.temp_path_ftldpick1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_ftldpick.p <- mean(nndist(ppp.spatial.temp_path_ftldpick))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_ftldpick.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_ftldpick$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_ftldpick.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_ftldpick.r > ann_path_ftldpick.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_ftldpick.r, main= "FTLD-Pick pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_ftldpick.p, ann_path_ftldpick.r), xlab = "ANN value")
abline(v=ann_path_ftldpick.p, col="black", lwd = 4)
# saved as 500*300


### FTLD-Tau
temp_path_ftldtau <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "FTLD-Tau") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_ftldtau <- st_as_sf(temp_path_ftldtau, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_ftldtau.sp <- as(spatial.temp_path_ftldtau, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_ftldtau.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_ftldtau1 <- as_Spatial(spatial.temp_path_ftldtau)
ppp.spatial.temp_path_ftldtau <- as.ppp(spatial.temp_path_ftldtau1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_ftldtau.p <- mean(nndist(ppp.spatial.temp_path_ftldtau))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_ftldtau.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_ftldtau$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_ftldtau.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_ftldtau.r > ann_path_ftldtau.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_ftldtau.r, main= "FTLD-Tau pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_ftldtau.p, ann_path_ftldtau.r), xlab = "ANN value")
abline(v=ann_path_ftldtau.p, col="black", lwd = 4)
# saved as 500*300


### FTLD-TDP43
temp_path_ftldtdp <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "FTLD-TDP-43") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_ftldtdp <- st_as_sf(temp_path_ftldtdp, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_ftldtdp.sp <- as(spatial.temp_path_ftldtdp, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_ftldtdp.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_ftldtdp1 <- as_Spatial(spatial.temp_path_ftldtdp)
ppp.spatial.temp_path_ftldtdp <- as.ppp(spatial.temp_path_ftldtdp1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_ftldtdp.p <- mean(nndist(ppp.spatial.temp_path_ftldtdp))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_ftldtdp.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_ftldtdp$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_ftldtdp.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_ftldtdp.r > ann_path_ftldtdp.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_ftldtdp.r, main= "FTLD-TDP-43 pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_ftldtdp.p, ann_path_ftldtdp.r), xlab = "ANN value")
abline(v=ann_path_ftldtdp.p, col="black", lwd = 4)
# saved as 500*300


### FTL Ubiq
temp_path_ftldubiq <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "FTLD-Ubiq") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_ftldubiq <- st_as_sf(temp_path_ftldubiq, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_ftldubiq.sp <- as(spatial.temp_path_ftldubiq, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_ftldubiq.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_ftldubiq1 <- as_Spatial(spatial.temp_path_ftldubiq)
ppp.spatial.temp_path_ftldubiq <- as.ppp(spatial.temp_path_ftldubiq1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_ftldubiq.p <- mean(nndist(ppp.spatial.temp_path_ftldubiq))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_ftldubiq.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_ftldubiq$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_ftldubiq.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_ftldubiq.r > ann_path_ftldubiq.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_ftldubiq.r, main= "FTLD-Ubiq pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_ftldubiq.p, ann_path_ftldubiq.r), xlab = "ANN value")
abline(v=ann_path_ftldubiq.p, col="black", lwd = 4)
# saved as 500*300


### LBD
temp_path_lbd <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "LBD") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_lbd <- st_as_sf(temp_path_lbd, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_lbd.sp <- as(spatial.temp_path_lbd, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_lbd.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_lbd1 <- as_Spatial(spatial.temp_path_lbd)
ppp.spatial.temp_path_lbd <- as.ppp(spatial.temp_path_lbd1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_lbd.p <- mean(nndist(ppp.spatial.temp_path_lbd))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_lbd.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_lbd$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_lbd.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_lbd.r > ann_path_lbd.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_lbd.r, main= "LBD pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_lbd.p, ann_path_lbd.r), xlab = "ANN value")
abline(v=ann_path_lbd.p, col="black", lwd = 4)
# saved as 500*300


### Other pathologies
temp_path_other <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "Other") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_other <- st_as_sf(temp_path_other, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_other.sp <- as(spatial.temp_path_other, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_other.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_other1 <- as_Spatial(spatial.temp_path_other)
ppp.spatial.temp_path_other <- as.ppp(spatial.temp_path_other1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_other.p <- mean(nndist(ppp.spatial.temp_path_other))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_other.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_other$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_other.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_other.r > ann_path_other.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_other.r, main= "Other pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_other.p, ann_path_other.r), xlab = "ANN value")
abline(v=ann_path_other.p, col="black", lwd = 4)
# saved as 500*300


### PSP
temp_path_PSP <- umap_df_rank_withMeta_1_pathsub_6 %>%
  group_by(ID) %>%
  filter(Primary == "PSP") %>%
  dplyr::select(V1, V2, ID, Primary, Visit) %>%
  dplyr::rename(x = V1, y = V2)

# I conduct the Average nearest neighbour analysis and compare it to a random model for each group. This is done using the code below.

spatial.temp_path_PSP <- st_as_sf(temp_path_PSP, 
                 coords = c("x", "y")) # I take the coordinates of the umap points 

spatial.temp_path_PSP.sp <- as(spatial.temp_path_PSP, "Spatial")  # turn it into a spatial df
class(spatial.temp_path_PSP.sp)

# now I convert this into a "ppp" object (there are two steps to do this- see above).
spatial.temp_path_PSP1 <- as_Spatial(spatial.temp_path_PSP)
ppp.spatial.temp_path_PSP <- as.ppp(spatial.temp_path_PSP1) # finally convert this into a ppp object


# now I do the average nearest neighbour analysis and build a "null model" by plotting points randomly within the UMAP polygon convex hull. I can then compare our model to see whether it clusters in relation to this null model (see these codes for more:  https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#average-nearest-neighbor-analysis) 

# first I conduct an ann on our data (ANN happening in euclidean space)
ann_path_PSP.p <- mean(nndist(ppp.spatial.temp_path_PSP))

# now i create a null distribution
n     <- 1000               # Number of simulations
ann_path_PSP.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=ppp.spatial.temp_path_PSP$n, win = sps)  # Generate random point locations within the constraint of the convex hull
  ann_path_PSP.r[i] <- mean(nndist(rand.p))  # Tally the ANN values of the random distribtuion
}

# compute a pvalue for the difference between the null model and our data
# for this, first find the number of simulated ANN values that are larger than our observed data
n.greater <- sum(ann_path_PSP.r > ann_path_PSP.p)
# now for the p value, end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value.
p <- min(n.greater + 1, n + 1 - n.greater) / (n +1)
p

# now make a historgram of the null distrubuion
hist(ann_path_PSP.r, main= "PSP pathology", las=1, breaks=40, col="grey", xlim=range(ann_path_PSP.p, ann_path_PSP.r), xlab = "ANN value")
abline(v=ann_path_PSP.p, col="black", lwd = 4)
# saved as 500*300
```

#### Associations with first clinical symptom reported

NACC has two vars: NACCCOGF and NACCBEHF which correspond to clinical ratings of the predominant symptom first recognised as a decline in the subject's cognition or behaviour. It will be worth exploring how these map on to the UMAP space. I have left-joined this in the path dataset so lets code it out and analyse the patterns.

```{r}

# NOTE: if B9CHG = 1 (No meaningful changes) then both these vairables will not be filled.

# For COGF, 0 = no impairment in cogn, 1 = memory, 2, orientation, 3, exec (judgment, planning, problem solving), 4 = language, 5 = visuospatial fn, 6 = attention/orient, 7 = fluctuating cogn, 8 = other, 99 = unknown (NA)

# for BEHF, 0 = no behav symptoms, 1 = apathy/withdrawl, 2 = depressed mood, 3 = psychosis, 4 = disinhibition, 5 = irritability, 6 = agitation, 7 personality change, 8 REM sleep behaviour disorder, 9 = anxiety, 10 = other, 99 = uknown (NA)

umap_df_rank_withMeta_1_path_1 <- umap_df_rank_withMeta_1_path %>%
  group_by(ID) %>%
  mutate(ClinBehav = case_when(NACCBEHF == 0 ~ "No behav. symptoms",
                               NACCBEHF == 1 ~ "Apathy/Withdrawal",
                               NACCBEHF == 2 ~ "Depressed Mood",
                               NACCBEHF == 3 ~ "Psychosis",
                               NACCBEHF == 4 ~ "Disinhibition",
                               NACCBEHF == 5 ~ "Irritability",
                               NACCBEHF == 6 ~ "Agitation",
                               NACCBEHF == 7 ~ "Personality change",
                               NACCBEHF == 8 ~ "REM sleep behav. disorder",
                               NACCBEHF == 9 ~ "Anxiety",
                               NACCBEHF == 10 ~ "Other",
                               TRUE ~ "No behav. symptoms"
                               )) %>%
  mutate(ClinCog = case_when(NACCCOGF == 0 ~ "No cogn. symptoms",
                             NACCCOGF == 1 ~ "Memory",
                             NACCCOGF == 2 ~ "Orientation",
                             NACCCOGF == 3 ~ "Executive fns.",
                             NACCCOGF == 4 ~ "Language",
                             NACCCOGF == 5 ~ "Visuospatial fns.",
                             NACCCOGF == 6 ~ "Attn./Concentration",
                             NACCCOGF == 7 ~ "Fluctuating cogn.",
                             NACCCOGF == 8 ~ "Other",
                             TRUE ~ "No cogn. symptoms"
                             ))

# map the UMAP but faceting by Clin Behav.

plotBEHF_umap <- umap_df_rank_withMeta_1_path_1 %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(colour = Dx.x), 
             size = 2,
             alpha = 0.5) +
  facet_wrap(~ClinBehav, ncol = 5) +
  theme_bw() +
  theme(legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.position = "bottom",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 10)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    scale_color_manual(values = group.colours) +
  labs(colour = "Group")
# saved as 1300*750


# I need to add a bar graph showing the numbers reporting these difficulties
temp1 <- umap_df_rank_withMeta_1_path_1 %>%
  group_by(Dx.x, Visit, ClinBehav) %>%
  summarise(n = n())

# plot this 
plotBEHF_count <- temp1 %>%
  ggplot(aes(fill = Dx.x, x = Visit, y = n, label = n)) +
  facet_wrap(~ClinBehav, ncol = 5) +
  geom_bar(position = "stack", stat = "identity") +   
  geom_text_repel(size = 3, position = position_stack(vjust = 0.5), direction = "x", max.overlaps = Inf) +
  theme_bw() +
  theme(legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.position = "bottom",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 10)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  scale_fill_manual(values = group.colours) +
  labs(fill = "Group") +
  ylab("Count") 
  #geom_text(size = 3, position = position_stack(vjust = 0.5))

# multiplot this
plot_grid(plotBEHF_umap,
          plotBEHF_count, ncol =1,
          labels = "AUTO")
# saved as 850*1200


# Repeat with COGF

plotCOGF_umap <- umap_df_rank_withMeta_1_path_1 %>%
  ggplot(aes(x = V1, 
             y = V2)) +
  geom_point(aes(colour = Dx.x), 
             size = 2,
             alpha = 0.5) +
  facet_wrap(~ClinCog, ncol = 4) +
  theme_bw() +
  theme(legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.position = "bottom",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 10)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
    scale_color_manual(values = group.colours) +
  labs(colour = "Group")
# saved as 1300*750


# I need to add a bar graph showing the numbers reporting these difficulties
temp2 <- umap_df_rank_withMeta_1_path_1 %>%
  group_by(Dx.x, Visit, ClinCog) %>%
  summarise(n = n())

# plot this 
plotCOGF_count <- temp2 %>%
  ggplot(aes(fill = Dx.x, x = Visit, y = n, label = n)) +
  facet_wrap(~ClinCog, ncol = 4) +
  geom_bar(position = "stack", stat = "identity") +   
  geom_text_repel(size = 3, position = position_stack(vjust = 0.5), direction = "x", max.overlaps = Inf) +
  theme_bw() +
  theme(legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.position = "bottom",
        legend.background = element_rect(colour = "black"),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        strip.text = element_text(size = 10)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text= element_text(size=15), axis.title = element_text(size=15)) +
  scale_fill_manual(values = group.colours) +
  labs(fill = "Group") +
  ylab("Count") 
  #geom_text(size = 3, position = position_stack(vjust = 0.5))

# multiplot this
plot_grid(plotCOGF_umap,
          plotCOGF_count, ncol=1,
          labels = "AUTO")
# saved as 850*1200

```

# Extra code (unused - ignore)

In the first step, we can safely substitute all -4s, 999s and 888s with NA.

```{r, eval=FALSE}

# # First, I will create a dataset with only clinical and neuropsychological variables. We no more require data on diagnosis etc., as all of those have been calculated already.
# 
# nacc_8 <- nacc_7 %>%
#   select(1:48, # demographics and other clinical details 
#          87:149, # CDR, NPI and GDS
#          150:196, # clinical assessment of neurological fns. (Categorical)
#          197:210, # clinical assessment of cogn. fns. (Categorical)
#          212:253, # clinical assessment of cogn./neurol. fns. (Categ. & continuous)
#          256:389 # cognitive/neuropsych assessment
#          )
```

***Note:*** If "No meaningful changes" (B9CHG = 1) was selected, the clinician did not complete the rest of the form. (p202- DD-UDS)

```{r, eval=FALSE}

# # Now, I'll write some code to keep replacing NAs, according to the DD-UDS for each variable. For this, the best way is to stagger a bunch of code to first pick out the common values across the entire dataframe and then the unique ones, based on each variables unique values.
# 
# nacc_9 <- nacc_8 %>% 
#   # first I replace -4, 888, 8888, 999, 9999 from the entire dataset with NA
#   mutate(across(everything(), ~replace(., . %in% c(-4, 888, 8888, 999, 9999), NA))) %>%
#   # for the genetic variables, I replace 9s with NA
#   mutate(across(35:48, ~replace(., . %in% 9, NA))) %>%
#   # for the CBI/NPI/GDS variables, I replace 9s with NA
#   mutate(across(61:100, ~replace(., . %in% c(9), NA))) %>%
#   # for total GDS score, substitute 88 with NA
#   mutate(NACCGDS = replace(NACCGDS, NACCGDS == 88, NA)) %>%
#   # for NACC functional assessment scale, replace 9 (Unknown) and 8 (Not applicable) with NA
#   mutate(across(102:111, ~replace(., . %in% c(8, 9), NA))) %>%
#   # for clinical examination columns, replace 8 and 9 with NA
#   mutate(across(112:158, ~replace(., . %in% c(8, 9), NA))) %>%
#   # for clinical judgment of symptoms, replace 8 (could not be assessed) and 9 with NA
#   mutate(across(159:172,  ~replace(., . %in% c(8, 9), NA))) %>%
#   # for NACC- reported clinical variables, replace 99 (unknown) with NA
#   mutate(across(173:175, ~replace(., . %in% 99, NA))) %>%
#   # for clinical reports of mood, replace 9 with NA
#   mutate(across(179:194, ~replace(., . %in% 9, NA))) %>%
#   # for first behavioural symptom and mode of onset of behv sx, replace 99 with NA
#   mutate(NACCBEHF = replace(NACCBEHF, NACCBEHF == 99, NA)) %>%
#   mutate(BEMODE = replace(BEMODE, BEMODE == 99, NA)) %>%
#   # for motor and other clinical symptoms, replace 9 and 99 with NA
#   mutate(across(201:208, ~replace(., . %in% c(9, 99), NA))) %>%
#   mutate(across(210:214, ~replace(., . %in% c(8, 9, 99), NA))) %>%
#   # for MMSE and neuropsych tests, replace 88 with NA
#   mutate(across(228:243, ~replace(., . %in% c(88, 99), NA))) %>%
#   # for MOCA, replace 88 with NA
#   mutate(across(292:293, ~replace(., .%in% 88, NA))) %>%
#   # for MOCA and Rey, replace 88/99 with NA
#   mutate(across(312:325, ~replace(., . %in% c(88, 99), NA))) %>%
#   # for Rey, replace 88 with NA
#   mutate(across(332:347, ~replace(., . %in% 88, NA)))
```

Now that I have substituted most missing data codes with NA, there are a few more test-specific changes that need to be made. For example, for the NPI, BEApathy etc, a score of 8 or 9 means not applicable (i.e., as per DD-UDS, this is stored as symptom not reported) so we can safely convert this to 0. For the GDS, a score of 9 means did not answer so this can be turned into NA. For MMSE/Logical Memory/Trails/Cogstat and all other neuropsych test, there are a number of codes related to refusal to participate so we can code these as NA as well. ***Note: for these tests, check if total score is impacted if you recode refusal codes to NA. I will go over each test one by one.***

```{r, eval=FALSE}
# 
# nacc_10 <- nacc_9 %>%
#   # For NPI Del, I seem to have missed replacing score of 9 with NA
#   mutate(DEL = replace(DEL, DEL==9, NA)) %>%
#   # for NPI all variables, replace score of 8 (no symptom reported) with 0
#   mutate(across(61:84, ~replace(., . %in% 8, 0))) %>%
#   # for NACC Functional Assessment Scale, replace 8 (Not Applicable) and 9 (Unknown) with 0
#   mutate(across(102:111, ~replace(., . %in% c(8, 9), 0))) %>%
#   # For NACC Clinician Judgment of Symptoms - BE Apathy symptoms, replace 9 (unknown) with NA 
#   mutate(BEAPATHY = replace(BEAPATHY, BEAPATHY == 9, NA)) %>%
#   # for MOMOPARK and MOMOALS, replace 8 (no changes) with 0
#   mutate(MOMOPARK = replace(MOMOPARK, MOMOPARK == 8, 0)) %>%
#   mutate(MOMOALS = replace(MOMOALS, MOMOALS == 8, 0)) %>%
#   # for MMSE orientation and pentagon (Only available subscores), replace 95, 96, 97, 98 with NA 
#   mutate(across(221:224, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for MMSE total, replace also 88 (missing at least one MMSE item) with NA 
#   mutate(NACCMMSE = replace(NACCMMSE, NACCMMSE == 88, NA)) %>%
#   # for logical memory, Benson figure, digits span, animal and veg fluency, replace 88 (Unknown) and 95:98 with NA. Note that max scores for all these tests are under 88 so I won't be substituting any non-NA values
#   mutate(across(228:243, ~replace(., . %in% c(88, 95, 96, 97, 98), NA))) %>%
#   # For Trails, replace 995:998 as NA
#   mutate(across(244:249, ~replace(., . %in% c(995, 996, 997, 998), NA))) %>%
#   # for Trails A commission errors and correct lines, replace 95:99 with NA
#   mutate(across(245:246, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for Trails B commission errors and correct lines, replace 95:99 with NA
#   mutate(across(248:249, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for WAIS digit symbol, Boston, UDS fluency, replace 95:98 with NA
#   mutate(across(250:260, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for MOCA, Craft, number span, MINT, Rey replace 88, 95:98 with NA
#   mutate(across(263:325, ~replace(., . %in% c(88, 95, 96, 97, 98), NA))) %>%
#   # for MOCA B - blind score - corrected for education, replace 99 (item/whole test not administered with NA) 
#   mutate(NACCMOCB = replace(NACCMOCB, NACCMOCB == 99, NA)) %>%
#   # for Oral Trails, replace 888, 995:998 with NA
#   mutate(across(326:331, ~replace(., . %in% c(888, 995, 996, 997, 998), NA))) %>%
#   # for RAVLT and verbal naming test, replace 88, 95:98 with NA
#   mutate(across(332:336, ~replace(., . %in% c(88, 95, 96, 97, 98), NA)))
```

To make it easy for missing data culling and imputation, I will now create a version of this dataset with only the tests of interest. I will unselect many of the test meta-information and other qualitative variables (most of which are again, test meta-info).

```{r, eval=FALSE}
# 
# # There is a fast way to do this but it involves a long code. I can use dput to output the full names of all columns in a vector format, then copy that list from the console to this markdown file, delete the variables that I do not want to keep, save it as a vector and call that into dplyr::select. This is a faster way to manually delete the columns I don't want to keep, rather than typing out each column name.
# 
# nacc_10_allcols <- dput(names(nacc_10))
# 
# # Copying dput output into the markdown file and deleting ones that I don't want. I will write another line of code to show the columns that I deleted.
# 
# nacc_10_keeps <- c("NACCID", "NACCVNUM", "Group", "ClinDiagDementia", "ClinDiagMCI", "ClinDiagMCISubtype", "NACCADC", "NACCUDSD", "NACCDAYS", 
# "NACCFDYS", "NACCDIED", "NACCMOD", "NACCYOD", "BIRTHYR", 
# "SEX", "EDUC", "RACE", "PRIMLANG", "INDEPEND", "HANDED", 
# "NACCAGEB", "NACCAGE",  "NORMCOG", "NACCFAM", "NACCMOM", "NACCDAD", "NACCAM", "NACCAMS", "NACCFM", "NACCFMS", "NACCOM",  "NACCFADM", 
# "NACCFFTD", "MEMORY", "ORIENT", "JUDGMENT", "COMMUN", "HOMEHOBB", 
# "PERSCARE", "CDRSUM", "CDRGLOB", "COMPORT", "CDRLANG", "DEL", "DELSEV", "HALL", "HALLSEV", "AGIT", "AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", "ELATSEV", "APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", "MOT", "MOTSEV", 
# "NITE", "NITESEV", "APP", "APPSEV", "SATIS", "DROPACT", "EMPTY", "BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "NACCGDS", "BILLS", "TAXES", "SHOPPING", "GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", "TRAVEL", "FOCLDEF", "GAITDIS", "EYEMOVE", "PARKSIGN", "RESTTRL", "RESTTRR", "SLOWINGL", "SLOWINGR", "RIGIDL", "RIGIDR", "BRADY", "PARKGAIT", "POSTINST", "CVDSIGNS", "CORTDEF", "SIVDFIND", "CVDMOTL", "CVDMOTR", "CORTVISL", "CORTVISR", "SOMATL", "SOMATR", "POSTCORT", "PSPCBS", "EYEPSP", "DYSPSP", "AXIALPSP", "GAITPSP", "APRAXSP", "APRAXL", "APRAXR", "CORTSENL", "CORTSENR", "ATAXL", "ATAXR", "ALIENLML", "ALIENLMR", "DYSTONL", "DYSTONR", "MYOCLLT", "MYOCLRT", "ALSFIND", "GAITNPH", "OTHNEUR", "B9CHG", "DECSUB", "DECIN", "DECCLIN", "DECCLCOG", "COGMEM", "COGORI", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", "COGFLAGO", "COGOTHR", "NACCCOGF", "COGMODE", "DECAGE", "DECCLBE", "BEAPATHY",
# "BEDEP", "BEVHALL", "BEVWELL", "BEVHAGO", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEREMAGO", "BEANX", "BEOTHR", "NACCBEHF", "BEMODE", "BEAGE", "DECCLMOT", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW", "NACCMOTF", "MOMODE", "MOMOPARK", "PARKAGE", "MOMOALS", "ALSAGE", "MOAGE", "COURSE", "FRSTCHG", "MMSEORDA", "MMSEORLO", "PENTAGON", "NACCMMSE", "LOGIMEM", "MEMUNITS", "UDSBENTC", "UDSBENTD", "UDSBENRS", "DIGIF", "DIGIB", "ANIMALS", "VEG", "TRAILA", "TRAILARR",
# "TRAILALI", "TRAILB", "TRAILBRR", "TRAILBLI", "WAIS", "BOSTON", "UDSVERFC", "UDSVERFN", "UDSVERNF", "UDSVERLC", "UDSVERLR", "UDSVERLN",
# "UDSVERTN", "UDSVERTE", "UDSVERTI", "COGSTAT", "MOCATOTS", "MOCATRAI", "MOCACUBE", "MOCACLOC", "MOCACLON", "MOCACLOH", "MOCANAMI", "MOCAREGI", "MOCADIGI", "MOCALETT", "MOCASER7", "MOCAREPE", "MOCAFLUE", "MOCAABST", "MOCARECN", "MOCARECC", "MOCARECR", "MOCAORDT", "MOCAORMO", "MOCAORYR", "MOCAORDY", "MOCAORPL", "MOCAORCT", "NACCMOCA", "MOCBTOTS", "NACCMOCB", "CRAFTVRS", "CRAFTURS", "DIGFORCT", "DIGBACCT", "CRAFTDVR", "CRAFTDRE", "CRAFTDTI", "CRAFTCUE", "MINTTOTS", "MINTTOTW", "MINTSCNG", "MINTSCNC", "MINTPCNG", "MINTPCNC", "REY1REC", "REY1INT", "REY2REC", "REY2INT", "REY3REC", "REY3INT", "REY4REC", "REY4INT", "REY5REC", "REY5INT", "REY6REC", "REY6INT", "OTRAILA", "OTRLARR", "OTRLALI", "OTRAILB", "OTRLBRR", "OTRLBLI", "REYDREC", "REYDINT", "REYTCOR", "REYFPOS", "VNTTOTW", "VNTPCNC")
# 
# # Here are all the columns I have removed
# setdiff(nacc_10_allcols, nacc_10_keeps)
# 
# 
# # I will call save these selected columns in a new df and call it voi = variables of interest
# nacc_voi <- nacc_10 %>%
#   select(c(nacc_10_keeps))
```

missing data handling

```{r, eval=FALSE}


# 
#  
# 
# 
# 
# # first let us see how many times 95, 995, 97, 997 occur in all columns
# 
# 
# 
# 
# 
# 
# # make a new version so we can toy around with it
# nacc_12 <- nacc_11
# 
# # next, I will create a dataset version keeping only the variables that may need to be imputed. This significantly simplifies the imputation step
# nacc_12_varstoimp <- nacc_12 %>%
#   select(NACCID, NACCVNUM, Group, NACCAGE, EDUC, DEL, HALL, AGIT, DEPD, ANX, ELAT, APA, DISN, IRR, MOT, NITE, APP, 86:100, 114:157, 160:170, 179:182, 184:190, 192:193, 200, 202:204, 208, 215:336)
# 
# 
# 
# 
# nacc_12_bin_PPA_t1 <- nacc_12_varstoimp %>%
#   filter(str_detect(Group, "PPA")) %>%
#   filter(NACCVNUM==1) %>%
#   select(DEL, HALL, AGIT, DEPD, ANX, ELAT, APA, DISN, IRR, MOT, NITE, APP) %>%
# 
# 
# (86:100, 114:157, 160:170, 179:182, 184:190, 192:193, 200, 202:204, 208, MMSECOMP, MMSEHEAR, MMSEVIS, PENTAGON, UDSBENRS) %>%
#   select(-NACCMMSE)
# 
# temp <- mice(nacc_12_bin_PPA_t1, method = "logreg")
# 
# completeddata <- complete(temp)
# 
# 
# 
# 
# # Next, I need to make a list of all variables that have a binary scoring system. These will be fed into the imputation step for the GLM
# nacc_12_bin_colnames <- nacc_12_varstoimp %>%
#   select(DEL, HALL, AGIT, DEPD, ANX, ELAT, APA, DISN, IRR, MOT, NITE, APP, 86:100, 114:157, 160:170, 179:182, 184:190, 192:193, 200, 202:204, 208, MMSECOMP, MMSEHEAR, MMSEVIS, PENTAGON, UDSBENRS) %>%
#   colnames()
# 
# 
# 
# 
# 
# # nacc_12 <- nacc_11
# # 
# # 
# # 
#  g<-nacc_12 %>% 
#    select(221:336) %>% 
#    select_if(~any(max(.x = 1)))
# # 
# # 
# # 
#  nacc_12 %>%
#    select(221:336) %>%
#    select_if(function(col) is.numeric(col) & min(col) = 1 & max(col) = 1) %>%
#    colnames()
# # 
# # 
# # 
# # 
# # 
# # 
# # colnames(nacc_12)[221:336] <- paste("Test_", colnames(nacc_12[,c(221:336)]), sep = "")
# 
# 
# df <- nacc_12_varstoimp
# 
# for(tst in paste("Test", 4:227, sep = "")) {
# 
#   # Initialize the column of imputed values with the raw values.
#   tst.imp = paste(tst, "imp", sep = ".")
#   df[,tst.imp] = df[,tst]
# 
#   # Build a separate model for each group.
#   for(grp in unique(df$Group)) {
# 
#     # The dataset for this model includes observations in the group in question,
#     # minus any observations with the special 888 and 999 codes.
#     temp.df = df %>%
#       mutate(test.score = .data[[tst]]) %>%
#       filter(Group == grp,
#              !(test.score %in% c(95, 995, 97, 997)))
# 
#     # Fit a logistic or linear regression, as appropriate, and use the model to
#     # generate predictions for the whole dataset.  For the logistic regression,
#     # stochastically chose 0 or 1 based on the probability; for the linear
#     # regression, add noise to the predictions based on the observed scale of
#     # residuals.
#     if(tst == "Test" %in% nacc_12_bin_colnames) {
#       m = glm(test.score ~ NACCVNUM + CDRSUM + MOCATOTS + Group + NACCAGE + EDUC,
#               data = temp.df, family = binomial(link = "logit"))
#       preds = as.numeric(predict(m, newdata = df, type = "response") < runif(nrow(df)))
#     } else {
#       m = lm(test.score ~  NACCVNUM + CDRSUM + MOCATOTS + Group + NACCAGE + EDUC,
#              data = temp.df)
#       preds = predict(m, newdata = df) + rnorm(nrow(df), 0,
#                                                coalesce(summary(m)$sigma, 0))
#     }
# 
#     # Add the predictions to the column of imputed values, but only for the
#     # current group and only for columns with the special codes.
#     df[,tst.imp] = case_when(df$Group != grp ~ df[,tst.imp],
#                              df[,tst.imp] == 95 ~ preds,
#                              df[,tst.imp] == 995 ~ preds,
#                              df[,tst.imp] == 97 ~ preds,
#                              df[,tst.imp] == 997 ~ preds,
#                              T ~ df[,tst.imp])
# 
#   }
# 
# }
# 
# 
# 
# 
# 
# 
# 
# 
# 
# nacc_12 <- nacc_11 %>%
#   mutate(across(221:336, case_when(
#     . == 95 ~ predict(lm(.~NACCAGE+EDUC+CDRSUM+MOCATOTS+NACCVNUM+Group)),
#     TRUE ~ .
#   )))
# 
# 
# 
# 
# 
# nacc_12 <- nacc_11 %>%
#   group_by(Group, NACCVNUM) %>%
#   # substitute all 96 and 996 with minimum number of those columns
#   mutate(across(221:336, ~replace(., . %in% c(95, 97), predict(lm(.~NACCAGE+EDUC+CDRSUM+MOCATOTS)))))
#   
# 
# 
# 
#   
# 
#   # for MMSE orientation and pentagon (Only available subscores), replace 95, 96, 97, 98 with NA 
#   mutate(across(221:224, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for MMSE total, replace also 88 (missing at least one MMSE item) with NA 
#   mutate(NACCMMSE = replace(NACCMMSE, NACCMMSE == 88, NA)) %>%
#   # for logical memory, Benson figure, digits span, animal and veg fluency, replace 88 (Unknown) and 95:98 with NA. Note that max scores for all these tests are under 88 so I won't be substituting any non-NA values
#   mutate(across(228:243, ~replace(., . %in% c(88, 95, 96, 97, 98), NA))) %>%
#   # For Trails, replace 995:998 as NA
#   mutate(across(244:249, ~replace(., . %in% c(995, 996, 997, 998), NA))) %>%
#   # for Trails A commission errors and correct lines, replace 95:99 with NA
#   mutate(across(245:246, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for Trails B commission errors and correct lines, replace 95:99 with NA
#   mutate(across(248:249, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for WAIS digit symbol, Boston, UDS fluency, replace 95:98 with NA
#   mutate(across(250:260, ~replace(., . %in% c(95, 96, 97, 98), NA))) %>%
#   # for MOCA, Craft, number span, MINT, Rey replace 88, 95:98 with NA
#   mutate(across(263:325, ~replace(., . %in% c(88, 95, 96, 97, 98), NA))) %>%
#   # for MOCA B - blind score - corrected for education, replace 99 (item/whole test not administered with NA) 
#   mutate(NACCMOCB = replace(NACCMOCB, NACCMOCB == 99, NA)) %>%
#   # for Oral Trails, replace 888, 995:998 with NA
#   mutate(across(326:331, ~replace(., . %in% c(888, 995, 996, 997, 998), NA))) %>%
#   # for RAVLT and verbal naming test, replace 88, 95:98 with NA
#   mutate(across(332:336, ~replace(., . %in% c(88, 95, 96, 97, 98), NA)))
```

Imputing data by splitting the dataset into binary vs. continuous datasets, by each time point, then running the imputation on each dataset and then merging it all.

```{r}

# in Whole group
# 
# 
# 
# # create a vector with names of all binary variables
# nacc_voi_dem_all_5_allcols <- dput(names(nacc_voi_dem_all_5))
# 
# # save a copy of this vector with only binary variables
# nacc_voi_dem_all_5_binvars <- c("NACCID", "NACCVNUM", "Group", "EDUC", "NACCAGE", "CDRSUM", "DEL",  "HALL", "AGIT", "DEPD", "ANX", "ELAT", "APA", "DISN", "IRR",  "MOT", "NITE", "APP", "SATIS", "DROPACT", "EMPTY", "BORED", 
# "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", 
# "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "DECSUB", "DECIN", "COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", "COGOTHR", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEOTHR", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW")
# 
# nacc_voi_dem_all_5_bincols <- c("DEL",  "HALL", "AGIT", "DEPD", "ANX", "ELAT", "APA", "DISN", "IRR",  "MOT", "NITE", "APP", "SATIS", "DROPACT", "EMPTY", "BORED", 
# "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", 
# "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "DECSUB", "DECIN", "COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", "COGOTHR", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEOTHR", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW")
# 
# # tally variables that are different between both vectors and feed these into the next step for creating a vector of continuous variables.
# dput(setdiff(nacc_voi_dem_all_5_allcols, nacc_voi_dem_all_5_binvars))
# 
# # save a copy of the vector with only continuous variables
# nacc_voi_dem_all_5_contvars <- c("NACCID", "NACCVNUM", "Group", "EDUC", "NACCAGE", "CDRSUM","MEMORY", "ORIENT", "JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", "CDRGLOB", "COMPORT", "CDRLANG", "DELSEV", "HALLSEV", "AGITSEV", 
# "DEPDSEV", "ANXSEV", "ELATSEV", "APASEV", "DISNSEV", "IRRSEV", 
# "MOTSEV", "NITESEV", "APPSEV",  "BILLS", "TAXES", "SHOPPING", "GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", "TRAVEL", "DECCLIN", "FRSTCHG", "ANIMALS", "VEG", "TRAILA", "TRAILARR", "TRAILBRR", "COGSTAT")
# 
# 
# 
# test <- mice (nacc_voi_dem_all_5, maxit = 0)
# 
# pred_test <- test$predictorMatrix
# 
# pred_test[, c("NACCID", "ClinDiagDementia", "ClinDiagMCI", "ClinDiagMCISubtype", "NACCADC", "NACCUDSD", "NACCDAYS", "NACCFDYS", 
# "NACCDIED", "NACCMOD", "BIRTHYR", "RACE", "PRIMLANG", 
# "INDEPEND", "HANDED", "NACCAGEB", "NORMCOG", "NACCFAM", 
# "NACCMOM", "NACCDAD", "NACCFADM", "NACCFFTD", "MEMORY", "ORIENT", 
# "JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", "CDRGLOB", 
# "COMPORT", "CDRLANG", "DEL", "DELSEV", "HALL", "HALLSEV", "AGIT", 
# "AGITSEV", "DEPD", "DEPDSEV", "ANX", "ANXSEV", "ELAT", "ELATSEV", 
# "APA", "APASEV", "DISN", "DISNSEV", "IRR", "IRRSEV", "MOT", "MOTSEV", 
# "NITE", "NITESEV", "APP", "APPSEV", "SATIS", "DROPACT", "EMPTY", 
# "BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", 
# "MEMPROB", "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", 
# "BILLS", "TAXES", "SHOPPING", "GAMES", "STOVE", "MEALPREP", "EVENTS", 
# "PAYATTN", "REMDATES", "TRAVEL", "DECSUB", "DECIN", "DECCLIN", 
# "COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", "COGFLUC", 
# "COGOTHR", "NACCCOGF", "COGMODE", "DECAGE", "BEAPATHY", "BEDEP", 
# "BEVHALL", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", 
# "BEPERCH", "BEREM", "BEOTHR", "NACCBEHF", "BEMODE", "MOGAIT", 
# "MOFALLS", "MOTREM", "MOSLOW", "NACCMOTF", "MOMODE", "COURSE", 
# "FRSTCHG", "ANIMALS", "VEG", "TRAILA", "TRAILARR", "TRAILBRR", 
# "COGSTAT")] <- 0
# 
# 
# pred_mat_logreg_all[, c("NACCID" ,"NACCVNUM", "DEL", "HALL", "AGIT", "DEPD", "ANX", "ELAT", "APA", "DISN", "IRR", "MOT", "NITE", "APP", "SATIS", "DROPACT", "EMPTY", "BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", 
# "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "DECSUB", 
# "DECIN", "COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", 
# "COGFLUC", "COGOTHR", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", 
# "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", 
# "BEOTHR", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW")] <- 0
# 
# 
# # create six versions of this dataset, three (one for each time point) with binary variables + Age, education, group, and CDR, and three (one for each time point)  with continuous variables + Age, education, group, and CDR. For bin variables, I will match it with the nacc_bin
# 
# # binary variable datasets
# 
# nacc_voi_dem_all_5_bin_t1 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 1) %>%
#   select(all_of(nacc_voi_dem_all_5_binvars)) %>%
#   mutate_if(is.integer, as.numeric) %>%
#   mutate_at(nacc_voi_dem_all_5_bincols, factor)
# 
# nacc_voi_dem_all_5_bin_t2 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 2) %>%
#   select(all_of(nacc_voi_dem_all_5_binvars)) %>%
#   mutate_if(is.integer, as.numeric)
# 
# nacc_voi_dem_all_5_bin_t3 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 3) %>%
#   select(all_of(nacc_voi_dem_all_5_binvars)) %>%
#   mutate_if(is.integer, as.numeric)

# continuous variable datasets
#   
# nacc_voi_dem_all_5_cont_t1 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 1) %>%
#   select(all_of(nacc_voi_dem_all_5_contvars)) %>%
#   mutate_if(is.integer, as.numeric)
# 
# nacc_voi_dem_all_5_cont_t2 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 2) %>%
#   select(all_of(nacc_voi_dem_all_5_contvars)) %>%
#   mutate_if(is.integer, as.numeric)
# 
# nacc_voi_dem_all_5_cont_t3 <- nacc_voi_dem_all_5 %>%
#   filter(NACCVNUM == 3) %>%
#   select(all_of(nacc_voi_dem_all_5_contvars)) %>%
#   mutate_if(is.integer, as.numeric)


# in PPA

# 
# # create a vector with names of all binary variables
# nacc_voi_dem_PPA_3_allcols <- dput(names(nacc_voi_dem_PPA_3))
# 
# # save a copy of this vector with only binary variables
# nacc_voi_dem_PPA_binvars <- c("NACCID", "NACCVNUM", "Group", "EDUC", "NACCAGE", "CDRSUM", "DEL",  "HALL", "AGIT", "DEPD", "ANX", "ELAT", "APA", "DISN", "IRR",  "MOT", "NITE", "APP", "DECSUB", "DECIN", "COGMEM", "COGJUDG", "COGLANG", 
# "COGVIS", "COGATTN", "COGFLUC", "COGOTHR", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", "BEOTHR", 
# "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW")
# 
# # tally variables that are different between both vectors and feed these into the next step for creating a vector of continuous variables.
# setdiff(nacc_voi_dem_PPA_3_allcols, nacc_voi_dem_PPA_binvars)
# 
# # save a copy of the vector with only continuous variables
# nacc_voi_dem_PPA_contvars <- c("NACCID", "NACCVNUM", "Group", "EDUC", "NACCAGE", "CDRSUM", "MEMORY", "ORIENT", "JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", "CDRGLOB", "COMPORT", "CDRLANG", "DELSEV", "HALLSEV", "AGITSEV", "DEPDSEV", "ANXSEV",  "ELATSEV",  "APASEV", "DISNSEV",  "IRRSEV", "MOTSEV", "NITESEV",  "APPSEV", "BILLS", "TAXES", "SHOPPING", 
# "GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", 
# "TRAVEL", "NACCCOGF", "COGMODE",  "DECAGE", "NACCBEHF", "BEMODE", "NACCMOTF", "MOMODE", "COURSE", "FRSTCHG",  "TRAILA", "TRAILBRR", "COGSTAT")
#    
# 
# # create six versions of this dataset, three (one for each time point) with binary variables + Age, education, group, and CDR, and three (one for each time point)  with continuous variables + Age, education, group, and CDR. For bin variables, I will match it with the nacc_bin
# 
# # binary variable datasets
# 
# nacc_voi_dem_PPA_3_bin_t1 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 1) %>%
#   select(all_of(nacc_voi_dem_PPA_binvars))
# 
# nacc_voi_dem_PPA_3_bin_t2 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 2) %>%
#   select(all_of(nacc_voi_dem_PPA_binvars))
# 
# nacc_voi_dem_PPA_3_bin_t3 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 3) %>%
#   select(all_of(nacc_voi_dem_PPA_binvars))
# 
# # continuous variable datasets
#   
# nacc_voi_dem_PPA_3_cont_t1 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 1) %>%
#   select(all_of(nacc_voi_dem_PPA_contvars))
# 
# nacc_voi_dem_PPA_3_cont_t2 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 2) %>%
#   select(all_of(nacc_voi_dem_PPA_contvars))
# 
# nacc_voi_dem_PPA_3_cont_t3 <- nacc_voi_dem_PPA_3 %>%
#   filter(NACCVNUM == 3) %>%
#   select(all_of(nacc_voi_dem_PPA_contvars))
# 
# 
# {r}
# 
# 
# # In each dataset, run MICE 
# 
# # create ID df first with the overall PPA dataset
# 
# data_impute_ID_logreg_all <- nacc_voi_dem_all_5 %>% 
#   select(1:6)
# # create another ID and time variable to match the other data
# data_impute_ID_logreg_all$Time <- rep_len(1:3, length.out = 5331)
# data_impute_ID_logreg_all$ID <- gl(1777, 3)
# data_impute_ID_logreg_all$ID <- as.numeric(data_impute_ID_logreg_all$ID)
# 
# # split this into three datasets for each time point
# data_impute_ID_bin_all_t1 <- data_impute_ID_logreg_all %>% 
#   filter(Time == 1)
# 
# data_impute_ID_bin_all_t2 <- data_impute_ID_logreg_all %>% 
#   filter(Time == 2)
# 
# data_impute_ID_bin_all_t3 <- data_impute_ID_logreg_all %>% 
#   filter(Time == 3)
# 
# 
# # next, create a predictor matrix specifying Age, Education, and CDR as predictors
# # to do this, there is some circular analysis invovled. The predictor matrix emerges from the mice results, so we have to run MICE and then extract and reset the predictor matrix.
# 
# test_logreg_all_bin_t1 <- mice(nacc_voi_dem_all_5_bin_t1,
#                                maxit = 0)
# 
# 
# pred_mat_logreg_all <- test_logreg_all_bin_t1$predictorMatrix
# 
# # keep only Group, Education, Age, CDR SOB as predictors (1) and set everything else to non-predictor (0)
# 
# dput(colnames(pred_mat_logreg_all))
# 
# # set all other variables to 0
# pred_mat_logreg_all[, c("NACCID" ,"NACCVNUM", "DEL", "HALL", "AGIT", "DEPD", "ANX", "ELAT", "APA", "DISN", "IRR", "MOT", "NITE", "APP", "SATIS", "DROPACT", "EMPTY", "BORED", "SPIRITS", "AFRAID", "HAPPY", "HELPLESS", "STAYHOME", "MEMPROB", 
# "WONDRFUL", "WRTHLESS", "ENERGY", "HOPELESS", "BETTER", "DECSUB", 
# "DECIN", "COGMEM", "COGJUDG", "COGLANG", "COGVIS", "COGATTN", 
# "COGFLUC", "COGOTHR", "BEAPATHY", "BEDEP", "BEVHALL", "BEAHALL", 
# "BEDEL", "BEDISIN", "BEIRRIT", "BEAGIT", "BEPERCH", "BEREM", 
# "BEOTHR", "MOGAIT", "MOFALLS", "MOTREM", "MOSLOW")] <- 0
# 
# 
# # conduct imputations on the time-specific datasets. If the imputation fails, you may have to specify where the binary data are by using something like data[,c(1:5)] to specify binvars at 1:5 cols
# 
# # Time 1 binary vars
# # run the imputation
# test_logreg_all_bin_t1 <- mice(nacc_voi_dem_all_5_bin_t1,
#                                m = 5,
#                                method = "logreg",
#                                maxit = 20,
#                                pred = pred_mat_logreg_all)
# 
# 
# 
# 
# test_logreg_all_bin_t1 <- mice(nacc_voi_dem_all_5_bin_t1,
#   m = 5,
#   method = "pmm",
#   maxit = 20,
#   pred = pred_mat_logreg_all)
# 
# # fit the imputed data
# data_logreg_all_bin_t1 <- mice::complete(test_logreg_all_bin_t1, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 1
# data_logreg_all_bin_t1$ID <- seq.int(nrow(data_logreg_all_bin_t1))
# data_logreg_all_bin_t1$Time <- rep(1, nrow(data_logreg_all_bin_t1))
# 
# 
# # Time 2 binary vars
# # run the imputation
# test_logreg_all_bin_t2 <- mice(nacc_voi_dem_all_5_bin_t2,
#                                m = 5,
#                                method = "logreg",
#                                maxit = 20,
#                                pred = pred_mat_logreg_all)
# 
# # fit the imputed data
# data_logreg_all_bin_t2 <- mice::complete(test_logreg_all_bin_t2, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 1
# data_logreg_all_bin_t2$ID <- seq.int(nrow(data_logreg_all_bin_t2))
# data_logreg_all_bin_t2$Time <- rep(1, nrow(data_logreg_all_bin_t2))
# 
# 
# # Time 3 binary vars
# # run the imputation
# test_logreg_all_bin_t3 <- mice(nacc_voi_dem_all_5_bin_t3,
#                                m = 5,
#                                method = "logreg",
#                                maxit = 20,
#                                pred = pred_mat_logreg_all)
# 
# # fit the imputed data
# data_logreg_all_bin_t3 <- mice::complete(test_logreg_all_bin_t3, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 1
# data_logreg_all_bin_t3$ID <- seq.int(nrow(data_logreg_all_bin_t3))
# data_logreg_all_bin_t3$Time <- rep(1, nrow(data_logreg_all_bin_t3))
# 
# 
# # for continuous variables I will run the imputation using norm.predict which is linear regression predicted values fitting.
# 
# # next, create a predictor matrix specifying Age, Education, and CDR as predictors
# # to do this, there is some circular analysis invovled. The predictor matrix emerges from the mice results, so we have to run MICE and then extract and reset the predictor matrix.
# 
# pred_mat_norm_predict <- test_pmm_PPA_cont_t1$predictorMatrix
# 
# # keep only Group, Education, Age, CDR SOB as predictors (1) and set everything else to non-predictor (0)
# 
# dput(colnames(pred_mat_norm_predict))
# 
# # set all other variables to 0
# pred_mat_norm_predict[, c("NACCID", "NACCVNUM", 
# "MEMORY", "ORIENT", "JUDGMENT", "COMMUN", "HOMEHOBB", "PERSCARE", 
# "CDRGLOB", "COMPORT", "CDRLANG", "DELSEV", "HALLSEV", "AGITSEV", 
# "DEPDSEV", "ANXSEV", "ELATSEV", "APASEV", "DISNSEV", "IRRSEV", 
# "MOTSEV", "NITESEV", "APPSEV", "BILLS", "TAXES", "SHOPPING", 
# "GAMES", "STOVE", "MEALPREP", "EVENTS", "PAYATTN", "REMDATES", 
# "TRAVEL", "NACCCOGF", "COGMODE", "DECAGE", "NACCBEHF", "BEMODE", 
# "NACCMOTF", "MOMODE", "COURSE", "FRSTCHG", "TRAILA", "TRAILBRR", 
# "COGSTAT")] <- 0
# 
# 
# # run the imputation
# test_pmm_PPA_cont_t1 <- mice(nacc_voi_dem_PPA_3_cont_t1,
#   m = 5,
#   method = "norm.predict",
#   maxit = 20,
#   pred = pred_mat_norm_predict)
# 
# # fit the imputed data
# data_pmm_PPA_cont_t1 <- mice::complete(test_pmm_PPA_cont_t1, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 1
# data_pmm_PPA_cont_t1$ID <- seq.int(nrow(data_pmm_PPA_cont_t1))
# data_pmm_PPA_cont_t1$Time <- rep(1, nrow(data_pmm_PPA_cont_t1))
# 
# # run the imputation
# test_pmm_PPA_cont_t2 <- mice(nacc_voi_dem_PPA_3_cont_t2,
#   m = 5,
#   method = "norm.predict",
#   maxit = 20,
#   pred = pred_mat_norm_predict)
# 
# # fit the imputed data
# data_pmm_PPA_cont_t2 <- mice::complete(test_pmm_PPA_cont_t2, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 2
# data_pmm_PPA_cont_t2$ID <- seq.int(nrow(data_pmm_PPA_cont_t2))
# data_pmm_PPA_cont_t2$Time <- rep(2, nrow(data_pmm_PPA_cont_t2))
# 
# # run the imputation
# test_pmm_PPA_cont_t3 <- mice(nacc_voi_dem_PPA_3_cont_t3,
#   m = 5,
#   method = "norm.predict",
#   maxit = 20,
#   pred = pred_mat_norm_predict)
#   
# # fit the imputed data
# data_pmm_PPA_cont_t3 <- mice::complete(test_pmm_PPA_cont_t3, 1)
# 
# # create an ID variable which is a sequence and a time variable which is a repeat of 3
# data_pmm_PPA_cont_t3$ID <- seq.int(nrow(data_pmm_PPA_cont_t3))
# data_pmm_PPA_cont_t3$Time <- rep(3, nrow(data_pmm_PPA_cont_t3))
# 
# # convert negative values into positive
# data_pmm_PPA_cont_t3$TRAILBRR <- abs(data_pmm_PPA_cont_t3$TRAILBRR)
# 
# 
# Following imputation, I will merge and append these datasets to get a full, final dataset.
# 
# {r}
# 
# # as all the data have ID and time variables, we can bind it to data_impute_ID_logreg_PPA using the ID and time as identifiers
# 
# # Merging will have to be done in stages
# 
# # merge time 1 data by ID
# 
# PPA_merged_dfs_t1 <- merge(data_impute_ID_logreg_PPA_t1,
#                            data_logreg_PPA_bin_t1,
#                            by = "ID", 
#                            all.x = T)
# 
# PPA_merged_dfs_t1_fin <- merge(PPA_merged_dfs_t1,
#                            data_pmm_PPA_cont_t1,
#                            by = "ID", 
#                            all.x = T)
# 
# 
# # merge time 2 data by ID
# PPA_merged_dfs_t2 <- merge(data_impute_ID_logreg_PPA_t2,
#                            data_logreg_PPA_bin_t2,
#                            by = "ID", 
#                            all.x = T)
# 
# PPA_merged_dfs_t2_fin <- merge(PPA_merged_dfs_t2,
#                            data_pmm_PPA_cont_t2,
#                            by = "ID", 
#                            all.x = T)
# 
# # merge time 3 data by ID
# PPA_merged_dfs_t3 <- merge(data_impute_ID_logreg_PPA_t3,
#                            data_logreg_PPA_bin_t3,
#                            by = "ID", 
#                            all.x = T)
# 
# PPA_merged_dfs_t3_fin <- merge(PPA_merged_dfs_t3,
#                            data_pmm_PPA_cont_t3,
#                            by = "ID", 
#                            all.x = T)
# 
# 
# # append all three dfs together 
# 
# final_PPA_merged_dfs_imputed <- rbind (PPA_merged_dfs_t1_fin,
#                  PPA_merged_dfs_t2_fin,
#                  PPA_merged_dfs_t3_fin
#                  )
# 
# # are there any NAs in this dataset?
# summary(is.na(final_PPA_merged_dfs_imputed))
# 
# 
# 
# # extract colnames and remove duplicated columns
# dput(colnames(final_PPA_merged_dfs_imputed))
# 
# # remove duplicated columns, remove .x and .y suffixes, rearrange column order and order df by NACCID
# 
# nacc_voi_dem_PPA_4 <- final_PPA_merged_dfs_imputed %>%
#   # remove selected columns
#   select(
#     -c("ID", "Time", "Time.x", "Time.y", "NACCID.x", "NACCID.y", "NACCVNUM.x", "NACCVNUM.y", "Group.x", "Group.y", "EDUC.y", "NACCAGE.y", "CDRSUM.y"
#       )) %>%
#   # rearrange column order to bring Education etc. to front
#   relocate( # all demographic variables first
#     NACCID, NACCVNUM, Group, ClinDiagDementia, ClinDiagMCI, ClinDiagMCISubtype, EDUC.x, NACCAGE.x, 
#     # then CDR variables
#     CDRSUM.x, MEMORY, ORIENT, JUDGMENT, COMMUN, HOMEHOBB, PERSCARE, 
# CDRGLOB, COMPORT, CDRLANG, 
# # then NPI variables
# DEL, HALL, AGIT, DEPD, ANX, ELAT, APA, DISN, IRR, MOT, NITE, APP, DELSEV, HALLSEV, AGITSEV, DEPDSEV, ANXSEV, ELATSEV, APASEV, DISNSEV, IRRSEV, MOTSEV, NITESEV, APPSEV, everything()) %>%
#   # order it by ID
#   arrange(NACCID)
# 
# # remove .x and .y suffixes from the colnames, if any remain
# colnames(nacc_voi_dem_PPA_4) <- gsub(".x","",colnames(nacc_voi_dem_PPA_4))
# 
# #nacc_voi_dem_PPA_4 %>% mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .))
# 
# # final check to see if any NAs exist in this df
# summary(is.na(nacc_voi_dem_PPA_4))
# 
# # the final dataset to use going forward is nacc_voi_dem_PPA_4
# 

```

```{r}


# 
# test_1 <- nacc_voi_dem_all_11 %>%
#   ungroup() %>%
#   filter(NACCVNUM == 1) %>%
#   select(Dx_Group, DELSEVREVPRCNT:APPSEVREVPRCNT, TRAILARRREVPRCNT:VEGREVPRCNT) %>%
#   as.data.frame()
# 
# 
# 
# test_2 <- data.frame(test_1[,-1], row.names=test_1[,1])
# 
# 
# test_1 <- data.frame(test_1, row.names = 1)
# 
# mds <- test_1 %>% dist() %>% cmdscale() %>% as_tibble()
# 
# 
# 
# test_1 %>% remove_rownames %>% column_to_rownames(var="Dx_Group")
# 
# 
# rownames(test_1) <- test_1[,1]
# test_1[,1] <- NULL
# 
# 
# test_2 <- test_1[,-1]
# rownames(test_2) <- test_1[, 1]
# 
# test_1 <- tibble::rownames_to_column(test_1, "Dx_Group")
# test_1 <- test_1[, -1]
# 
# colnames(mds) <- c("Dim.1", "Dim.2")
# 
# 
# ggscatter(mds, x = "Dim.1", y = "Dim.2", 
#           label = rownames(test_1$Dx_Group),
#           size = 1,
#           repel = TRUE)
# 
# 
# fit <- psych::principal(test_1, rotate="varimax", nfactors = 6, scores = T)
# print(fit$scores[1:5,])
# 
# 
# 
# 
# pca_g <- psych::principal(nacc_voi_dem_all_11_time1, nfactors=5, scores=TRUE, rotate = "varimax")
```

Euclidean distance per component

Now, lets calculate the change in scores in individuals who have the same diagnosis throughout their 3 years. We can do this two ways. First, I calculate the raw PC scores. Lets do this for component 1.

```{r}

# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("RC")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "RC1" ~ "PC1",
#     variable == "RC2" ~ "PC2",
#     variable == "RC3" ~ "PC3",
#     variable == "RC4" ~ "PC4",
#     variable == "RC5" ~ "PC5",
#     variable == "RC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#  # filter(Component == "PC1") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1,
#              position = position_jitter(width = 0.05)) +
#    # facet the plot by Dx
#   facet_grid(Component ~ Dx,
#              space = "free") + 
#   # show the mean value for each time point (in the form of a triangle)
#    stat_summary(
#      aes(group = Component), #fill = Component),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 5,
#     shape = 24, 
#     fill = "red") +
#   #  scale_fill_manual("Component", 
#   #                    values = c("PC1" = "#E69F00", 
#   #                               "PC2" = "#56B4E9",
#   #                               "PC3" = "#009E73",
#   #                               "PC4" = "#F0E442",
#   #                               "PC5" = "#D55E00", 
#   #                               "PC6" = "#CC79A7")) +
#  
#   # fix final themes and X/Y axis text size
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12)) +
#   ylab("Score") +
#   # remove legend
#   theme(legend.position="none")
```

Next, I calculate the euclidean distance for each PC score. By doing it by each PC, it calculates the mean distance for every score from every other score (irrespective of group). This allows to look at changes in each patient group, relative to themselves and relative to the others. Lets visualise this just for component 1.

```{r}
# 
# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC1") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Now for comp 2

```{r}

# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC2") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Comp 3

```{r}
# 
# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC3") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Comp 4

```{r}

# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC4") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Comp 5

```{r}

# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC5") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Comp 6

```{r}

# fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     )) %>%
#   # select only PC1 for now
#   filter(Component == "PC6") %>%
#   # plot these variables in ggplot
#   ggplot(aes(x = Visit, y = value, group = ID)) +
#   # add a line
#   geom_line(alpha = 0.1) +
#   # add points
#   geom_point(alpha = 0.1) +
#   # add a mean for each time point
#    stat_summary(
#      aes(group = Dx),
#     geom = "point",
#     fun = "mean",
#     col = "black",
#     size = 3,
#     shape = 24,
#     fill = "red"
#   ) +
#   # facet the plot by Dx
#     facet_grid(~Dx) +
#   theme_bw() +
#   theme_classic() +
#   theme(axis.text.y = element_text(size = 12),
#         axis.text.x = element_text(size = 12),
#         axis.title.x = element_text(size = 12),
#         axis.title.y = element_text(size = 12),
#         strip.text = element_text(size = 12))
```

Now, in the next analyses, I will create a new df with just the distance matrix so I can use that to make distance plots. The idea is to calculate distance matrices for each person from themselves and from the others in the same group and from others in different groups.

```{r}

# fit_scores_df_ID_dist <- fit_scores_df_ID %>%
#   # group by ID
#   group_by(ID, Dx) %>%
#   # keep individuals who have the same Dx for all three time periods
#   filter(n() == 3) %>%
#   # calculate distance matrix for each column
#   mutate(distPC1 = dist(RC1, method = "euclidean"),
#          distPC2 = dist(RC2, method = "euclidean"),
#          distPC3 = dist(RC3, method = "euclidean"),
#          distPC4 = dist(RC4, method = "euclidean"),
#          distPC5 = dist(RC5, method = "euclidean"),
#          distPC6 = dist(RC6, method = "euclidean")) %>%
#   # select the distance variables, ID, Dx, and Time
#   select(ID, Dx, Visit, starts_with("dist")) %>%
#   ungroup() %>%
#   # reshape the data
#   reshape2::melt() %>%
#   # create repeating factor for plotting
#   mutate(Component = case_when(
#     variable == "distPC1" ~ "PC1",
#     variable == "distPC2" ~ "PC2",
#     variable == "distPC3" ~ "PC3",
#     variable == "distPC4" ~ "PC4",
#     variable == "distPC5" ~ "PC5",
#     variable == "distPC6" ~ "PC6"
#     ))
```

Now, in the below analyses, I will calculate distance matrices for each person from themselves and from the others.

```{r}

# # Component 1
# 
# # extract the first component, create a euclidean distance matrix and save as a new matrix
# fit_dist_PC1 <- dist(fit_scores[,1], method = "euclidean")
# 
# fit_dist_PC1 <- as.matrix(fit_dist_PC1)
# 
# # reshape this dataframe into long format
# fit_dist_PC1_long <- reshape2::melt(fit_dist_PC1)
# 
# # append ID columns into this df
# fit_dist_PC1_long_ID <- fit_dist_PC1_long %>%
#     # sequentially join in the PID df
#   # First, join in the PID column but by Var1
#   left_join(ID_Diag_VNUM_vec, by=c("Var1")) %>%
#   # rename the PID col to PID.Var1
#   rename(PID.Var1 = PID, Var2 = Var2.x) %>%
#   # remove Var2.y as duplicated
#   select(-Var2.y) %>%
#   # left join by Var 2
#   left_join(ID_Diag_VNUM_vec, by=c("Var2")) %>%
#   # rename PID to PID.var2
#   rename(PID.Var2 = PID, Var1 = Var1.x) %>%
#   # remove Var1.y as duplicated
#   select(-Var1.y) 
# 
# # for both PIDs split the string 
# fit_dist_PC1_long_ID_2 <- fit_dist_PC1_long_ID %>%
#   separate(PID.Var1, c("ID.Var1", "Dx.Var1", "Visit.Var1"), "_") %>%
#   separate(PID.Var2, c("ID.Var2", "Dx.Var2", "Visit.Var2"), "_")
# 
# # fit a distance plot
# ggplot(fit_dist_PC1_long_ID_2) +
#   geom_tile(aes(x=Visit.Var1, 
#         y = Visit.Var2, 
#         fill = value)) +
#   scale_fill_gradient2() +
#   facet_grid(Dx.Var1 ~ Dx.Var2, 
#              scales = "free", 
#              space = "free") +
#   scale_y_discrete(limits = rev) +
#   theme_bw() +
#   theme_classic() 

```

This works great! Repeat for other comps.

```{r}

# Comp 2
# 
# #  create a euclidean distance matrix and save as a new matrix
# fit_dist_PC2 <- dist(fit_scores[,2], method = "euclidean")
# 
# fit_dist_PC2 <- as.matrix(fit_dist_PC2)
# 
# # reshape this dataframe into long format
# fit_dist_PC2_long <- reshape2::melt(fit_dist_PC2)
# 
# # append ID columns into this df
# fit_dist_PC2_long_ID <- fit_dist_PC2_long %>%
#     # sequentially join in the PID df
#   # First, join in the PID column but by Var1
#   left_join(ID_Diag_VNUM_vec, by=c("Var1")) %>%
#   # rename the PID col to PID.Var1
#   rename(PID.Var1 = PID, Var2 = Var2.x) %>%
#   # remove Var2.y as duplicated
#   select(-Var2.y) %>%
#   # left join by Var 2
#   left_join(ID_Diag_VNUM_vec, by=c("Var2")) %>%
#   # rename PID to PID.var2
#   rename(PID.Var2 = PID, Var1 = Var1.x) %>%
#   # remove Var1.y as duplicated
#   select(-Var1.y) 
# 
# # for both PIDs split the string 
# fit_dist_PC2_long_ID_2 <- fit_dist_PC2_long_ID %>%
#   separate(PID.Var1, c("ID.Var1", "Dx.Var1", "Visit.Var1"), "_") %>%
#   separate(PID.Var2, c("ID.Var2", "Dx.Var2", "Visit.Var2"), "_")
# 
# # fit a distance plot
# ggplot(fit_dist_PC2_long_ID_2) +
#   geom_tile(aes(x=Visit.Var1, 
#         y = Visit.Var2, 
#         fill = value)) +
#   scale_fill_gradient2() +
#   facet_grid(Dx.Var1 ~ Dx.Var2, 
#              scales = "free", 
#              space = "free") +
#   scale_y_discrete(limits = rev) +
#   theme_bw() +
#   theme_classic() 
```

```{r}



# # fit a line graph to indicate relative distance at each time point
# 
# ggplot(fit_dist_PC1_long_ID_2, 
#        aes(x = Visit.Var1, 
#            y = value, 
#            group = ID.Var1)) +
#   facet_wrap(~Dx.Var1) 
#   #geom_point(aes(group = ID.Var1)) 
#   geom_line(aes(group = ID.Var1))
#   
#   geom_line(alpha = 0.1) +
#   geom_point(shape=21, 
#              position=position_jitter(width=0.2, height=.1), 
#              alpha = 0.1) +
#   facet_wrap(~Dx.Var1)
```

Now, time to examine within each group and each time point using histograms etc.

```{r}

# filter out individuals at Time 1 
#fit_dist_PC1_long_ID_2 %>% 
#filter(Visit.Var1 == 1 & Visit.Var2 == 1)
  

```

tsne

```{r}

# test_tSNE <- fit_scores_df_ID %>%
#   # select scores of interest
#   select(1:6) %>%
#   # create a row number ID so we can left-join in the meta-data later
#   mutate(ID.x=row_number()) 
# 
# test_tSNE_meta <- fit_scores_df_ID %>%
#   select(ID, Dx, Visit) %>%
#   mutate(ID.x=row_number()) 
# 
# tSNE_fit <- test_tSNE %>% 
#   column_to_rownames("ID.x") %>%
#   Rtsne(
#     perplexity = 8
#   )
# 
# tSNE_df <- tSNE_fit$Y %>% 
#   as.data.frame() %>%
#   rename(tSNE1="V1",
#          tSNE2="V2") %>%
#   mutate(ID.x=row_number())
# 
# tSNE_df <- tSNE_df %>%
#   inner_join(test_tSNE_meta, by="ID.x")
# 
# 
# tSNE_df %>%
#   ggplot(aes(x = tSNE1, 
#              y = tSNE2,
#              color = Dx)) +
#   geom_point() +
#   geom_line(aes(group=ID)) +
#   theme(legend.position="bottom")


```

UMAP parameter playing around

Its also importnat that I play around with UMAP parameters to see what happens when I change them up. The main parameters that affect clustering and visualisation are spread and min_dist. I found a page that talks about testing these parameters so I'm going to use their code <https://jef.works/blog/2022/01/19/exploring-umap-parameters/>

```{r}

# ## try some different parameters
# umap_params <- expand.grid(
# 	spread   = c(0.5, 1, 4),
# 	min_dist = c(0.01, 0.15, 0.75)
# )
# 	
# umaps <- lapply(seq(nrow(umap_params)), function(i) {
# 	print(i)
# 	emb <- umap(
# 		X         = test_umap[,1:6],
# 		spread    = umap_params$spread[i],
# 		min_dist  = umap_params$min_dist[i],
# 	)
# 	rownames(emb) <- colnames(mat)
# 	return(emb)
# })
# 
# library(data.table)
# d <- rbindlist(lapply(seq(nrow(umap_params)), function(i) {
# 	data.table(
# 		x = umaps[[i]][,1],
# 		y = umaps[[i]][,2],
# 		spread = umap_params$spread[i],
# 		min_dist = umap_params$min_dist[i],
# 		group = com
# 	)
# }))
# 												  
# library(ggplot2)
# library(scattermore)
# p <- ggplot(d) +
# 	geom_scattermore(
# 		mapping = aes(x = x, y = y, color = group),
# 		pointsize = 1,
# 		pixels    = c(1000, 1000)
# 	) +
# 	theme(
# 		axis.text = element_blank(),
# 		axis.ticks = element_blank(),
# 		axis.title = element_blank(),
# 		legend.position = "none"
# 	) +
# 	facet_wrap(min_dist ~ spread , 
# 		labeller = label_both,
# 		scales = "free")
# p <- p + theme_minimal() +
# 	theme(legend.position = "none")
```

Factor analysis for mixed data

```{r}

# 
# test <- nacc_voi_dem_all_11_time1
# 
# cols1 <- c("SATISREVPRCNT", "DROPACTREVPRCNT", 
# "EMPTYREVPRCNT", "BOREDREVPRCNT", "SPIRITSREVPRCNT", "AFRAIDREVPRCNT", 
# "HAPPYREVPRCNT", "HELPLESSREVPRCNT", "STAYHOMEREVPRCNT", "MEMPROBREVPRCNT", 
# "WONDRFULREVPRCNT", "WRTHLESSREVPRCNT", "ENERGYREVPRCNT", "HOPELESSREVPRCNT", 
# "BETTERREVPRCNT", "COGVISREVPRCNT", "COGATTNREVPRCNT", 
# "COGFLUCREVPRCNT", "COGOTHRREVPRCNT", "BEAPATHYREVPRCNT", "BEDEPREVPRCNT", 
# "BEVHALLREVPRCNT", "BEAHALLREVPRCNT", "BEDELREVPRCNT", "BEDISINREVPRCNT", 
# "BEIRRITREVPRCNT", "BEAGITREVPRCNT", "BEPERCHREVPRCNT", "BEREMREVPRCNT", 
# "BEOTHRREVPRCNT", "MOGAITREVPRCNT", "MOFALLSREVPRCNT", "MOTREMREVPRCNT", 
# "MOSLOWREVPRCNT")
# 
# test[cols1] <- lapply(test[cols1], factor)
# 
# g <- FactoMineR::FAMD(test, ncp = 6)
```

Weighted UMAP

its worth considering a weighted UMAP where each PC score is weighted for the proportion variance explained by that respective component. This is because PC 1 may be more important than PC 2 and so on. I don't know if this makes sense because the scores never change in the PC space - only the loadngs do - so there's no sense multipliying the scores with the loadings of the PC space.

Update: i tried this method out and it doesn't work.

```{r}

# 
# fit_pca_noLL_scores_df_ID_weighted <- fit_pca_noLL_scores_df_ID
# 
# fit_pca_noLL_scores_df_ID_weighted$RC1 <- fit_pca_noLL_scores_df_ID_weighted$RC1 * 0.1884402
# fit_pca_noLL_scores_df_ID_weighted$RC2 <- fit_pca_noLL_scores_df_ID_weighted$RC2 * 0.08837595 
# fit_pca_noLL_scores_df_ID_weighted$RC3 <- fit_pca_noLL_scores_df_ID_weighted$RC3 * 0.06826017 
# fit_pca_noLL_scores_df_ID_weighted$RC4 <- fit_pca_noLL_scores_df_ID_weighted$RC4 * 0.06438296  
# fit_pca_noLL_scores_df_ID_weighted$RC5 <- fit_pca_noLL_scores_df_ID_weighted$RC5 * 0.05482835 
# fit_pca_noLL_scores_df_ID_weighted$RC6 <- fit_pca_noLL_scores_df_ID_weighted$RC6 * 0.05056845
# 
# 
# 
# 
# 
# test_umap_weighted <- fit_pca_noLL_scores_df_ID_weighted %>%
#   # select scores of interest
#   select(1:6) %>%
#   # create a row number ID so we can left-join in the meta-data later
#   mutate(ID.x=row_number()) 
# 
# # create a meta-dataset with ID and identifiers that we can left-join in later
# test_umap_weighted_meta <- fit_pca_noLL_scores_df_ID_weighted %>%
#   select(ID, Dx, Visit) %>%
#   mutate(ID.x=row_number()) 
# 
# # run the UMAP
# umap_weighted_fit <- test_umap_weighted %>% 
#   column_to_rownames("ID.x") %>%
#   umap(input = "data" # inputting a data matrix, not a distance matrix
#        )
#        #) # note that I am using the default parameters with k nearest neighbours as 15
# 
# # extract the UMAP dimensions and merge the scores in
# umap_weighted_df <- umap_weighted_fit$layout %>% 
#   as.data.frame() %>%
#   mutate(ID.x=row_number())
# 
# # join in the meta-data
# umap_weighted_df <- umap_weighted_df %>%
#   inner_join(test_umap_weighted_meta, by="ID.x")
# 
# # make a first plot of the UMAP results
# umap_weighted_p1 <- umap_weighted_df %>%
#   ggplot(aes(x = V1, 
#              y = V2,
#              color = Dx)) +
#   geom_point() +
#   geom_line(aes(group=ID)) +
#   scale_colour_manual(values = group.colours) +
#   theme_bw() +
#   theme(legend.position = c(0.15,0.2),
#         legend.background = element_rect(colour = "black")) +
#   guides(colour=guide_legend(title="Group")) +
#   ggtitle("All groups") +
#   theme(plot.title = element_text(hjust = 0.5)) +
#   ggtitle("All groups") +
#   theme(axis.text= element_text(size=15), axis.title = element_text(size=15))
# 

```

Try same approach with MFA

I came across a thread that recommended using MFA to do the same thing as int he step above (<https://stats.stackexchange.com/questions/18617/can-i-do-a-pca-on-repeated-measures-for-data-reduction>).

```{r}

#fit_mfa <- MFA(test_2,
#               group = c(1, 1, 1, 63),
#               type = c("c", "c", "c"),
#               name.group = c("1", "2", "3"))

```

one of the issues is that some patients have changed Dx with time.

see whether distance changes in those who change diagnosis

**Procrustes rotation to examine how different are individuals at each times**

One thing we are interested in is to see how different is each patient from themselves acrss all time points, and from other groups. For this, we can compare PC scores from T1, T2 and T3 in procrustes rotations and examine factor congruence metrics. See this link for more: <https://mindcultureevolution.com/2017/05/27/cross-cultural-structural-invariance-testing-how-to-run-the-procrustean-factor-rotation-magic-in-r-2/>

```{r}
# 
# # first, I split the PC scores into 3 dfs and remove the meta variables
# pc_fit_t1 <- fit_pca_noLL_scores_df_ID %>%
#   filter(Visit == 1) %>%
#   select(1:6)
# 
# pc_fit_t2 <- fit_pca_noLL_scores_df_ID %>%
#   filter(Visit == 2) %>%
#   select(1:6)
# 
# pc_fit_t3 <- fit_pca_noLL_scores_df_ID %>%
#   filter(Visit == 3) %>%
#   select(1:6)
# 
# # compare T1 nad T2
# proc <- vegan::procrustes(pc_fit_t1, 
#                           pc_fit_t2,
#                           scale = F
#                           )


```

#### Test euclidean distance

Since we have 6 comps, we can create a Euclidean distance matrix for every person, for all 6 comps. What this does is basically collapses all components to make one point per person per time point. If we think of all 6 comps, collapsed, as the overall "profile" of the person, measuring the Euclidean distance can tell us how each person evolves over time points in terms of their overall profile and how this profile varies from others with the same/different diagnosis.

```{r}

# # first, let us create a new meta ID df. This is because I will re-arrange the dataframe to make sure all IDs are together, then all visits are together, and then all Dxs are clubbed together.
# 
# fit_pca_noLL_scores_df_ID_meta <- fit_pca_noLL_scores_df_ID %>%
#    arrange(Visit, Dx, ID) %>%
#   select(7:9) %>%
#   # add a sequence of integers that we can use to merge to the euclidean distance matrix. I will create two of these variables because we'll melt both axes of the matrix in the next two steps
#   mutate(Var1 = seq.int(nrow(.))) %>%
#   mutate(Var2 = seq.int(nrow(.)))
# 
# # create euclidean distance matrix
# fit_pca_noLL_scores_df_ID_distmatrix <- fit_pca_noLL_scores_df_ID %>%
#   arrange(Visit, Dx, ID) %>%
#   select(1:6) %>%
#   dist(method = "euclidean") %>%
#   as.matrix()
# 
# # melt this matrix
# fit_pca_noLL_scores_df_ID_distmatrix_long <- reshape2::melt(fit_pca_noLL_scores_df_ID_distmatrix)
# 
# # merge in the ID variable
# fit_pca_noLL_scores_df_ID_distmatrix_long_ID <- fit_pca_noLL_scores_df_ID_distmatrix_long %>%
#   left_join(fit_pca_noLL_scores_df_ID_meta, by = c("Var1")) 
# 
# fit_pca_noLL_scores_df_ID_distmatrix_long_ID_2 <- fit_pca_noLL_scores_df_ID_distmatrix_long_ID %>%
#   left_join(fit_pca_noLL_scores_df_ID_meta, by = c("Var2")) %>%
#   arrange(Visit.x, Dx.x, Visit.y) %>%
#   mutate(seqint1 = seq.int(nrow(.))) %>%
#   mutate(seqint2 = seq.int(nrow(.))) %>%
#   mutate(across(value, round, digits = 2))
# 
# # ^ this matrix gives us an estimate of where each group ends, each time point ends and each ID ends. Now, I use the matrix to make a dissimilarity plot and then I can put a box around where each group/time point is
# 
# # to check the coordinates of where to place the edge of the rectangle, you can use dplyr::filter to check where Visit.1x and Visit.2x start and end at 1 & 1, or 2 & 2 or 3 & 3. For example:
# # View(fit_pca_noLL_scores_df_ID_distmatrix_long_ID_2 %>%
# #       filter(Visit.x == 1 & Visit.y == 1)) 
# # this will tell you the corresponding Var1 and Var2 numbers and you can use these coordinates. Repeat for each dx Group by adding filter(Dx.x == "AD" & Dx.y == "AD")
# 
# ggplot(fit_pca_noLL_scores_df_ID_distmatrix_long_ID_2, 
#        aes(x = Var1.x, y = Var2)) +
#   geom_raster(aes(fill = value)) +
#   scale_fill_gradient2(low = "azure", mid = "azure2", high = "azure4") + # as there are a lot of low-loading values, I set this colour scheme so that the high loading values largely stand out. Lets see if it visualises it nicely.
#   theme(axis.text.x = element_blank(),
#         axis.ticks.x = element_blank(),
#         axis.title.x = element_blank(),
#         axis.text.y = element_blank(),
#         axis.ticks.y = element_blank(),
#         axis.title.y = element_blank()
#         ) +
#   # add boxes indicating each time point and each dx group
#   # for Visit 1
#  geom_rect(aes(xmin = 1,
#                 xmax = 390,
#                 ymin = 1,
#                 ymax = 390),
#             fill = NA,
#            size = 1.5,
#             color = "black") +
#   # For visit 2
#   geom_rect(aes(xmin = 391,
#                 xmax = 780,
#                 ymin = 391,
#                 ymax = 780),
#             fill = NA,
#             size = 1.5,
#             color = "black") +
#   # For visit 3
#   geom_rect(aes(xmin = 781,
#                 xmax = 1170,
#                 ymin = 781,
#                 ymax = 1170),
#             fill = NA,
#             size = 1.5,
#             color = "black") +
#   # For AD (T1)
#   geom_rect(aes(xmin = 1,
#                 xmax = 114,
#                 ymin = 1,
#                 ymax = 114),
#             fill = NA,
#             size = 1,
#             color = "#D55E00") +
#   # For AD (T2)
#   geom_rect(aes(xmin = 391,
#                 xmax = 519,
#                 ymin = 391,
#                 ymax = 519),
#             fill = NA,
#             size = 1,
#             color = "#D55E00") +
#   # For AD (T3)
#   geom_rect(aes(xmin = 781,
#                 xmax = 911,
#                 ymin = 781,
#                 ymax = 911),
#             fill = NA,
#             size = 1,
#             color = "#D55E00") +
#   # for bvFTD (T1)
#   geom_rect(aes(xmin = 115,
#                 xmax = 221,
#                 ymin = 115,
#                 ymax = 221),
#             fill = NA,
#             size = 1,
#             color = "#009E73") +
#   # For bvFTD (T2)
#   geom_rect(aes(xmin = 520,
#                 xmax = 602,
#                 ymin = 520,
#                 ymax = 602),
#             fill = NA,
#             size = 1,
#             color = "#009E73") +
#   # For bvFTD (T3)
#   geom_rect(aes(xmin = 912,
#                 xmax = 987,
#                 ymin = 912,
#                 ymax = 987),
#             fill = NA,
#             size = 1,
#             color = "#009E73") +
#   # for FTLD-motor (T1)
#   geom_rect(aes(xmin = 222,
#                 xmax = 263,
#                 ymin = 222,
#                 ymax = 263),
#             fill = NA,
#             size = 1,
#             color = "#0072B2") +
#   # For FTLD-motor (T2)
#   geom_rect(aes(xmin = 603,
#                 xmax = 637,
#                 ymin = 603,
#                 ymax = 637),
#             fill = NA,
#             size = 1,
#             color = "#0072B2") +
#   # For FTLD-motor (T3)
#   geom_rect(aes(xmin = 988,
#                 xmax = 1026,
#                 ymin = 988,
#                 ymax = 1026),
#             fill = NA,
#             size = 1,
#             color = "#0072B2") +
#    # for FTLD-NoS (T1)
#   geom_rect(aes(xmin = 264,
#                 xmax = 287,
#                 ymin = 264,
#                 ymax = 287),
#             fill = NA,
#             size = 1,
#             color = "#56B4E9") +
#   # For FTLD-NoS (T2)
#   geom_rect(aes(xmin = 638,
#                 xmax = 675,
#                 ymin = 638,
#                 ymax = 675),
#             fill = NA,
#             size = 1,
#             color = "#56B4E9") +
#   # For FTLD-NoS (T3)
#   geom_rect(aes(xmin = 1027,
#                 xmax = 1066,
#                 ymin = 1027,
#                 ymax = 1066),
#             fill = NA,
#             size = 1,
#             color = "#56B4E9") +
#    # for PPA (T1)
#   geom_rect(aes(xmin = 288,
#                 xmax = 390,
#                 ymin = 288,
#                 ymax = 390),
#             fill = NA,
#             size = 1,
#             color = "#CC79A7") +
#   # For PPA (T2)
#   geom_rect(aes(xmin = 676,
#                 xmax = 780,
#                 ymin = 676,
#                 ymax = 780),
#             fill = NA,
#             size = 1,
#             color = "#CC79A7") +
#   # For PPA (T3)
#   geom_rect(aes(xmin = 1067,
#                 xmax = 1170,
#                 ymin = 1067,
#                 ymax = 1170),
#             fill = NA,
#             size = 1,
#             color = "#CC79A7") +
#   # change background to blank
#   theme_void() +
#   # rename legend
#   guides(fill=guide_legend(title="Similarity"))
# 
# # predecided colour schemes = AD = "#D55E00"; bvFTD = "#009E73"; FTLD-motor = "#0072B2" ; FTLD-NOS = "#56B4E9" ; PPA = "#CC79A7"

```

Trajectory analysis

```{r}



# ggplot(temp, aes(x = V1, 
#               y = V2,
#               group = ID)) +
#   geom_path() +
#   stat_smooth() +
#   geom_point(data=temp %>% 
#                filter(VisitBin!="NA"),
#              aes(fill=VisitBin, 
#                  shape=VisitBin),
#              size=4) + ##This adds markers to denote the start and end points of each trajectory
#   #    geom_text(data=temp %>% 
#   #                filter(VisitBin=="First"),
#             #    aes(label=ID),
#    #             color="black",
#    #             vjust=0,
#    #             hjust=1) + ##This labels the start of each trajectory with the 'CycloneNo' variable
#       guides(colour=F) 
# 
# 
# Visit <- umap_df_rank_withMeta_1$Visit
# Group <- umap_df_rank_withMeta_1$Dx.x
# ID <- umap_df_rank_withMeta_1$ID
# coords <- cbind(umap_d2f_rank_withMeta_1$V1, umap_df_rank_withMeta_1$V2)
# 
# 
# # calculate euclidean distance
# D = dist(coords)
# 
# oldpar <- par(mar=c(4,4,1,1))
# ecotraj::trajectoryPCoA(D, ID, Visit, lwd = 2,
#                survey.labels = T)


### extra Average nearest neighbours




#####
# 
# 
# temp_poly_plot <- temp %>%
#   ggplot(aes(x, y)) +
#   ggalt::geom_encircle()
# 
# # now get the coordinates of the polygon from the temp_poly_plot object
# ggbld <- ggplot_build(temp_poly_plot)
# gdata <- ggbld$data[[1]]
# head(gdata)
# 
# 
# x_coord <- as.vector(temp$x)
# y_coord <- as.vector(temp$y)
# 
# temp_coords <- temp %>%
#   dplyr::select(x,y)
# 
# xym <- cbind(x_coord, y_coord)
# 
# 
# library(sf)
# df <- data.frame(x = x_coord, y = x_coord)
# pts1 <- st_as_sf(x = df, coords = c('x', 'y'))
# my_hull <- st_convex_hull(st_union(pts1))
# plot(my_hull)
# 
# 
# p = Polygon(xym)
# ps = Polygons(list(p),1)
# sps = SpatialPolygons(list(ps))
# plot(sps)
# 
# # create a polygon object of the outline of the UMAP space
# p = Polygon(temp_coords, hole = F)
# 
# polys <- as.SpatialPolygons.GridTopology(temp_coords)
# 
# ps = Polygons(list(p),1)
# sps = SpatialPolygons(list(ps))
# plot(sps)
# 
# # now, to do this analysis, I need to convert this into a spatial object.
# # First, I convert it into a spatial dataframe
# 
# spatial.temp <- st_as_sf(temp, 
#                  coords = c("x", "y"))
# 
# spatial.temp.sp <- as(spatial.temp, "Spatial")
# class(spatial.temp.sp)
# 
# spatial.temp.owin <- as(spatial.temp.sp, "owin")
# class(spatial.temp.owin)
# 
# # now I convert this into a "ppp" object (there are two steps to do this- see above).
# spatial.temp1 <- as_Spatial(spatial.temp)
# ppp.spatial.temp <- as.ppp(spatial.temp1)
# 
# # conduct average neighbour analysis
# ann.p <- mean(nndist(ppp.spatial.temp))
# 
# n     <- 599L               # Number of simulations
# ann.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
# for (i in 1:n){
#   rand.p   <- rpoint(n=ppp.spatial.temp$n)  # Generate random point locations
#   ann.r[i] <- mean(nndist(rand.p, k=1))  # Tally the ANN values
#   }
# 
# ann.r
# 
# 
# 
# 
# plot(rand.p, pch=16, main=NULL, cols=rgb(0,0,0,0.5))
# hist(ann.r, main=NULL, las=1, breaks=40, col="bisque", xlim=range(ann.p, ann.r))





# associations with genetics




#7 Genetics: There are two questions asking if AD- and FTD-related genes are present:
#7.1 AD-genes
#pathsub <- pathsub %>%
#  mutate(adgene = case_when((NPPDXP == "1")  ~ 1,
#                               TRUE ~ 0)) %>%
#  select(adgene, everything())
#7.2 FTD-genes
#pathsub <- pathsub %>%
#  mutate(ftdgene = case_when((NPPDXQ == "1")  ~ 1,
#                            TRUE ~ 0)) %>%
#  select(ftdgene, everything())

```

Path extra code

```{r}

# 
# #1.Start with AD
# #1.1 There are 5 variables related to AD. Visualize how many rows having missing values (i.e., where all values are -4) 
# # all values of 0 (i.e., not AD), and all values of 8 (i.e., not assessed). Doing this we get a subset of individuals with NFT.
# umap_df_rank_withMeta_1_pathsub %>%
#   dplyr::select(NPTHAL:NACCDIFF) %>%
#   filter(!if_all(NPTHAL:NACCDIFF, ~. == 0)) %>%
#   filter(!if_all(NPTHAL:NACCDIFF, ~. == -4)) %>%
#   filter(!if_all(NPTHAL:NACCDIFF, ~. == 8))
# 
# #1.2 Create a new variable for tau to indicate presence (1) or absence (0) of NFT
# umap_df_rank_withMeta_1_pathsub_new <- umap_df_rank_withMeta_1_pathsub %>%
#   mutate(nft = case_when(NACCBRAA > "0" & NACCBRAA < "7"  ~ 1,
#                          TRUE ~ 0)) %>%
# #1.3 Create a new varaible for 'amyloid' to indicate presence or absence of plaques
#   mutate(amy = case_when((NPTHAL > "0" & NPTHAL < "8") |(NACCNEUR > "0" & NACCNEUR < "8") | 
#                            (NPADNC > "0" & NPADNC < "8") | (NACCDIFF > "0" & NACCDIFF < "8")    ~ 1,
#                          TRUE ~ 0)) %>%
# #1.4 As a final step, I'm creating a variable for 'AD' if both tau NFT and amyloid plaques are present/was assessed
#   mutate(Alzheimer = case_when((nft ==1 & amy == 1)  ~ 1,
#                          TRUE ~ 0)) %>%
#   #2 CVD
# #Create a variable for presence/absence of CVD
#   mutate(CVD = case_when(NACCVASC == "1" | (NACCAMY > "0" & NACCAMY < "8") | NPLINF == "1" |
#                            NPLAC == "1" | NPINF == "1" |  NACCINF == "1" | NPHEM == "1" |NPMICRO == "1" | 
#                            NPOLD == "1" | 
#                              NACCMICR == "1" | NACCHEM == "1" | (NACCARTE > "0" & NACCARTE < "8") |
#                            (NPWMR > "0" & NPWMR < "8") | NPPATH == "1" | NACCNEC == "1" | 
#                            NPPATH2 == "1" | NPPATH4 == "1" | NPPATH5 == "1" | NPPATH6 == "1" | 
#                            NPPATH7 == "1" | NPPATH11 == "1" | NPPATHO == "1" | NPART == "1" |NPOANG == "1" ~ 1,
#                          TRUE ~ 0)) %>%
# 
# #3 Lewy body
# #3.1 Create a variable for evidence of LBD (variables NACCLEWY and NPLBOD)
#   mutate(LewyBody = case_when((NACCLEWY > "0" & NACCLEWY < "8") |(NPLBOD > "0" & NPLBOD < "8")  ~ 1,
#                          TRUE ~ 0)) %>%
# #3.2 I'm separing the third variable related to LBD (i.e., NPNLOSS) because this question is about neuronal loss in substantia nigra
# # and could be caused by Parkinson's Disease
#   mutate(subnigra = case_when((NPNLOSS > "0" & NPNLOSS < "4")  ~ 1,
#                           TRUE ~ 0)) %>%
# #3.3 Combined lewy body evidence and neuronal loss in substantia nigra if helpful (note: both have to be present):
#   mutate(LBD = case_when((LewyBody ==1 & subnigra == 1)  ~ 1,
#                         TRUE ~ 0)) %>%
# 
# #4 Hippocampal sclerosis
#  # mutate(hs = case_when((NPHIPSCL > "0" & NPHIPSCL < "8") |NPSCL == "1" ~ 1,
# #                          TRUE ~ 0)) %>%
# 
# #5 FTLD 
# #5.1 Tauopathy: I'm going to create multiple variables for tauopathies because I want to differentiate the subtypes 
# #Starting with if identified as a tauopathy combining variables NPFTDTAU, NPFTDT2 (3R tau), NPFTDT6 (4R tau),
# #"Frontemporal dementia and parkinsonism with tau-positive or argyrophilic inclusions",
# #argyrophilic grains, tangle dominant disease, "other 3R + 4R tauopathy", and tau-other: 
#   mutate(FTLD_Tau = case_when((NPFTDTAU == "1" | NPFTDT2 == "1" | NPFTDT6 == "1" |
#                           NPFRONT == "1"|NPFTDT5 == "1" | NPFTDT9 == "1" |NPFTDT10 == "1" |NPTAU == "1")  ~ 1,
#                          TRUE ~ 0)) %>%
# #5.2 Pick's disease
#   mutate(Pick = case_when((NACCPICK == "1")  ~ 1,
#                          TRUE ~ 0)) %>%
# #5.3 Corticobasal Degeneration
#   mutate(CBD = case_when((NACCCBD == "1")  ~ 1,
#                            TRUE ~ 0)) %>%
# #5.3 Progressive supranuclear palsy 
#   mutate(PSP = case_when((NACCPROG == "1")  ~ 1,
#                          TRUE ~ 0)) %>%
# #5.4 FTD with ubiquitin-positive (tau-negative) inclusions: this is combining answers "1" (FTD with motor neuron disease)
# #and "2" (FTD without motor neuron disease)
#   mutate(FTLD_Ubiq = case_when((NPFTD > "0" & NPFTD < "3")  ~ 1,
#                          TRUE ~ 0)) %>%
#   
# #5.5 TDP-43 
#   mutate(FTLD_TDP43 = case_when((NPFTDTDP == "1" | NPTDPA == "1" |NPTDPB == "1" |NPTDPC == "1" |
#                               NPTDPD == "1" |NPTDPE == "1")  ~ 1,
#                              TRUE ~ 0)) %>%
# #5.5 ALS/MND
# #  mutate(ALSMND = case_when((NPALSMND > "0" & NPALSMND < "8")  ~ 1,
# #                           TRUE ~ 0)) %>%
# #5.6 Other FTLD including FTLD-NOS (includes dementia lacking distinctive histology [DLDH] and FTLD with no inclusions [FTLD-NI] detected by tau, TDP-43, or ubiquitin/ p62 IHC)
#   mutate(FTLD_Other = case_when((NPOFTD == "1" | NPOFTD5 == "1")  ~ 1,
#                            TRUE ~ 0)) %>%
# 
# #6 Other Path: Lumping all under one variable "otherpath" including Pigment-spheroid degeneration/NBIA, 
# # white matter disease, and neoplasm
#   mutate(Other = case_when((NPPDXA == "1" | NPPDXG == "1" | NPPDXL == "1"| NACCOTHP == "1")  ~ 1,
#                                TRUE ~ 0))
# 
# # Finally, there is info on whether AD, FTLD DLB, CVD etc were primary or contributing pathologies. I'll need to account for these because otherwise, there are pts with 6 pathologies co-occurring
# umap_df_rank_withMeta_1_pathsub_new_1 <- umap_df_rank_withMeta_1_pathsub_new %>%
#   mutate(PrimPath = case_when(NPPAD == "1" ~ "AD",
#                                  NPPLEWY == "1" ~ "LBD (Primary)",
#                                  NPPVASC == "1" ~ "CVD (Primary)",
#                                  NPPFTLD == "1" ~ "FTLD (Primary)",
#                                  NPPHIPP == "1" ~ "Hipp. Scl. (Primary)",
#                                  NPPPRION == "1" ~ "Prion-assoc. (Primary)",
#                                  NPPADP == "1" ~ "AD, insuff for Dx. (Primary)",
#                                  TRUE ~ "Other")) %>%
#   mutate(ContribPath = case_when(NPCAD == "1" ~ "AD (Contrib.)",
#                                  NPCLEWY == "1" ~ "LBD (Contrib.)",
#                                  NPCVASC == "1" ~ "CVD (Contrib.)",
#                                  NPCFTLD == "1" ~ "FTLD (Contrib.)",
#                                  NPCHIPP == "1" ~ "Hipp. Scl. (Contrib.)",
#                                  NPCPRION == "1" ~ "Prion-assoc. (Contrib.)",
#                                  NPCADP == "1" ~ "AD, insuff for Dx. (Contrib.)",
#                                  TRUE ~ "Other")) %>%
#   rowwise() %>%
#   mutate(NoReport = sum(c_across(274:288))) # the people with 0 here do not have any reported values for AD, FTLD, VASC, DLB etc. so we can remove them.
# 
# # Now I manually examine whether those individuals with a NoReport score of 0 have any values across any other pathology. If they don't AND if the
# # create a temp df to examine this
# test <- umap_df_rank_withMeta_1_pathsub_new_1 %>%
#   group_by(ID) %>%
#   dplyr::select(ID, Dx.x, NoReport, 95:292) %>%
#   filter(NoReport == 0)
# 
# # how many people are in this dataset
# n_distinct(test$ID) # 69 total people
# 
# # remove all data points that are coded as -4 and - 4.4 (missing or not available)
# test[test == -4] <- NA
# test[test == -4.4] <- NA
# # now retain only the cols that have any values.
# test1 <- test[colSums(!is.na(test)) > 0]
#   
# # This means, all those individuals who have 0 for NoReport can be safely removed as they have no pathological information across any of the neuropath cols.
# 
# # create a new df removing those individuals and take it forward
# umap_df_rank_withMeta_1_pathsub_new_2 <- umap_df_rank_withMeta_1_pathsub_new_1 %>%
#   group_by(ID) %>%
#   filter(NoReport > 0) 
# 
# # how many people are in this new dataset? 
# n_distinct(umap_df_rank_withMeta_1_pathsub_new_2$ID)
# # 140 - that means 140 + 69 from above is 209 which means the df has not been split in a weird way as 209 people have deceased status overall
# 
# # tally the number of people and their Dx with pathological data
# umap_df_rank_withMeta_1_pathsub_new_2 %>%
#   group_by(ID) %>%
#   filter(Visit == 1) %>%
#   ungroup() %>%
#   group_by(Dx.x) %>%
#   count()
# 
# # Now, for a lot of people who have "Other" as their primary pathology, we can infer what the primary path is by looking in more detail at the AD, FTLD, CVD, cols
# 
# umap_df_rank_withMeta_1_pathsub_new_3 <- umap_df_rank_withMeta_1_pathsub_new_2 %>%
#   group_by(ID) %>%
#   mutate(PrimPath = case_when(
#     (AD == 0 & CBD == 1) ~ "CBD",
#     (AD == 0 & Pick == 1) ~ "Pick",
#     (AD == 0 & PSP == 1) ~ "PSP",
#     (AD == 0 & FTLD_Ubiq == 1) ~ "FTLD-Ubiq",
#     (AD == 0 & FTLD_TDP43 == 1) ~ "FTLD-TDP43",
#     (AD == 0 & FTLD_Other == 1) ~ "FTLD-Other",
#     (AD == 0 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 1) ~ "FTLD-Tau",
#     (AD == 0 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 0 & LBD == 0 & CVD == 1) ~ "CVD",
#     (AD == 1 & (FTLD_Other == 1 | CBD == 1 | Pick == 1 | PSP == 1 | FTLD_Ubiq == 1 | FTLD_TDP43 == 1 | FTLD_Tau == 1) & LBD == 0 & CVD == 1) ~ "AD+CVD+FTLD",
#     (AD == 1 & (FTLD_Other == 1 | CBD == 1 | Pick == 1 | PSP == 1 | FTLD_Ubiq == 1 | FTLD_TDP43 == 1 | FTLD_Tau == 1) & LBD == 0 & CVD == 0) ~ "AD+FTLD",
#     (AD == 1 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 0 & LBD == 0 & Other == 0 & CVD == 1) ~ "AD+CVD",
#     (AD == 0 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 0 & LBD == 1 & CVD == 1) ~ "CVD+LBD",
#     (AD == 1 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 0 & LBD == 1 & Other == 0 & CVD == 1) ~ "AD+CVD+LBD",
#     (AD == 0 & (FTLD_Other == 1 | CBD == 1 | Pick == 1 | PSP == 1 | FTLD_Ubiq == 1 | FTLD_TDP43 == 1 | FTLD_Tau == 1) & LBD == 0 & CVD == 1) ~ "CVD+FTLD",
#     (AD == 1 & (FTLD_Other == 1 | CBD == 1 | Pick == 1 | PSP == 1 | FTLD_Ubiq == 1 | FTLD_TDP43 == 1 | FTLD_Tau == 1) & LBD == 1 & CVD == 1) ~ "AD+CVD+FTLD+LBD",
#     (AD == 0 & FTLD_Other == 0 & CBD == 0 & Pick == 0 & PSP == 0 & FTLD_Ubiq == 0 & FTLD_TDP43 == 0 & FTLD_Tau == 0 & LBD == 0 & Other == 1 & CVD == 0) ~ "Other",
#     TRUE ~ PrimPath))
# 
# # Plot the distribution of these pathologies by clinical Dx.
# 
# # create a new df
# umap_df_rank_withMeta_1_pathsub_new_4 <- umap_df_rank_withMeta_1_pathsub_new_3 %>%
#   # remove the original NACC path cols and keep only the new cols with the UMAP and PCA stuff..
#   dplyr::select(1:22, 26:66, 277:278, 281:291)

# vascular coding
 # mutate(Path_VascularChanges = case_when(NACCVASC == "1" | 
 #                           NPLINF == "1" |
 #                           NPLAC == "1" | 
 #                           NPINF == "1" |  
 #                           NACCINF == "1" | 
 #                           NPHEM == "1" | 
 #                           NPMICRO == "1" | 
 #                           NPOLD == "1" | 
 #                           NACCMICR == "1" | 
 #                           NACCHEM == "1" | 
 #                           (NACCARTE > "0" & NACCARTE < "8") |
 #                           (NPWMR > "0" & NPWMR < "8") | 
 #                           NPPATH == "1" | 
 #                           NACCNEC == "1" | 
 #                           NPPATH2 == "1" |
 #                           NPPATH3 == "1" |
 #                           NPPATH4 == "1" | 
 #                           NPPATH5 == "1" | 
 #                           NPPATH6 == "1" | 
 #                           NPPATH7 == "1" | 
 #                           NPPATH11 == "1" | 
 #                           NPPATHO == "1" | 
 #                           NPART == "1" |
 #                           NPOANG == "1" ~ "CVD",
 #                         NACCVASC == 0 ~ NA_character_,
 #                         TRUE ~ NA_character_)) %>%
```
